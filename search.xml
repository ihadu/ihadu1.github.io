<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hbase的SQL中间层—Phoenix</title>
      <link href="/2021/10/25/Hbase%E7%9A%84SQL%E4%B8%AD%E9%97%B4%E5%B1%82%E2%80%94Phoenix/"/>
      <url>/2021/10/25/Hbase%E7%9A%84SQL%E4%B8%AD%E9%97%B4%E5%B1%82%E2%80%94Phoenix/</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase的SQL中间层——Phoenix"><a href="#Hbase的SQL中间层——Phoenix" class="headerlink" title="Hbase的SQL中间层——Phoenix"></a>Hbase的SQL中间层——Phoenix</h1><h2 id="一、Phoenix简介"><a href="#一、Phoenix简介" class="headerlink" title="一、Phoenix简介"></a>一、Phoenix简介</h2><p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data  JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p><p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p><div align="center"> <img width="600px"  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-hadoop.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-hadoop.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="二、Phoenix安装"><a href="#二、Phoenix安装" class="headerlink" title="二、Phoenix安装"></a>二、Phoenix安装</h2><blockquote><p>我们可以按照官方安装说明进行安装，官方说明如下：</p><ul><li>download and expand our installation tar</li><li>copy the phoenix server jar that is compatible with your HBase installation into the lib directory of every region server</li><li>restart the region servers</li><li>add the phoenix client jar to the classpath of your HBase client</li><li>download and setup SQuirrel as your SQL client so you can issue adhoc SQL against your HBase cluster</li></ul></blockquote><h3 id="2-1-下载并解压"><a href="#2-1-下载并解压" class="headerlink" title="2.1 下载并解压"></a>2.1 下载并解压</h3><p>官方针对 Apache 版本和 CDH 版本的 HBase 均提供了安装包，按需下载即可。官方下载地址: <a href="http://phoenix.apache.org/download.html">http://phoenix.apache.org/download.html</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载</span></span><br><span class="line">wget http://mirror.bit.edu.cn/apache/phoenix/apache-phoenix-4.14.0-cdh5.14.2/bin/apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz</span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压</span></span><br><span class="line">tar tar apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz</span><br></pre></td></tr></table></figure><h3 id="2-2-拷贝Jar包"><a href="#2-2-拷贝Jar包" class="headerlink" title="2.2 拷贝Jar包"></a>2.2 拷贝Jar包</h3><p>按照官方文档的说明，需要将 <code>phoenix server jar</code> 添加到所有 <code>Region Servers</code> 的安装目录的 <code>lib</code> 目录下。</p><p>这里由于我搭建的是 HBase 伪集群，所以只需要拷贝到当前机器的 HBase 的 lib 目录下。如果是真实集群，则使用 scp 命令分发到所有 <code>Region Servers</code> 机器上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/app/apache-phoenix-4.14.0-cdh5.14.2-bin/phoenix-4.14.0-cdh5.14.2-server.jar /usr/app/hbase-1.2.0-cdh5.15.2/lib</span><br></pre></td></tr></table></figure><h3 id="2-3-重启-Region-Servers"><a href="#2-3-重启-Region-Servers" class="headerlink" title="2.3 重启 Region Servers"></a>2.3 重启 Region Servers</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 停止Hbase</span></span><br><span class="line">stop-hbase.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动Hbase</span></span><br><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><h3 id="2-4-启动Phoenix"><a href="#2-4-启动Phoenix" class="headerlink" title="2.4 启动Phoenix"></a>2.4 启动Phoenix</h3><p>在 Phoenix 解压目录下的 <code>bin</code> 目录下执行如下命令，需要指定 Zookeeper 的地址：</p><ul><li>如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则默认采用内置的 Zookeeper 服务，端口为 2181；</li><li>如果是 HBase 是集群模式并采用外置的 Zookeeper 集群，则按照自己的实际情况进行指定。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> ./sqlline.py hadoop001:2181</span></span><br></pre></td></tr></table></figure><h3 id="2-5-启动结果"><a href="#2-5-启动结果" class="headerlink" title="2.5 启动结果"></a>2.5 启动结果</h3><p>启动后则进入了 Phoenix 交互式 SQL 命令行，可以使用 <code>!table</code> 或 <code>!tables</code> 查看当前所有表的信息</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/phoenix-shell.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/phoenix-shell.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="三、Phoenix-简单使用"><a href="#三、Phoenix-简单使用" class="headerlink" title="三、Phoenix 简单使用"></a>三、Phoenix 简单使用</h2><h3 id="3-1-创建表"><a href="#3-1-创建表" class="headerlink" title="3.1 创建表"></a>3.1 创建表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> us_population (</span><br><span class="line">      state <span class="type">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">      city <span class="type">VARCHAR</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">      population <span class="type">BIGINT</span></span><br><span class="line">      <span class="keyword">CONSTRAINT</span> my_pk <span class="keyword">PRIMARY</span> KEY (state, city));</span><br></pre></td></tr></table></figure><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-create-table.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-create-table.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div>新建的表会按照特定的规则转换为 HBase 上的表，关于表的信息，可以通过 Hbase Web UI 进行查看：<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-web-ui-phoenix.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-web-ui-phoenix.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div>### 3.2 插入数据<p>Phoenix 中插入数据采用的是 <code>UPSERT</code> 而不是 <code>INSERT</code>,因为 Phoenix 并没有更新操作，插入相同主键的数据就视为更新，所以 <code>UPSERT</code> 就相当于 <code>UPDATE</code>+<code>INSERT</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">UPSERT INTO us_population VALUES(&#x27;NY&#x27;,&#x27;New York&#x27;,8143197);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;Los Angeles&#x27;,3844829);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;IL&#x27;,&#x27;Chicago&#x27;,2842518);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;Houston&#x27;,2016582);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;PA&#x27;,&#x27;Philadelphia&#x27;,1463281);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;AZ&#x27;,&#x27;Phoenix&#x27;,1461575);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;San Antonio&#x27;,1256509);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;San Diego&#x27;,1255540);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;Dallas&#x27;,1213825);</span><br><span class="line">UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;San Jose&#x27;,912332);</span><br></pre></td></tr></table></figure><h3 id="3-3-修改数据"><a href="#3-3-修改数据" class="headerlink" title="3.3 修改数据"></a>3.3 修改数据</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 插入主键相同的数据就视为更新</span></span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;NY&#x27;</span>,<span class="string">&#x27;New York&#x27;</span>,<span class="number">999999</span>);</span><br></pre></td></tr></table></figure><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-update.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-update.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div>### 3.4 删除数据<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> us_population <span class="keyword">WHERE</span> city<span class="operator">=</span><span class="string">&#x27;Dallas&#x27;</span>;</span><br></pre></td></tr></table></figure><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-delete.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-delete.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div>### 3.5 查询数据<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> state <span class="keyword">as</span> &quot;州&quot;,<span class="built_in">count</span>(city) <span class="keyword">as</span> &quot;市&quot;,<span class="built_in">sum</span>(population) <span class="keyword">as</span> &quot;热度&quot;</span><br><span class="line"><span class="keyword">FROM</span> us_population</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> state</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="built_in">sum</span>(population) <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-select.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-select.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="3-6-退出命令"><a href="#3-6-退出命令" class="headerlink" title="3.6 退出命令"></a>3.6 退出命令</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">!</span>quit</span><br></pre></td></tr></table></figure><h3 id="3-7-扩展"><a href="#3-7-扩展" class="headerlink" title="3.7 扩展"></a>3.7 扩展</h3><p>从上面的操作中可以看出，Phoenix 支持大多数标准的 SQL 语法。关于 Phoenix 支持的语法、数据类型、函数、序列等详细信息，因为涉及内容很多，可以参考其官方文档，官方文档上有详细的说明：</p><ul><li><p><strong>语法 (Grammar)</strong> ：<a href="https://phoenix.apache.org/language/index.html">https://phoenix.apache.org/language/index.html</a></p></li><li><p><strong>函数 (Functions)</strong> ：<a href="http://phoenix.apache.org/language/functions.html">http://phoenix.apache.org/language/functions.html</a></p></li><li><p><strong>数据类型 (Datatypes)</strong> ：<a href="http://phoenix.apache.org/language/datatypes.html">http://phoenix.apache.org/language/datatypes.html</a></p></li><li><p><strong>序列 (Sequences)</strong> :<a href="http://phoenix.apache.org/sequences.html">http://phoenix.apache.org/sequences.html</a></p></li><li><p><strong>联结查询 (Joins)</strong> ：<a href="http://phoenix.apache.org/joins.html">http://phoenix.apache.org/joins.html</a></p></li></ul><h2 id="四、Phoenix-Java-API"><a href="#四、Phoenix-Java-API" class="headerlink" title="四、Phoenix Java API"></a>四、Phoenix Java API</h2><p>因为 Phoenix 遵循 JDBC 规范，并提供了对应的数据库驱动 <code>PhoenixDriver</code>，这使得采用 Java 语言对其进行操作的时候，就如同对其他关系型数据库一样，下面给出基本的使用示例。</p><h3 id="4-1-引入Phoenix-core-JAR包"><a href="#4-1-引入Phoenix-core-JAR包" class="headerlink" title="4.1 引入Phoenix core JAR包"></a>4.1 引入Phoenix core JAR包</h3><p>如果是 maven 项目，直接在 maven 中央仓库找到对应的版本，导入依赖即可：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core --&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果是普通项目，则可以从 Phoenix 解压目录下找到对应的 JAR 包，然后手动引入：</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/phoenix-core-jar.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/phoenix-core-jar.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div>### 4.2 简单的Java API实例<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PhoenixJavaApi</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载数据库驱动</span></span><br><span class="line">        Class.forName(<span class="string">&quot;org.apache.phoenix.jdbc.PhoenixDriver&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 指定数据库地址,格式为 jdbc:phoenix:Zookeeper 地址</span></span><br><span class="line"><span class="comment">         * 如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则 HBase 默认使用内置的 Zookeeper，默认端口为 2181</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        Connection connection = DriverManager.getConnection(<span class="string">&quot;jdbc:phoenix:192.168.200.226:2181&quot;</span>);</span><br><span class="line"></span><br><span class="line">        PreparedStatement statement = connection.prepareStatement(<span class="string">&quot;SELECT * FROM us_population&quot;</span>);</span><br><span class="line"></span><br><span class="line">        ResultSet resultSet = statement.executeQuery();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (resultSet.next()) &#123;</span><br><span class="line">            System.out.println(resultSet.getString(<span class="string">&quot;city&quot;</span>) + <span class="string">&quot; &quot;</span></span><br><span class="line">                    + resultSet.getInt(<span class="string">&quot;population&quot;</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        statement.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果如下：</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-java-api-result.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/Phoenix-java-api-result.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p>实际的开发中我们通常都是采用第三方框架来操作数据库，如 <code>mybatis</code>，<code>Hibernate</code>，<code>Spring Data</code> 等。关于 Phoenix 与这些框架的整合步骤参见下一篇文章：<a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/Spring+Mybtais+Phoenix%E6%95%B4%E5%90%88.md">Spring/Spring Boot + Mybatis + Phoenix</a></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://phoenix.apache.org/">http://phoenix.apache.org/</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase容灾与备份</title>
      <link href="/2021/10/25/Hbase%E5%AE%B9%E7%81%BE%E4%B8%8E%E5%A4%87%E4%BB%BD/"/>
      <url>/2021/10/25/Hbase%E5%AE%B9%E7%81%BE%E4%B8%8E%E5%A4%87%E4%BB%BD/</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase容灾与备份"><a href="#Hbase容灾与备份" class="headerlink" title="Hbase容灾与备份"></a>Hbase容灾与备份</h1><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>本文主要介绍 Hbase 常用的三种简单的容灾备份方案，即<strong>CopyTable</strong>、<strong>Export</strong>/<strong>Import</strong>、<strong>Snapshot</strong>。分别介绍如下：</p><h2 id="二、CopyTable"><a href="#二、CopyTable" class="headerlink" title="二、CopyTable"></a>二、CopyTable</h2><h3 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h3><p><strong>CopyTable</strong>可以将现有表的数据复制到新表中，具有以下特点：</p><ul><li>支持时间区间 、row 区间 、改变表名称 、改变列族名称 、以及是否 Copy 已被删除的数据等功能；</li><li>执行命令前，需先创建与原表结构相同的新表；</li><li><code>CopyTable</code> 的操作是基于 HBase Client API 进行的，即采用 <code>scan</code> 进行查询, 采用 <code>put</code> 进行写入。</li></ul><h3 id="2-2-命令格式"><a href="#2-2-命令格式" class="headerlink" title="2.2 命令格式"></a>2.2 命令格式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt;</span><br></pre></td></tr></table></figure><h3 id="2-3-常用命令"><a href="#2-3-常用命令" class="headerlink" title="2.3 常用命令"></a>2.3 常用命令</h3><ol><li>同集群下 CopyTable</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=tableCopy  tableOrig</span><br></pre></td></tr></table></figure><ol start="2"><li>不同集群下 CopyTable</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 两表名称相同的情况</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--peer.adr=dstClusterZK:2181:/hbase tableOrig</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 也可以指新的表名</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--peer.adr=dstClusterZK:2181:/hbase \</span><br><span class="line">--new.name=tableCopy tableOrig</span><br></pre></td></tr></table></figure><ol start="3"><li>下面是一个官方给的比较完整的例子，指定开始和结束时间，集群地址，以及只复制指定的列族：</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.CopyTable \</span><br><span class="line">--starttime=1265875194289 \</span><br><span class="line">--endtime=1265878794289 \</span><br><span class="line">--peer.adr=server1,server2,server3:2181:/hbase \</span><br><span class="line">--families=myOldCf:myNewCf,cf2,cf3 TestTable</span><br></pre></td></tr></table></figure><h3 id="2-4-更多参数"><a href="#2-4-更多参数" class="headerlink" title="2.4 更多参数"></a>2.4 更多参数</h3><p>可以通过 <code>--help</code> 查看更多支持的参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hbase org.apache.hadoop.hbase.mapreduce.CopyTable --<span class="built_in">help</span></span></span><br></pre></td></tr></table></figure><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-copy-table.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-copy-table.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="三、Export-Import"><a href="#三、Export-Import" class="headerlink" title="三、Export/Import"></a>三、Export/Import</h2><h3 id="3-1-简介"><a href="#3-1-简介" class="headerlink" title="3.1 简介"></a>3.1 简介</h3><ul><li><code>Export</code> 支持导出数据到 HDFS, <code>Import</code> 支持从 HDFS 导入数据。<code>Export</code> 还支持指定导出数据的开始时间和结束时间，因此可以用于增量备份。</li><li><code>Export</code> 导出与 <code>CopyTable</code> 一样，依赖 HBase 的 <code>scan</code> 操作</li></ul><h3 id="3-2-命令格式"><a href="#3-2-命令格式" class="headerlink" title="3.2 命令格式"></a>3.2 命令格式</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Export</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Inport</span></span><br><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;</span><br></pre></td></tr></table></figure><ul><li>导出的 <code>outputdir</code> 目录可以不用预先创建，程序会自动创建。导出完成后，导出文件的所有权将由执行导出命令的用户所拥有。</li><li>默认情况下，仅导出给定 <code>Cell</code> 的最新版本，而不管历史版本。要导出多个版本，需要将 <code>&lt;versions&gt;</code> 参数替换为所需的版本数。</li></ul><h3 id="3-3-常用命令"><a href="#3-3-常用命令" class="headerlink" title="3.3 常用命令"></a>3.3 常用命令</h3><ol><li>导出命令</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Export tableName  hdfs 路径/tableName.db</span><br></pre></td></tr></table></figure><ol start="2"><li>导入命令</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase org.apache.hadoop.hbase.mapreduce.Import tableName  hdfs 路径/tableName.db</span><br></pre></td></tr></table></figure><h2 id="四、Snapshot"><a href="#四、Snapshot" class="headerlink" title="四、Snapshot"></a>四、Snapshot</h2><h3 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1 简介"></a>4.1 简介</h3><p>HBase 的快照 (Snapshot) 功能允许您获取表的副本 (包括内容和元数据)，并且性能开销很小。因为快照存储的仅仅是表的元数据和 HFiles 的信息。快照的 <code>clone</code> 操作会从该快照创建新表，快照的 <code>restore</code> 操作会将表的内容还原到快照节点。<code>clone</code> 和 <code>restore</code> 操作不需要复制任何数据，因为底层 HFiles(包含 HBase 表数据的文件) 不会被修改，修改的只是表的元数据信息。</p><h3 id="4-2-配置"><a href="#4-2-配置" class="headerlink" title="4.2 配置"></a>4.2 配置</h3><p>HBase 快照功能默认没有开启，如果要开启快照，需要在 <code>hbase-site.xml</code> 文件中添加如下配置项：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.snapshot.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="4-3-常用命令"><a href="#4-3-常用命令" class="headerlink" title="4.3 常用命令"></a>4.3 常用命令</h3><p>快照的所有命令都需要在 Hbase Shell 交互式命令行中执行。</p><h4 id="1-Take-a-Snapshot"><a href="#1-Take-a-Snapshot" class="headerlink" title="1. Take a Snapshot"></a>1. Take a Snapshot</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 拍摄快照</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> snapshot <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;快照名&#x27;</span></span></span><br></pre></td></tr></table></figure><p>默认情况下拍摄快照之前会在内存中执行数据刷新。以保证内存中的数据包含在快照中。但是如果你不希望包含内存中的数据，则可以使用 <code>SKIP_FLUSH</code> 选项禁止刷新。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 禁止内存刷新</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> snapshot  <span class="string">&#x27;表名&#x27;</span>, <span class="string">&#x27;快照名&#x27;</span>, &#123;SKIP_FLUSH =&gt; <span class="literal">true</span>&#125;</span></span><br></pre></td></tr></table></figure><h4 id="2-Listing-Snapshots"><a href="#2-Listing-Snapshots" class="headerlink" title="2. Listing Snapshots"></a>2. Listing Snapshots</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取快照列表</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> list_snapshots</span></span><br></pre></td></tr></table></figure><h4 id="3-Deleting-Snapshots"><a href="#3-Deleting-Snapshots" class="headerlink" title="3. Deleting Snapshots"></a>3. Deleting Snapshots</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除快照</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> delete_snapshot <span class="string">&#x27;快照名&#x27;</span></span></span><br></pre></td></tr></table></figure><h4 id="4-Clone-a-table-from-snapshot"><a href="#4-Clone-a-table-from-snapshot" class="headerlink" title="4. Clone a table from snapshot"></a>4. Clone a table from snapshot</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 从现有的快照创建一张新表</span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash">  clone_snapshot <span class="string">&#x27;快照名&#x27;</span>, <span class="string">&#x27;新表名&#x27;</span></span></span><br></pre></td></tr></table></figure><h4 id="5-Restore-a-snapshot"><a href="#5-Restore-a-snapshot" class="headerlink" title="5. Restore a snapshot"></a>5. Restore a snapshot</h4><p>将表恢复到快照节点，恢复操作需要先禁用表</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> <span class="built_in">disable</span> <span class="string">&#x27;表名&#x27;</span></span></span><br><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> restore_snapshot <span class="string">&#x27;快照名&#x27;</span></span></span><br></pre></td></tr></table></figure><p>这里需要注意的是：是如果 HBase 配置了基于 Replication 的主从复制，由于 Replication 在日志级别工作，而快照在文件系统级别工作，因此在还原之后，会出现副本与主服务器处于不同的状态的情况。这时候可以先停止同步，所有服务器还原到一致的数据点后再重新建立同步。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://blog.cloudera.com/blog/2012/06/online-hbase-backups-with-copytable-2/">Online Apache HBase Backups with CopyTable</a></li><li><a href="http://hbase.apache.org/book.htm">Apache HBase ™ Reference Guide</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase协处理器</title>
      <link href="/2021/10/25/Hbase%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8/"/>
      <url>/2021/10/25/Hbase%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase-协处理器"><a href="#Hbase-协处理器" class="headerlink" title="Hbase 协处理器"></a>Hbase 协处理器</h1><h2 id="一、简述"><a href="#一、简述" class="headerlink" title="一、简述"></a>一、简述</h2><p>在使用 HBase 时，如果你的数据量达到了数十亿行或数百万列，此时能否在查询中返回大量数据将受制于网络的带宽，即便网络状况允许，但是客户端的计算处理也未必能够满足要求。在这种情况下，协处理器（Coprocessors）应运而生。它允许你将业务计算代码放入在 RegionServer 的协处理器中，将处理好的数据再返回给客户端，这可以极大地降低需要传输的数据量，从而获得性能上的提升。同时协处理器也允许用户扩展实现 HBase 目前所不具备的功能，如权限校验、二级索引、完整性约束等。</p><h2 id="二、协处理器类型"><a href="#二、协处理器类型" class="headerlink" title="二、协处理器类型"></a>二、协处理器类型</h2><h3 id="2-1-Observer协处理器"><a href="#2-1-Observer协处理器" class="headerlink" title="2.1 Observer协处理器"></a>2.1 Observer协处理器</h3><h4 id="1-功能"><a href="#1-功能" class="headerlink" title="1. 功能"></a>1. 功能</h4><p>Observer 协处理器类似于关系型数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。通常可以用来实现下面功能：</p><ul><li><strong>权限校验</strong>：在执行 <code>Get</code> 或 <code>Put</code> 操作之前，您可以使用 <code>preGet</code> 或 <code>prePut</code> 方法检查权限；</li><li><strong>完整性约束</strong>： HBase 不支持关系型数据库中的外键功能，可以通过触发器在插入或者删除数据的时候，对关联的数据进行检查；</li><li><strong>二级索引</strong>： 可以使用协处理器来维护二级索引。</li></ul></br><h4 id="2-类型"><a href="#2-类型" class="headerlink" title="2. 类型"></a>2. 类型</h4><p>当前 Observer 协处理器有以下四种类型：</p><ul><li><strong>RegionObserver</strong> :<br>允许您观察 Region 上的事件，例如 Get 和 Put 操作。</li><li><strong>RegionServerObserver</strong> :<br>允许您观察与 RegionServer 操作相关的事件，例如启动，停止或执行合并，提交或回滚。</li><li><strong>MasterObserver</strong> :<br>允许您观察与 HBase Master 相关的事件，例如表创建，删除或 schema 修改。</li><li><strong>WalObserver</strong> :<br>允许您观察与预写日志（WAL）相关的事件。</li></ul></br><h4 id="3-接口"><a href="#3-接口" class="headerlink" title="3. 接口"></a>3. 接口</h4><p>以上四种类型的 Observer 协处理器均继承自 <code>Coprocessor</code> 接口，这四个接口中分别定义了所有可用的钩子方法，以便在对应方法前后执行特定的操作。通常情况下，我们并不会直接实现上面接口，而是继承其 Base 实现类，Base 实现类只是简单空实现了接口中的方法，这样我们在实现自定义的协处理器时，就不必实现所有方法，只需要重写必要方法即可。</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-coprocessor.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-coprocessor.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p>这里以 <code>RegionObservers </code> 为例，其接口类中定义了所有可用的钩子方法，下面截取了部分方法的定义，多数方法都是成对出现的，有 <code>pre</code> 就有 <code>post</code>：</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/RegionObserver.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/RegionObserver.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div></br><h4 id="4-执行流程"><a href="#4-执行流程" class="headerlink" title="4. 执行流程"></a>4. 执行流程</h4><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/RegionObservers-works.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/RegionObservers-works.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><ul><li>客户端发出 put 请求</li><li>该请求被分派给合适的 RegionServer 和 region</li><li>coprocessorHost 拦截该请求，然后在该表的每个 RegionObserver 上调用 prePut()</li><li>如果没有被 <code>prePut()</code> 拦截，该请求继续送到 region，然后进行处理</li><li>region 产生的结果再次被 CoprocessorHost 拦截，调用 <code>postPut()</code></li><li>假如没有 <code>postPut()</code> 拦截该响应，最终结果被返回给客户端</li></ul><p>如果大家了解 Spring，可以将这种执行方式类比于其 AOP 的执行原理即可，官方文档当中也是这样类比的：</p><blockquote><p>If you are familiar with Aspect Oriented Programming (AOP), you can think of a coprocessor as applying advice by intercepting a request and then running some custom code,before passing the request on to its final destination (or even changing the destination).</p><p>如果您熟悉面向切面编程（AOP），您可以将协处理器视为通过拦截请求然后运行一些自定义代码来使用 Advice，然后将请求传递到其最终目标（或者更改目标）。</p></blockquote><h3 id="2-2-Endpoint协处理器"><a href="#2-2-Endpoint协处理器" class="headerlink" title="2.2  Endpoint协处理器"></a>2.2  Endpoint协处理器</h3><p>Endpoint 协处理器类似于关系型数据库中的存储过程。客户端可以调用 Endpoint 协处理器在服务端对数据进行处理，然后再返回。</p><p>以聚集操作为例，如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，然后在客户端上遍历扫描结果，这必然会加重了客户端处理数据的压力。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出来，仅仅将该 max 值返回给客户端。之后客户端只需要将每个 Region 的最大值进行比较而找到其中最大的值即可。</p><h2 id="三、协处理的加载方式"><a href="#三、协处理的加载方式" class="headerlink" title="三、协处理的加载方式"></a>三、协处理的加载方式</h2><p>要使用我们自己开发的协处理器，必须通过静态（使用 HBase 配置）或动态（使用 HBase Shell 或 Java API）加载它。</p><ul><li>静态加载的协处理器称之为 <strong>System Coprocessor</strong>（系统级协处理器）,作用范围是整个 HBase 上的所有表，需要重启 HBase 服务；</li><li>动态加载的协处理器称之为 <strong>Table Coprocessor</strong>（表处理器），作用于指定的表，不需要重启 HBase 服务。</li></ul><p>其加载和卸载方式分别介绍如下。</p><h2 id="四、静态加载与卸载"><a href="#四、静态加载与卸载" class="headerlink" title="四、静态加载与卸载"></a>四、静态加载与卸载</h2><h3 id="4-1-静态加载"><a href="#4-1-静态加载" class="headerlink" title="4.1 静态加载"></a>4.1 静态加载</h3><p>静态加载分以下三步：</p><ol><li>在 <code>hbase-site.xml</code> 定义需要加载的协处理器。</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.coprocessor.region.classes<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.myname.hbase.coprocessor.endpoint.SumEndPoint<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code> &lt;name&gt;</code> 标签的值必须是下面其中之一：</p><ul><li>RegionObservers 和 Endpoints 协处理器：<code>hbase.coprocessor.region.classes</code></li><li>WALObservers 协处理器： <code>hbase.coprocessor.wal.classes</code></li><li>MasterObservers 协处理器：<code>hbase.coprocessor.master.classes</code></li></ul><p><code>&lt;value&gt;</code> 必须是协处理器实现类的全限定类名。如果为加载指定了多个类，则类名必须以逗号分隔。</p><ol start="2"><li><p>将 jar(包含代码和所有依赖项) 放入 HBase 安装目录中的 <code>lib</code> 目录下；</p></li><li><p>重启 HBase。</p></li></ol></br><h3 id="4-2-静态卸载"><a href="#4-2-静态卸载" class="headerlink" title="4.2 静态卸载"></a>4.2 静态卸载</h3><ol><li><p>从 hbase-site.xml 中删除配置的协处理器的&lt;property&gt;元素及其子元素；</p></li><li><p>从类路径或 HBase 的 lib 目录中删除协处理器的 JAR 文件（可选）；</p></li><li><p>重启 HBase。</p></li></ol><h2 id="五、动态加载与卸载"><a href="#五、动态加载与卸载" class="headerlink" title="五、动态加载与卸载"></a>五、动态加载与卸载</h2><p>使用动态加载协处理器，不需要重新启动 HBase。但动态加载的协处理器是基于每个表加载的，只能用于所指定的表。<br>此外，在使用动态加载必须使表脱机（disable）以加载协处理器。动态加载通常有两种方式：Shell 和 Java API 。</p><blockquote><p>以下示例基于两个前提：</p><ol><li>coprocessor.jar 包含协处理器实现及其所有依赖项。</li><li>JAR 包存放在 HDFS 上的路径为：hdfs：// &lt;namenode&gt;：&lt;port&gt; / user / &lt;hadoop-user&gt; /coprocessor.jar</li></ol></blockquote><h3 id="5-1-HBase-Shell动态加载"><a href="#5-1-HBase-Shell动态加载" class="headerlink" title="5.1 HBase Shell动态加载"></a>5.1 HBase Shell动态加载</h3><ol><li>使用 HBase Shell 禁用表</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt; disable &#x27;tableName&#x27;</span><br></pre></td></tr></table></figure><ol start="2"><li>使用如下命令加载协处理器</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt; alter &#x27;tableName&#x27;, METHOD =&gt; &#x27;table_att&#x27;, &#x27;Coprocessor&#x27;=&gt;&#x27;hdfs://&lt;namenode&gt;:&lt;port&gt;/</span><br><span class="line">user/&lt;hadoop-user&gt;/coprocessor.jar| org.myname.hbase.Coprocessor.RegionObserverExample|1073741823|</span><br><span class="line">arg1=1,arg2=2&#x27;</span><br></pre></td></tr></table></figure><p><code>Coprocessor</code> 包含由管道（|）字符分隔的四个参数，按顺序解释如下：</p><ul><li><p><strong>JAR 包路径</strong>：通常为 JAR 包在 HDFS 上的路径。关于路径以下两点需要注意：</p></li><li><p>允许使用通配符，例如：<code>hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/*.jar</code> 来添加指定的 JAR 包；</p></li><li><p>可以使指定目录，例如：<code>hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/</code> ，这会添加目录中的所有 JAR 包，但不会搜索子目录中的 JAR 包。</p></li><li><p><strong>类名</strong>：协处理器的完整类名。</p></li><li><p><strong>优先级</strong>：协处理器的优先级，遵循数字的自然序，即值越小优先级越高。可以为空，在这种情况下，将分配默认优先级值。</p></li><li><p><strong>可选参数</strong> ：传递的协处理器的可选参数。</p></li></ul><ol start="3"><li>启用表</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt; enable &#x27;tableName&#x27;</span><br></pre></td></tr></table></figure><ol start="4"><li>验证协处理器是否已加载</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt; describe &#x27;tableName&#x27;</span><br></pre></td></tr></table></figure><p>协处理器出现在 <code>TABLE_ATTRIBUTES</code> 属性中则代表加载成功。</p></br><h3 id="5-2-HBase-Shell动态卸载"><a href="#5-2-HBase-Shell动态卸载" class="headerlink" title="5.2 HBase Shell动态卸载"></a>5.2 HBase Shell动态卸载</h3><ol><li>禁用表</li></ol> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> <span class="built_in">disable</span> <span class="string">&#x27;tableName&#x27;</span></span></span><br></pre></td></tr></table></figure><ol start="2"><li>移除表协处理器</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> alter <span class="string">&#x27;tableName&#x27;</span>, METHOD =&gt; <span class="string">&#x27;table_att_unset&#x27;</span>, NAME =&gt; <span class="string">&#x27;coprocessor$1&#x27;</span></span></span><br></pre></td></tr></table></figure><ol start="3"><li>启用表</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">hbase&gt;</span><span class="bash"> <span class="built_in">enable</span> <span class="string">&#x27;tableName&#x27;</span></span></span><br></pre></td></tr></table></figure></br><h3 id="5-3-Java-API-动态加载"><a href="#5-3-Java-API-动态加载" class="headerlink" title="5.3 Java API 动态加载"></a>5.3 Java API 动态加载</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">TableName tableName = TableName.valueOf(<span class="string">&quot;users&quot;</span>);</span><br><span class="line">String path = <span class="string">&quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;</span>;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">Admin admin = connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">HTableDescriptor hTableDescriptor = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line">HColumnDescriptor columnFamily1 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">&quot;personalDet&quot;</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line">HColumnDescriptor columnFamily2 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">&quot;salaryDet&quot;</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">hTableDescriptor.setValue(<span class="string">&quot;COPROCESSOR$1&quot;</span>, path + <span class="string">&quot;|&quot;</span></span><br><span class="line">+ RegionObserverExample.class.getCanonicalName() + <span class="string">&quot;|&quot;</span></span><br><span class="line">+ Coprocessor.PRIORITY_USER);</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure><p>在 HBase 0.96 及其以后版本中，HTableDescriptor 的 addCoprocessor() 方法提供了一种更为简便的加载方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">TableName tableName = TableName.valueOf(<span class="string">&quot;users&quot;</span>);</span><br><span class="line">Path path = <span class="keyword">new</span> Path(<span class="string">&quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;</span>);</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">Admin admin = connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">HTableDescriptor hTableDescriptor = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line">HColumnDescriptor columnFamily1 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">&quot;personalDet&quot;</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line">HColumnDescriptor columnFamily2 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">&quot;salaryDet&quot;</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">hTableDescriptor.addCoprocessor(RegionObserverExample.class.getCanonicalName(), path,</span><br><span class="line">Coprocessor.PRIORITY_USER, <span class="keyword">null</span>);</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure><h3 id="5-4-Java-API-动态卸载"><a href="#5-4-Java-API-动态卸载" class="headerlink" title="5.4 Java API 动态卸载"></a>5.4 Java API 动态卸载</h3><p>卸载其实就是重新定义表但不设置协处理器。这会删除所有表上的协处理器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">TableName tableName = TableName.valueOf(<span class="string">&quot;users&quot;</span>);</span><br><span class="line">String path = <span class="string">&quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;</span>;</span><br><span class="line">Configuration conf = HBaseConfiguration.create();</span><br><span class="line">Connection connection = ConnectionFactory.createConnection(conf);</span><br><span class="line">Admin admin = connection.getAdmin();</span><br><span class="line">admin.disableTable(tableName);</span><br><span class="line">HTableDescriptor hTableDescriptor = <span class="keyword">new</span> HTableDescriptor(tableName);</span><br><span class="line">HColumnDescriptor columnFamily1 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">&quot;personalDet&quot;</span>);</span><br><span class="line">columnFamily1.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily1);</span><br><span class="line">HColumnDescriptor columnFamily2 = <span class="keyword">new</span> HColumnDescriptor(<span class="string">&quot;salaryDet&quot;</span>);</span><br><span class="line">columnFamily2.setMaxVersions(<span class="number">3</span>);</span><br><span class="line">hTableDescriptor.addFamily(columnFamily2);</span><br><span class="line">admin.modifyTable(tableName, hTableDescriptor);</span><br><span class="line">admin.enableTable(tableName);</span><br></pre></td></tr></table></figure><h2 id="六、协处理器案例"><a href="#六、协处理器案例" class="headerlink" title="六、协处理器案例"></a>六、协处理器案例</h2><p>这里给出一个简单的案例，实现一个类似于 Redis 中 <code>append</code> 命令的协处理器，当我们对已有列执行 put 操作时候，HBase 默认执行的是 update 操作，这里我们修改为执行 append 操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> redis append 命令示例</span></span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash">  EXISTS mykey</span></span><br><span class="line">(integer) 0</span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash">  APPEND mykey <span class="string">&quot;Hello&quot;</span></span></span><br><span class="line">(integer) 5</span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash">  APPEND mykey <span class="string">&quot; World&quot;</span></span></span><br><span class="line">(integer) 11</span><br><span class="line"><span class="meta">redis&gt;</span><span class="bash">  GET mykey</span> </span><br><span class="line">&quot;Hello World&quot;</span><br></pre></td></tr></table></figure><h3 id="6-1-创建测试表"><a href="#6-1-创建测试表" class="headerlink" title="6.1 创建测试表"></a>6.1 创建测试表</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建一张杂志表 有文章和图片两个列族</span></span><br><span class="line">hbase &gt;  create &#x27;magazine&#x27;,&#x27;article&#x27;,&#x27;picture&#x27;</span><br></pre></td></tr></table></figure><h3 id="6-2-协处理器编程"><a href="#6-2-协处理器编程" class="headerlink" title="6.2 协处理器编程"></a>6.2 协处理器编程</h3><blockquote><p>完整代码可见本仓库：<a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Hbase/hbase-observer-coprocessor">hbase-observer-coprocessor</a></p></blockquote><p>新建 Maven 工程，导入下面依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-server<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>继承 <code>BaseRegionObserver</code> 实现我们自定义的 <code>RegionObserver</code>,对相同的 <code>article:content</code> 执行 put 命令时，将新插入的内容添加到原有内容的末尾，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AppendRegionObserver</span> <span class="keyword">extends</span> <span class="title">BaseRegionObserver</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">byte</span>[] columnFamily = Bytes.toBytes(<span class="string">&quot;article&quot;</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">byte</span>[] qualifier = Bytes.toBytes(<span class="string">&quot;content&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prePut</span><span class="params">(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit,</span></span></span><br><span class="line"><span class="params"><span class="function">                       Durability durability)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (put.has(columnFamily, qualifier)) &#123;</span><br><span class="line">            <span class="comment">// 遍历查询结果，获取指定列的原值</span></span><br><span class="line">            Result rs = e.getEnvironment().getRegion().get(<span class="keyword">new</span> Get(put.getRow()));</span><br><span class="line">            String oldValue = <span class="string">&quot;&quot;</span>;</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : rs.rawCells())</span><br><span class="line">                <span class="keyword">if</span> (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123;</span><br><span class="line">                    oldValue = Bytes.toString(CellUtil.cloneValue(cell));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取指定列新插入的值</span></span><br><span class="line">            List&lt;Cell&gt; cells = put.get(columnFamily, qualifier);</span><br><span class="line">            String newValue = <span class="string">&quot;&quot;</span>;</span><br><span class="line">            <span class="keyword">for</span> (Cell cell : cells) &#123;</span><br><span class="line">                <span class="keyword">if</span> (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123;</span><br><span class="line">                    newValue = Bytes.toString(CellUtil.cloneValue(cell));</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Append 操作</span></span><br><span class="line">            put.addColumn(columnFamily, qualifier, Bytes.toBytes(oldValue + newValue));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="6-3-打包项目"><a href="#6-3-打包项目" class="headerlink" title="6.3 打包项目"></a>6.3 打包项目</h3><p>使用 maven 命令进行打包，打包后的文件名为 <code>hbase-observer-coprocessor-1.0-SNAPSHOT.jar</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mvn clean package</span></span><br></pre></td></tr></table></figure><h3 id="6-4-上传JAR包到HDFS"><a href="#6-4-上传JAR包到HDFS" class="headerlink" title="6.4 上传JAR包到HDFS"></a>6.4 上传JAR包到HDFS</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 上传项目到HDFS上的hbase目录</span></span><br><span class="line">hadoop fs -put /usr/app/hbase-observer-coprocessor-1.0-SNAPSHOT.jar /hbase</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看上传是否成功</span></span><br><span class="line">hadoop fs -ls /hbase</span><br></pre></td></tr></table></figure><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-hdfs.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-hdfs.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="6-5-加载协处理器"><a href="#6-5-加载协处理器" class="headerlink" title="6.5 加载协处理器"></a>6.5 加载协处理器</h3><ol><li>加载协处理器前需要先禁用表</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt;  disable &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure><ol start="2"><li>加载协处理器</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt;   alter &#x27;magazine&#x27;, METHOD =&gt; &#x27;table_att&#x27;, &#x27;Coprocessor&#x27;=&gt;&#x27;hdfs://hadoop001:8020/hbase/hbase-observer-coprocessor-1.0-SNAPSHOT.jar|com.ihadyou.AppendRegionObserver|1001|&#x27;</span><br></pre></td></tr></table></figure><ol start="3"><li>启用表</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt;  enable &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure><ol start="4"><li>查看协处理器是否加载成功</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt;  desc &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure><p>协处理器出现在 <code>TABLE_ATTRIBUTES</code> 属性中则代表加载成功，如下图：</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-load.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-load.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="6-6-测试加载结果"><a href="#6-6-测试加载结果" class="headerlink" title="6.6 测试加载结果"></a>6.6 测试加载结果</h3><p>插入一组测试数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;Hello&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;</span><br><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;World&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;</span><br></pre></td></tr></table></figure><p>可以看到对于指定列的值已经执行了 append 操作：</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-helloworld.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-helloworld.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p>插入一组对照数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:author&#x27;,&#x27;zhangsan&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:author&#x27;</span><br><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:author&#x27;,&#x27;lisi&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:author&#x27;</span><br></pre></td></tr></table></figure><p>可以看到对于正常的列还是执行 update 操作:</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-lisi.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-cp-lisi.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="6-7-卸载协处理器"><a href="#6-7-卸载协处理器" class="headerlink" title="6.7 卸载协处理器"></a>6.7 卸载协处理器</h3><ol><li>卸载协处理器前需要先禁用表</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt;  disable &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure><ol start="2"><li>卸载协处理器</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt; alter &#x27;magazine&#x27;, METHOD =&gt; &#x27;table_att_unset&#x27;, NAME =&gt; &#x27;coprocessor$1&#x27;</span><br></pre></td></tr></table></figure><ol start="3"><li>启用表</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt;  enable &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure><ol start="4"><li>查看协处理器是否卸载成功</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt;  desc &#x27;magazine&#x27;</span><br></pre></td></tr></table></figure><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-co-unload.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-co-unload.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="6-8-测试卸载结果"><a href="#6-8-测试卸载结果" class="headerlink" title="6.8 测试卸载结果"></a>6.8 测试卸载结果</h3><p>依次执行下面命令可以测试卸载是否成功</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;</span><br><span class="line">hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;Hello&#x27;</span><br><span class="line">hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;</span><br></pre></td></tr></table></figure><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-unload-test.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-unload-test.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="http://hbase.apache.org/book.html#cp">Apache HBase Coprocessors</a></li><li><a href="https://blogs.apache.org/hbase/entry/coprocessor_introduction">Apache HBase Coprocessor Introduction</a></li><li><a href="https://www.itread01.com/content/1546245908.html">HBase 高階知識</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase过滤器</title>
      <link href="/2021/10/25/Hbase%E8%BF%87%E6%BB%A4%E5%99%A8/"/>
      <url>/2021/10/25/Hbase%E8%BF%87%E6%BB%A4%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase-过滤器详解"><a href="#Hbase-过滤器详解" class="headerlink" title="Hbase 过滤器详解"></a>Hbase 过滤器详解</h1><h2 id="一、HBase过滤器简介"><a href="#一、HBase过滤器简介" class="headerlink" title="一、HBase过滤器简介"></a>一、HBase过滤器简介</h2><p>Hbase 提供了种类丰富的过滤器（filter）来提高数据处理的效率，用户可以通过内置或自定义的过滤器来对数据进行过滤，所有的过滤器都在服务端生效，即谓词下推（predicate push down）。这样可以保证过滤掉的数据不会被传送到客户端，从而减轻网络传输和客户端处理的压力。</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-fliter.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-fliter.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="二、过滤器基础"><a href="#二、过滤器基础" class="headerlink" title="二、过滤器基础"></a>二、过滤器基础</h2><h3 id="2-1-Filter接口和FilterBase抽象类"><a href="#2-1-Filter接口和FilterBase抽象类" class="headerlink" title="2.1  Filter接口和FilterBase抽象类"></a>2.1  Filter接口和FilterBase抽象类</h3><p>Filter 接口中定义了过滤器的基本方法，FilterBase 抽象类实现了 Filter 接口。所有内置的过滤器则直接或者间接继承自 FilterBase 抽象类。用户只需要将定义好的过滤器通过 <code>setFilter</code> 方法传递给 <code>Scan</code> 或 <code>put</code> 的实例即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setFilter(Filter filter)</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Scan 中定义的 setFilter</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> Scan <span class="title">setFilter</span><span class="params">(Filter filter)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">super</span>.setFilter(filter);</span><br><span class="line">   <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// Get 中定义的 setFilter</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> Get <span class="title">setFilter</span><span class="params">(Filter filter)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">super</span>.setFilter(filter);</span><br><span class="line">   <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>FilterBase 的所有子类过滤器如下：<div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-filterbase-subclass.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-filterbase-subclass.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div></p><blockquote><p>说明：上图基于当前时间点（2019.4）最新的 Hbase-2.1.4 ，下文所有说明均基于此版本。</p></blockquote><h3 id="2-2-过滤器分类"><a href="#2-2-过滤器分类" class="headerlink" title="2.2 过滤器分类"></a>2.2 过滤器分类</h3><p>HBase 内置过滤器可以分为三类：分别是比较过滤器，专用过滤器和包装过滤器。分别在下面的三个小节中做详细的介绍。</p><h2 id="三、比较过滤器"><a href="#三、比较过滤器" class="headerlink" title="三、比较过滤器"></a>三、比较过滤器</h2><p>所有比较过滤器均继承自 <code>CompareFilter</code>。创建一个比较过滤器需要两个参数，分别是<strong>比较运算符</strong>和<strong>比较器实例</strong>。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">CompareFilter</span><span class="params">(<span class="keyword">final</span> CompareOp compareOp,<span class="keyword">final</span> ByteArrayComparable comparator)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">this</span>.compareOp = compareOp;</span><br><span class="line">   <span class="keyword">this</span>.comparator = comparator;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="3-1-比较运算符"><a href="#3-1-比较运算符" class="headerlink" title="3.1 比较运算符"></a>3.1 比较运算符</h3><ul><li>LESS (&lt;)</li><li>LESS_OR_EQUAL (&lt;=)</li><li>EQUAL (=)</li><li>NOT_EQUAL (!=)</li><li>GREATER_OR_EQUAL (&gt;=)</li><li>GREATER (&gt;)</li><li>NO_OP (排除所有符合条件的值)</li></ul><p>比较运算符均定义在枚举类 <code>CompareOperator</code> 中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Public</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">CompareOperator</span> </span>&#123;</span><br><span class="line">  LESS,</span><br><span class="line">  LESS_OR_EQUAL,</span><br><span class="line">  EQUAL,</span><br><span class="line">  NOT_EQUAL,</span><br><span class="line">  GREATER_OR_EQUAL,</span><br><span class="line">  GREATER,</span><br><span class="line">  NO_OP,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意：在 1.x 版本的 HBase 中，比较运算符定义在 <code>CompareFilter.CompareOp</code> 枚举类中，但在 2.0 之后这个类就被标识为 @deprecated ，并会在 3.0 移除。所以 2.0 之后版本的 HBase 需要使用 <code>CompareOperator</code> 这个枚举类。</p></blockquote><h3 id="3-2-比较器"><a href="#3-2-比较器" class="headerlink" title="3.2 比较器"></a>3.2 比较器</h3><p>所有比较器均继承自 <code>ByteArrayComparable</code> 抽象类，常用的有以下几种：</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-bytearraycomparable.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-bytearraycomparable.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><ul><li><strong>BinaryComparator</strong>  : 使用 <code>Bytes.compareTo(byte []，byte [])</code> 按字典序比较指定的字节数组。</li><li><strong>BinaryPrefixComparator</strong> : 按字典序与指定的字节数组进行比较，但只比较到这个字节数组的长度。</li><li><strong>RegexStringComparator</strong> :  使用给定的正则表达式与指定的字节数组进行比较。仅支持 <code>EQUAL</code> 和 <code>NOT_EQUAL</code> 操作。</li><li><strong>SubStringComparator</strong> : 测试给定的子字符串是否出现在指定的字节数组中，比较不区分大小写。仅支持 <code>EQUAL</code> 和 <code>NOT_EQUAL</code> 操作。</li><li><strong>NullComparator</strong> ：判断给定的值是否为空。</li><li><strong>BitComparator</strong> ：按位进行比较。</li></ul><p><code>BinaryPrefixComparator</code> 和 <code>BinaryComparator</code> 的区别不是很好理解，这里举例说明一下：</p><p>在进行 <code>EQUAL</code> 的比较时，如果比较器传入的是 <code>abcd</code> 的字节数组，但是待比较数据是 <code>abcdefgh</code>：</p><ul><li>如果使用的是 <code>BinaryPrefixComparator</code> 比较器，则比较以 <code>abcd</code> 字节数组的长度为准，即 <code>efgh</code> 不会参与比较，这时候认为 <code>abcd</code> 与 <code>abcdefgh</code> 是满足 <code>EQUAL</code> 条件的；</li><li>如果使用的是 <code>BinaryComparator</code> 比较器，则认为其是不相等的。</li></ul><h3 id="3-3-比较过滤器种类"><a href="#3-3-比较过滤器种类" class="headerlink" title="3.3 比较过滤器种类"></a>3.3 比较过滤器种类</h3><p>比较过滤器共有五个（Hbase 1.x 版本和 2.x 版本相同），见下图：</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-compareFilter.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-compareFilter.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><ul><li><strong>RowFilter</strong> ：基于行键来过滤数据；</li><li><strong>FamilyFilterr</strong> ：基于列族来过滤数据；</li><li><strong>QualifierFilterr</strong> ：基于列限定符（列名）来过滤数据；</li><li><strong>ValueFilterr</strong> ：基于单元格 (cell) 的值来过滤数据；</li><li><strong>DependentColumnFilter</strong> ：指定一个参考列来过滤其他列的过滤器，过滤的原则是基于参考列的时间戳来进行筛选 。</li></ul><p>前四种过滤器的使用方法相同，均只要传递比较运算符和运算器实例即可构建，然后通过 <code>setFilter</code> 方法传递给 <code>scan</code>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Filter filter  = <span class="keyword">new</span> RowFilter(CompareOperator.LESS_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>)));</span><br><span class="line"> scan.setFilter(filter);    </span><br></pre></td></tr></table></figure><p><code>DependentColumnFilter</code> 的使用稍微复杂一点，这里单独做下说明。</p><h3 id="3-4-DependentColumnFilter"><a href="#3-4-DependentColumnFilter" class="headerlink" title="3.4 DependentColumnFilter"></a>3.4 DependentColumnFilter</h3><p>可以把 <code>DependentColumnFilter</code> 理解为<strong>一个 valueFilter 和一个时间戳过滤器的组合</strong>。<code>DependentColumnFilter</code> 有三个带参构造器，这里选择一个参数最全的进行说明：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DependentColumnFilter(<span class="keyword">final</span> <span class="keyword">byte</span> [] family, <span class="keyword">final</span> <span class="keyword">byte</span>[] qualifier,</span><br><span class="line">                               <span class="keyword">final</span> <span class="keyword">boolean</span> dropDependentColumn, <span class="keyword">final</span> CompareOperator op,</span><br><span class="line">                               <span class="keyword">final</span> ByteArrayComparable valueComparator)</span><br></pre></td></tr></table></figure><ul><li><strong>family</strong>  ：列族</li><li><strong>qualifier</strong> ：列限定符（列名）</li><li><strong>dropDependentColumn</strong> ：决定参考列是否被包含在返回结果内，为 true 时表示参考列被返回，为 false 时表示被丢弃</li><li><strong>op</strong> ：比较运算符</li><li><strong>valueComparator</strong> ：比较器</li></ul><p>这里举例进行说明：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DependentColumnFilter dependentColumnFilter = <span class="keyword">new</span> DependentColumnFilter( </span><br><span class="line">    Bytes.toBytes(<span class="string">&quot;student&quot;</span>),</span><br><span class="line">    Bytes.toBytes(<span class="string">&quot;name&quot;</span>),</span><br><span class="line">    <span class="keyword">false</span>,</span><br><span class="line">    CompareOperator.EQUAL, </span><br><span class="line">    <span class="keyword">new</span> BinaryPrefixComparator(Bytes.toBytes(<span class="string">&quot;xiaolan&quot;</span>)));</span><br></pre></td></tr></table></figure><ul><li><p>首先会去查找 <code>student:name</code> 中值以 <code>xiaolan</code> 开头的所有数据获得 <code>参考数据集</code>，这一步等同于 valueFilter 过滤器；</p></li><li><p>其次再用参考数据集中所有数据的时间戳去检索其他列，获得时间戳相同的其他列的数据作为 <code>结果数据集</code>，这一步等同于时间戳过滤器；</p></li><li><p>最后如果 <code>dropDependentColumn</code> 为 true，则返回 <code>参考数据集</code>+<code>结果数据集</code>，若为 false，则抛弃参考数据集，只返回 <code>结果数据集</code>。</p></li></ul><h2 id="四、专用过滤器"><a href="#四、专用过滤器" class="headerlink" title="四、专用过滤器"></a>四、专用过滤器</h2><p>专用过滤器通常直接继承自 <code>FilterBase</code>，适用于范围更小的筛选规则。</p><h3 id="4-1-单列列值过滤器-SingleColumnValueFilter"><a href="#4-1-单列列值过滤器-SingleColumnValueFilter" class="headerlink" title="4.1 单列列值过滤器 (SingleColumnValueFilter)"></a>4.1 单列列值过滤器 (SingleColumnValueFilter)</h3><p>基于某列（参考列）的值决定某行数据是否被过滤。其实例有以下方法：</p><ul><li><strong>setFilterIfMissing(boolean filterIfMissing)</strong> ：默认值为 false，即如果该行数据不包含参考列，其依然被包含在最后的结果中；设置为 true 时，则不包含；</li><li><strong>setLatestVersionOnly(boolean latestVersionOnly)</strong> ：默认为 true，即只检索参考列的最新版本数据；设置为 false，则检索所有版本数据。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter(</span><br><span class="line">                &quot;student&quot;.getBytes(), </span><br><span class="line">                &quot;name&quot;.getBytes(), </span><br><span class="line">                CompareOperator.EQUAL, </span><br><span class="line">                new SubstringComparator(&quot;xiaolan&quot;));</span><br><span class="line">singleColumnValueFilter.setFilterIfMissing(true);</span><br><span class="line">scan.setFilter(singleColumnValueFilter);</span><br></pre></td></tr></table></figure><h3 id="4-2-单列列值排除器-SingleColumnValueExcludeFilter"><a href="#4-2-单列列值排除器-SingleColumnValueExcludeFilter" class="headerlink" title="4.2 单列列值排除器 (SingleColumnValueExcludeFilter)"></a>4.2 单列列值排除器 (SingleColumnValueExcludeFilter)</h3><p><code>SingleColumnValueExcludeFilter</code> 继承自上面的 <code>SingleColumnValueFilter</code>，过滤行为与其相反。</p><h3 id="4-3-行键前缀过滤器-PrefixFilter"><a href="#4-3-行键前缀过滤器-PrefixFilter" class="headerlink" title="4.3 行键前缀过滤器 (PrefixFilter)"></a>4.3 行键前缀过滤器 (PrefixFilter)</h3><p>基于 RowKey 值决定某行数据是否被过滤。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PrefixFilter prefixFilter = <span class="keyword">new</span> PrefixFilter(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>));</span><br><span class="line">scan.setFilter(prefixFilter);</span><br></pre></td></tr></table></figure><h3 id="4-4-列名前缀过滤器-ColumnPrefixFilter"><a href="#4-4-列名前缀过滤器-ColumnPrefixFilter" class="headerlink" title="4.4 列名前缀过滤器 (ColumnPrefixFilter)"></a>4.4 列名前缀过滤器 (ColumnPrefixFilter)</h3><p>基于列限定符（列名）决定某行数据是否被过滤。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ColumnPrefixFilter columnPrefixFilter = <span class="keyword">new</span> ColumnPrefixFilter(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>));</span><br><span class="line"> scan.setFilter(columnPrefixFilter);</span><br></pre></td></tr></table></figure><h3 id="4-5-分页过滤器-PageFilter"><a href="#4-5-分页过滤器-PageFilter" class="headerlink" title="4.5 分页过滤器 (PageFilter)"></a>4.5 分页过滤器 (PageFilter)</h3><p>可以使用这个过滤器实现对结果按行进行分页，创建 PageFilter 实例的时候需要传入每页的行数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">PageFilter</span><span class="params">(<span class="keyword">final</span> <span class="keyword">long</span> pageSize)</span> </span>&#123;</span><br><span class="line">    Preconditions.checkArgument(pageSize &gt;= <span class="number">0</span>, <span class="string">&quot;must be positive %s&quot;</span>, pageSize);</span><br><span class="line">    <span class="keyword">this</span>.pageSize = pageSize;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>下面的代码体现了客户端实现分页查询的主要逻辑，这里对其进行一下解释说明：</p><p>客户端进行分页查询，需要传递 <code>startRow</code>(起始 RowKey)，知道起始 <code>startRow</code> 后，就可以返回对应的 pageSize 行数据。这里唯一的问题就是，对于第一次查询，显然 <code>startRow</code> 就是表格的第一行数据，但是之后第二次、第三次查询我们并不知道 <code>startRow</code>，只能知道上一次查询的最后一条数据的 RowKey（简单称之为 <code>lastRow</code>）。</p><p>我们不能将 <code>lastRow</code> 作为新一次查询的 <code>startRow</code> 传入，因为 scan 的查询区间是[startRow，endRow) ，即前开后闭区间，这样 <code>startRow</code> 在新的查询也会被返回，这条数据就重复了。</p><p>同时在不使用第三方数据库存储 RowKey 的情况下，我们是无法通过知道 <code>lastRow</code> 的下一个 RowKey 的，因为 RowKey 的设计可能是连续的也有可能是不连续的。</p><p>由于 Hbase 的 RowKey 是按照字典序进行排序的。这种情况下，就可以在 <code>lastRow</code> 后面加上 <code>0</code> ，作为 <code>startRow</code> 传入，因为按照字典序的规则，某个值加上 <code>0</code> 后的新值，在字典序上一定是这个值的下一个值，对于 HBase 来说下一个 RowKey 在字典序上一定也是等于或者大于这个新值的。</p><p>所以最后传入 <code>lastRow</code>+<code>0</code>，如果等于这个值的 RowKey 存在就从这个值开始 scan,否则从字典序的下一个 RowKey 开始 scan。</p><blockquote><p>25 个字母以及数字字符，字典排序如下:</p><p><code>&#39;0&#39; &lt; &#39;1&#39; &lt; &#39;2&#39; &lt; ... &lt; &#39;9&#39; &lt; &#39;a&#39; &lt; &#39;b&#39; &lt; ... &lt; &#39;z&#39;</code></p></blockquote><p>分页查询主要实现逻辑：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">byte</span>[] POSTFIX = <span class="keyword">new</span> <span class="keyword">byte</span>[] &#123; <span class="number">0x00</span> &#125;;</span><br><span class="line">Filter filter = <span class="keyword">new</span> PageFilter(<span class="number">15</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> totalRows = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">byte</span>[] lastRow = <span class="keyword">null</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">    scan.setFilter(filter);</span><br><span class="line">    <span class="keyword">if</span> (lastRow != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// 如果不是首行 则 lastRow + 0</span></span><br><span class="line">        <span class="keyword">byte</span>[] startRow = Bytes.add(lastRow, POSTFIX);</span><br><span class="line">        System.out.println(<span class="string">&quot;start row: &quot;</span> +</span><br><span class="line">                           Bytes.toStringBinary(startRow));</span><br><span class="line">        scan.withStartRow(startRow);</span><br><span class="line">    &#125;</span><br><span class="line">    ResultScanner scanner = table.getScanner(scan);</span><br><span class="line">    <span class="keyword">int</span> localRows = <span class="number">0</span>;</span><br><span class="line">    Result result;</span><br><span class="line">    <span class="keyword">while</span> ((result = scanner.next()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">        System.out.println(localRows++ + <span class="string">&quot;: &quot;</span> + result);</span><br><span class="line">        totalRows++;</span><br><span class="line">        lastRow = result.getRow();</span><br><span class="line">    &#125;</span><br><span class="line">    scanner.close();</span><br><span class="line">    <span class="comment">//最后一页，查询结束  </span></span><br><span class="line">    <span class="keyword">if</span> (localRows == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(<span class="string">&quot;total rows: &quot;</span> + totalRows);</span><br></pre></td></tr></table></figure><blockquote><p>需要注意的是在多台 Regin Services 上执行分页过滤的时候，由于并行执行的过滤器不能共享它们的状态和边界，所以有可能每个过滤器都会在完成扫描前获取了 PageCount 行的结果，这种情况下会返回比分页条数更多的数据，分页过滤器就有失效的可能。</p></blockquote><h3 id="4-6-时间戳过滤器-TimestampsFilter"><a href="#4-6-时间戳过滤器-TimestampsFilter" class="headerlink" title="4.6 时间戳过滤器 (TimestampsFilter)"></a>4.6 时间戳过滤器 (TimestampsFilter)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Long&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">list.add(<span class="number">1554975573000L</span>);</span><br><span class="line">TimestampsFilter timestampsFilter = <span class="keyword">new</span> TimestampsFilter(list);</span><br><span class="line">scan.setFilter(timestampsFilter);</span><br></pre></td></tr></table></figure><h3 id="4-7-首次行键过滤器-FirstKeyOnlyFilter"><a href="#4-7-首次行键过滤器-FirstKeyOnlyFilter" class="headerlink" title="4.7 首次行键过滤器 (FirstKeyOnlyFilter)"></a>4.7 首次行键过滤器 (FirstKeyOnlyFilter)</h3><p><code>FirstKeyOnlyFilter</code> 只扫描每行的第一列，扫描完第一列后就结束对当前行的扫描，并跳转到下一行。相比于全表扫描，其性能更好，通常用于行数统计的场景，因为如果某一行存在，则行中必然至少有一列。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FirstKeyOnlyFilter firstKeyOnlyFilter = <span class="keyword">new</span> FirstKeyOnlyFilter();</span><br><span class="line">scan.set(firstKeyOnlyFilter);</span><br></pre></td></tr></table></figure><h2 id="五、包装过滤器"><a href="#五、包装过滤器" class="headerlink" title="五、包装过滤器"></a>五、包装过滤器</h2><p>包装过滤器就是通过包装其他过滤器以实现某些拓展的功能。</p><h3 id="5-1-SkipFilter过滤器"><a href="#5-1-SkipFilter过滤器" class="headerlink" title="5.1 SkipFilter过滤器"></a>5.1 SkipFilter过滤器</h3><p><code>SkipFilter</code> 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，则拓展过滤整行数据。下面是一个使用示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义 ValueFilter 过滤器</span></span><br><span class="line">Filter filter1 = <span class="keyword">new</span> ValueFilter(CompareOperator.NOT_EQUAL,</span><br><span class="line">      <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">&quot;xxx&quot;</span>)));</span><br><span class="line"><span class="comment">// 使用 SkipFilter 进行包装</span></span><br><span class="line">Filter filter2 = <span class="keyword">new</span> SkipFilter(filter1);</span><br></pre></td></tr></table></figure><h3 id="5-2-WhileMatchFilter过滤器"><a href="#5-2-WhileMatchFilter过滤器" class="headerlink" title="5.2 WhileMatchFilter过滤器"></a>5.2 WhileMatchFilter过滤器</h3><p><code>WhileMatchFilter</code> 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，<code>WhileMatchFilter</code> 则结束本次扫描，返回已经扫描到的结果。下面是其使用示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Filter filter1 = <span class="keyword">new</span> RowFilter(CompareOperator.NOT_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">&quot;rowKey4&quot;</span>)));</span><br><span class="line"></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">scan.setFilter(filter1);</span><br><span class="line">ResultScanner scanner1 = table.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span> (Result result : scanner1) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Cell cell : result.listCells()) &#123;</span><br><span class="line">        System.out.println(cell);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">scanner1.close();</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">&quot;--------------------&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用 WhileMatchFilter 进行包装</span></span><br><span class="line">Filter filter2 = <span class="keyword">new</span> WhileMatchFilter(filter1);</span><br><span class="line"></span><br><span class="line">scan.setFilter(filter2);</span><br><span class="line">ResultScanner scanner2 = table.getScanner(scan);</span><br><span class="line"><span class="keyword">for</span> (Result result : scanner1) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Cell cell : result.listCells()) &#123;</span><br><span class="line">        System.out.println(cell);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">scanner2.close();</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">rowKey0/student</span>:<span class="string">name/1555035006994/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey1/student</span>:<span class="string">name/1555035007019/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey2/student</span>:<span class="string">name/1555035007025/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey3/student</span>:<span class="string">name/1555035007037/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey5/student</span>:<span class="string">name/1555035007051/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey6/student</span>:<span class="string">name/1555035007057/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey7/student</span>:<span class="string">name/1555035007062/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey8/student</span>:<span class="string">name/1555035007068/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey9/student</span>:<span class="string">name/1555035007073/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="attr">--------------------</span></span><br><span class="line"><span class="meta">rowKey0/student</span>:<span class="string">name/1555035006994/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey1/student</span>:<span class="string">name/1555035007019/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey2/student</span>:<span class="string">name/1555035007025/Put/vlen=8/seqid=0</span></span><br><span class="line"><span class="meta">rowKey3/student</span>:<span class="string">name/1555035007037/Put/vlen=8/seqid=0</span></span><br></pre></td></tr></table></figure><p>可以看到被包装后，只返回了 <code>rowKey4</code> 之前的数据。</p><h2 id="六、FilterList"><a href="#六、FilterList" class="headerlink" title="六、FilterList"></a>六、FilterList</h2><p>以上都是讲解单个过滤器的作用，当需要多个过滤器共同作用于一次查询的时候，就需要使用 <code>FilterList</code>。<code>FilterList</code> 支持通过构造器或者 <code>addFilter</code> 方法传入多个过滤器。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 构造器传入</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FilterList</span><span class="params">(<span class="keyword">final</span> Operator operator, <span class="keyword">final</span> List&lt;Filter&gt; filters)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FilterList</span><span class="params">(<span class="keyword">final</span> List&lt;Filter&gt; filters)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">FilterList</span><span class="params">(<span class="keyword">final</span> Filter... filters)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">// 方法传入</span></span></span><br><span class="line"><span class="function"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFilter</span><span class="params">(List&lt;Filter&gt; filters)</span></span></span><br><span class="line"><span class="function"> <span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addFilter</span><span class="params">(Filter filter)</span></span></span><br></pre></td></tr></table></figure><p>多个过滤器组合的结果由 <code>operator</code> 参数定义 ，其可选参数定义在 <code>Operator</code> 枚举类中。只有 <code>MUST_PASS_ALL</code> 和 <code>MUST_PASS_ONE</code> 两个可选的值：</p><ul><li><strong>MUST_PASS_ALL</strong> ：相当于 AND，必须所有的过滤器都通过才认为通过；</li><li><strong>MUST_PASS_ONE</strong> ：相当于 OR，只有要一个过滤器通过则认为通过。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Public</span><br><span class="line">  <span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">Operator</span> </span>&#123;</span><br><span class="line">    <span class="comment">/** !AND */</span></span><br><span class="line">    MUST_PASS_ALL,</span><br><span class="line">    <span class="comment">/** !OR */</span></span><br><span class="line">    MUST_PASS_ONE</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>使用示例如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">List&lt;Filter&gt; filters = <span class="keyword">new</span> ArrayList&lt;Filter&gt;();</span><br><span class="line"></span><br><span class="line">Filter filter1 = <span class="keyword">new</span> RowFilter(CompareOperator.GREATER_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">&quot;XXX&quot;</span>)));</span><br><span class="line">filters.add(filter1);</span><br><span class="line"></span><br><span class="line">Filter filter2 = <span class="keyword">new</span> RowFilter(CompareOperator.LESS_OR_EQUAL,</span><br><span class="line">                               <span class="keyword">new</span> BinaryComparator(Bytes.toBytes(<span class="string">&quot;YYY&quot;</span>)));</span><br><span class="line">filters.add(filter2);</span><br><span class="line"></span><br><span class="line">Filter filter3 = <span class="keyword">new</span> QualifierFilter(CompareOperator.EQUAL,</span><br><span class="line">                                     <span class="keyword">new</span> RegexStringComparator(<span class="string">&quot;ZZZ&quot;</span>));</span><br><span class="line">filters.add(filter3);</span><br><span class="line"></span><br><span class="line">FilterList filterList = <span class="keyword">new</span> FilterList(filters);</span><br><span class="line"></span><br><span class="line">Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">scan.setFilter(filterList);</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.oreilly.com/library/view/hbase-the-definitive/9781449314682/ch04.html">HBase: The Definitive Guide _&gt;  Chapter 4. Client API: Advanced Features</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase JAVA API使用</title>
      <link href="/2021/10/25/Hbase%20JAVA%20API%E4%BD%BF%E7%94%A8/"/>
      <url>/2021/10/25/Hbase%20JAVA%20API%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="HBase-Java-API-的基本使用"><a href="#HBase-Java-API-的基本使用" class="headerlink" title="HBase Java API 的基本使用"></a>HBase Java API 的基本使用</h1><h2 id="一、简述"><a href="#一、简述" class="headerlink" title="一、简述"></a>一、简述</h2><p>截至到目前 (2019.04)，HBase 有两个主要的版本，分别是 1.x 和 2.x ，两个版本的 Java API 有所不同，1.x 中某些方法在 2.x 中被标识为 <code>@deprecated</code> 过时。所以下面关于 API 的样例，我会分别给出 1.x 和 2.x 两个版本。完整的代码见本仓库：</p><blockquote><ul><li><p><a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Hbase/hbase-java-api-1.x">Java API 1.x Examples</a></p></li><li><p><a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Hbase/hbase-java-api-2.x">Java API 2.x Examples</a></p></li></ul></blockquote><p>同时你使用的客户端的版本必须与服务端版本保持一致，如果用 2.x 版本的客户端代码去连接 1.x 版本的服务端，会抛出 <code>NoSuchColumnFamilyException</code> 等异常。</p><h2 id="二、Java-API-1-x-基本使用"><a href="#二、Java-API-1-x-基本使用" class="headerlink" title="二、Java API 1.x 基本使用"></a>二、Java API 1.x 基本使用</h2><h4 id="2-1-新建Maven工程，导入项目依赖"><a href="#2-1-新建Maven工程，导入项目依赖" class="headerlink" title="2.1 新建Maven工程，导入项目依赖"></a>2.1 新建Maven工程，导入项目依赖</h4><p>要使用 Java API 操作 HBase，需要引入 <code>hbase-client</code>。这里选取的 <code>HBase Client</code> 的版本为 <code>1.2.0</code>。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="2-2-API-基本使用"><a href="#2-2-API-基本使用" class="headerlink" title="2.2 API 基本使用"></a>2.2 API 基本使用</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">        <span class="comment">// 如果是集群 则主机名用逗号分隔</span></span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建 HBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName      表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilies 列族的数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">createTable</span><span class="params">(String tableName, List&lt;String&gt; columnFamilies)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="keyword">if</span> (admin.tableExists(tableName)) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            HTableDescriptor tableDescriptor = <span class="keyword">new</span> HTableDescriptor(TableName.valueOf(tableName));</span><br><span class="line">            columnFamilies.forEach(columnFamily -&gt; &#123;</span><br><span class="line">                HColumnDescriptor columnDescriptor = <span class="keyword">new</span> HColumnDescriptor(columnFamily);</span><br><span class="line">                columnDescriptor.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">                tableDescriptor.addFamily(columnDescriptor);</span><br><span class="line">            &#125;);</span><br><span class="line">            admin.createTable(tableDescriptor);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除 hBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteTable</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="comment">// 删除表前需要先禁用表</span></span><br><span class="line">            admin.disableTable(tableName);</span><br><span class="line">            admin.deleteTable(tableName);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier        列标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value            数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, String qualifier,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 String value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">            put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> pairList         列标识和值的集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">            pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue())));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据 rowKey 获取指定行的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Result <span class="title">getRow</span><span class="params">(String tableName, String rowKey)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">return</span> table.get(get);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取指定行指定列 (cell) 的最新版本的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName    表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey       唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamily 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier    列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getCell</span><span class="params">(String tableName, String rowKey, String columnFamily, String qualifier)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">if</span> (!get.isCheckExistenceOnly()) &#123;</span><br><span class="line">                get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                Result result = table.get(get);</span><br><span class="line">                <span class="keyword">byte</span>[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                <span class="keyword">return</span> Bytes.toString(resultValue);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索全表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList 过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName, FilterList filterList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName   表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> startRowKey 起始 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRowKey   终止 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList  过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName, String startRowKey, String endRowKey,</span></span></span><br><span class="line"><span class="params"><span class="function">                                           FilterList filterList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            scan.setStartRow(Bytes.toBytes(startRowKey));</span><br><span class="line">            scan.setStopRow(Bytes.toBytes(endRowKey));</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行记录</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteRow</span><span class="params">(String tableName, String rowKey)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行的指定列</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey     唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> familyName 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier  列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteColumn</span><span class="params">(String tableName, String rowKey, String familyName,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          String qualifier)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">            delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-单元测试"><a href="#2-3-单元测试" class="headerlink" title="2.3 单元测试"></a>2.3 单元测试</h3><p>以单元测试的方式对上面封装的 API 进行测试。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseUtilsTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TABLE_NAME = <span class="string">&quot;class&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String TEACHER = <span class="string">&quot;teacher&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String STUDENT = <span class="string">&quot;student&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 新建表</span></span><br><span class="line">        List&lt;String&gt; columnFamilies = Arrays.asList(TEACHER, STUDENT);</span><br><span class="line">        <span class="keyword">boolean</span> table = HBaseUtils.createTable(TABLE_NAME, columnFamilies);</span><br><span class="line">        System.out.println(<span class="string">&quot;表创建结果:&quot;</span> + table);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insertData</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs1 = Arrays.asList(<span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;Tom&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;22&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;gender&quot;</span>, <span class="string">&quot;1&quot;</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">&quot;rowKey1&quot;</span>, STUDENT, pairs1);</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs2 = Arrays.asList(<span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;Jack&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;33&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;gender&quot;</span>, <span class="string">&quot;2&quot;</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">&quot;rowKey2&quot;</span>, STUDENT, pairs2);</span><br><span class="line"></span><br><span class="line">        List&lt;Pair&lt;String, String&gt;&gt; pairs3 = Arrays.asList(<span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;Mike&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;44&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> Pair&lt;&gt;(<span class="string">&quot;gender&quot;</span>, <span class="string">&quot;1&quot;</span>));</span><br><span class="line">        HBaseUtils.putRow(TABLE_NAME, <span class="string">&quot;rowKey3&quot;</span>, STUDENT, pairs3);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getRow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Result result = HBaseUtils.getRow(TABLE_NAME, <span class="string">&quot;rowKey1&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (result != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.println(Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">&quot;name&quot;</span>))));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getCell</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String cell = HBaseUtils.getCell(TABLE_NAME, <span class="string">&quot;rowKey2&quot;</span>, STUDENT, <span class="string">&quot;age&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;cell age :&quot;</span> + cell);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getScanner</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME);</span><br><span class="line">        <span class="keyword">if</span> (scanner != <span class="keyword">null</span>) &#123;</span><br><span class="line">            scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + <span class="string">&quot;-&gt;&quot;</span> + Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">&quot;name&quot;</span>)))));</span><br><span class="line">            scanner.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getScannerWithFilter</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        FilterList filterList = <span class="keyword">new</span> FilterList(FilterList.Operator.MUST_PASS_ALL);</span><br><span class="line">        SingleColumnValueFilter nameFilter = <span class="keyword">new</span> SingleColumnValueFilter(Bytes.toBytes(STUDENT),</span><br><span class="line">                Bytes.toBytes(<span class="string">&quot;name&quot;</span>), CompareOperator.EQUAL, Bytes.toBytes(<span class="string">&quot;Jack&quot;</span>));</span><br><span class="line">        filterList.addFilter(nameFilter);</span><br><span class="line">        ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME, filterList);</span><br><span class="line">        <span class="keyword">if</span> (scanner != <span class="keyword">null</span>) &#123;</span><br><span class="line">            scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + <span class="string">&quot;-&gt;&quot;</span> + Bytes</span><br><span class="line">                    .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(<span class="string">&quot;name&quot;</span>)))));</span><br><span class="line">            scanner.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteColumn</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> b = HBaseUtils.deleteColumn(TABLE_NAME, <span class="string">&quot;rowKey2&quot;</span>, STUDENT, <span class="string">&quot;age&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;删除结果: &quot;</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteRow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> b = HBaseUtils.deleteRow(TABLE_NAME, <span class="string">&quot;rowKey2&quot;</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;删除结果: &quot;</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">deleteTable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> b = HBaseUtils.deleteTable(TABLE_NAME);</span><br><span class="line">        System.out.println(<span class="string">&quot;删除结果: &quot;</span> + b);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="三、Java-API-2-x-基本使用"><a href="#三、Java-API-2-x-基本使用" class="headerlink" title="三、Java API 2.x 基本使用"></a>三、Java API 2.x 基本使用</h2><h4 id="3-1-新建Maven工程，导入项目依赖"><a href="#3-1-新建Maven工程，导入项目依赖" class="headerlink" title="3.1 新建Maven工程，导入项目依赖"></a>3.1 新建Maven工程，导入项目依赖</h4><p>这里选取的 <code>HBase Client</code> 的版本为最新的 <code>2.1.4</code>。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hbase<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hbase-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-2-API-的基本使用"><a href="#3-2-API-的基本使用" class="headerlink" title="3.2 API 的基本使用"></a>3.2 API 的基本使用</h4><p>2.x 版本相比于 1.x 废弃了一部分方法，关于废弃的方法在源码中都会指明新的替代方法，比如，在 2.x 中创建表时：<code>HTableDescriptor</code> 和 <code>HColumnDescriptor</code> 等类都标识为废弃，取而代之的是使用 <code>TableDescriptorBuilder</code> 和 <code>ColumnFamilyDescriptorBuilder</code> 来定义表和列族。</p><div align="center"> <img width="700px"  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/deprecated.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/deprecated.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p>以下为 HBase  2.x 版本 Java API 的使用示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseUtils</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        Configuration configuration = HBaseConfiguration.create();</span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.property.clientPort&quot;</span>, <span class="string">&quot;2181&quot;</span>);</span><br><span class="line">        <span class="comment">// 如果是集群 则主机名用逗号分隔</span></span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.zookeeper.quorum&quot;</span>, <span class="string">&quot;hadoop001&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建 HBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName      表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilies 列族的数组</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">createTable</span><span class="params">(String tableName, List&lt;String&gt; columnFamilies)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="keyword">if</span> (admin.tableExists(TableName.valueOf(tableName))) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName));</span><br><span class="line">            columnFamilies.forEach(columnFamily -&gt; &#123;</span><br><span class="line">                ColumnFamilyDescriptorBuilder cfDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(columnFamily));</span><br><span class="line">                cfDescriptorBuilder.setMaxVersions(<span class="number">1</span>);</span><br><span class="line">                ColumnFamilyDescriptor familyDescriptor = cfDescriptorBuilder.build();</span><br><span class="line">                tableDescriptor.setColumnFamily(familyDescriptor);</span><br><span class="line">            &#125;);</span><br><span class="line">            admin.createTable(tableDescriptor.build());</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除 hBase 表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteTable</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            HBaseAdmin admin = (HBaseAdmin) connection.getAdmin();</span><br><span class="line">            <span class="comment">// 删除表前需要先禁用表</span></span><br><span class="line">            admin.disableTable(TableName.valueOf(tableName));</span><br><span class="line">            admin.deleteTable(TableName.valueOf(tableName));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier        列标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value            数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, String qualifier,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 String value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">            put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 插入数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName        表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey           唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamilyName 列族名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> pairList         列标识和值的集合</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">putRow</span><span class="params">(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Put put = <span class="keyword">new</span> Put(Bytes.toBytes(rowKey));</span><br><span class="line">            pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue())));</span><br><span class="line">            table.put(put);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据 rowKey 获取指定行的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Result <span class="title">getRow</span><span class="params">(String tableName, String rowKey)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">return</span> table.get(get);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 获取指定行指定列 (cell) 的最新版本的数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName    表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey       唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> columnFamily 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier    列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">getCell</span><span class="params">(String tableName, String rowKey, String columnFamily, String qualifier)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Get get = <span class="keyword">new</span> Get(Bytes.toBytes(rowKey));</span><br><span class="line">            <span class="keyword">if</span> (!get.isCheckExistenceOnly()) &#123;</span><br><span class="line">                get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                Result result = table.get(get);</span><br><span class="line">                <span class="keyword">byte</span>[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier));</span><br><span class="line">                <span class="keyword">return</span> Bytes.toString(resultValue);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索全表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList 过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName, FilterList filterList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 检索表中指定数据</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName   表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> startRowKey 起始 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRowKey   终止 RowKey</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> filterList  过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ResultScanner <span class="title">getScanner</span><span class="params">(String tableName, String startRowKey, String endRowKey,</span></span></span><br><span class="line"><span class="params"><span class="function">                                           FilterList filterList)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Scan scan = <span class="keyword">new</span> Scan();</span><br><span class="line">            scan.withStartRow(Bytes.toBytes(startRowKey));</span><br><span class="line">            scan.withStopRow(Bytes.toBytes(endRowKey));</span><br><span class="line">            scan.setFilter(filterList);</span><br><span class="line">            <span class="keyword">return</span> table.getScanner(scan);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行记录</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName 表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey    唯一标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteRow</span><span class="params">(String tableName, String rowKey)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 删除指定行指定列</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> tableName  表名</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> rowKey     唯一标识</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> familyName 列族</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> qualifier  列标识</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">deleteColumn</span><span class="params">(String tableName, String rowKey, String familyName,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          String qualifier)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">            Delete delete = <span class="keyword">new</span> Delete(Bytes.toBytes(rowKey));</span><br><span class="line">            delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier));</span><br><span class="line">            table.delete(delete);</span><br><span class="line">            table.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="四、正确连接Hbase"><a href="#四、正确连接Hbase" class="headerlink" title="四、正确连接Hbase"></a>四、正确连接Hbase</h2><p>在上面的代码中，在类加载时就初始化了 Connection 连接，并且之后的方法都是复用这个 Connection，这时我们可能会考虑是否可以使用自定义连接池来获取更好的性能表现？实际上这是没有必要的。</p><p>首先官方对于 <code>Connection</code> 的使用说明如下：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Connection</span> <span class="string">Pooling For applications which require high-end multithreaded   </span></span><br><span class="line"><span class="attr">access</span> <span class="string">(e.g., web-servers or  application servers  that may serve many   </span></span><br><span class="line"><span class="attr">application</span> <span class="string">threads in a single JVM), you can pre-create a Connection,   </span></span><br><span class="line"><span class="attr">as</span> <span class="string">shown in the following example:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">对于高并发多线程访问的应用程序（例如，在单个</span> <span class="string">JVM 中存在的为多个线程服务的 Web 服务器或应用程序服务器），  </span></span><br><span class="line"><span class="meta">您只需要预先创建一个</span> <span class="string">Connection。例子如下：</span></span><br><span class="line"></span><br><span class="line"><span class="meta">//</span> <span class="string">Create a connection to the cluster.</span></span><br><span class="line"><span class="attr">Configuration</span> <span class="string">conf = HBaseConfiguration.create();</span></span><br><span class="line"><span class="attr">try</span> <span class="string">(Connection connection = ConnectionFactory.createConnection(conf);</span></span><br><span class="line">     <span class="attr">Table</span> <span class="string">table = connection.getTable(TableName.valueOf(tablename))) &#123;</span></span><br><span class="line">  <span class="meta">//</span> <span class="string">use table as needed, the table returned is lightweight</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure><p>之所以能这样使用，这是因为 Connection 并不是一个简单的 socket 连接，<a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Connection.html">接口文档</a> 中对 Connection 的表述是：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">A</span> <span class="string">cluster connection encapsulating lower level individual connections to actual servers and a  </span></span><br><span class="line"><span class="attr">connection</span> <span class="string">to zookeeper.  Connections are instantiated through the ConnectionFactory class.  </span></span><br><span class="line"><span class="attr">The</span> <span class="string">lifecycle of the connection is managed by the caller,  who has to close() the connection   </span></span><br><span class="line"><span class="attr">to</span> <span class="string">release the resources. </span></span><br><span class="line"></span><br><span class="line"><span class="attr">Connection</span> <span class="string">是一个集群连接，封装了与多台服务器（Matser/Region Server）的底层连接以及与 zookeeper 的连接。  </span></span><br><span class="line"><span class="meta">连接通过</span> <span class="string">ConnectionFactory  类实例化。连接的生命周期由调用者管理，调用者必须使用 close() 关闭连接以释放资源。</span></span><br></pre></td></tr></table></figure><p>之所以封装这些连接，是因为 HBase 客户端需要连接三个不同的服务角色：</p><ul><li><strong>Zookeeper</strong> ：主要用于获取 <code>meta</code> 表的位置信息，Master 的信息；</li><li><strong>HBase Master</strong> ：主要用于执行 HBaseAdmin 接口的一些操作，例如建表等；</li><li><strong>HBase RegionServer</strong> ：用于读、写数据。</li></ul><div align="center"> <img width="700px"  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-arc.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-arc.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p>Connection 对象和实际的 Socket 连接之间的对应关系如下图：</p><div align="center"> <img width="700px"   src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-connection.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-connection.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><blockquote><p>上面两张图片引用自博客：<a href="https://yq.aliyun.com/articles/581702?spm=a2c4e.11157919.spm-cont-list.1.146c27aeFxoMsN%20%E8%BF%9E%E6%8E%A5HBase%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF">连接 HBase 的正确姿势</a></p></blockquote><p>在 HBase 客户端代码中，真正对应 Socket 连接的是 <code>RpcConnection</code> 对象。HBase 使用 <code>PoolMap</code> 这种数据结构来存储客户端到 HBase 服务器之间的连接。<code>PoolMap</code> 的内部有一个 <code>ConcurrentHashMap</code> 实例，其 key 是 <code>ConnectionId</code>(封装了服务器地址和用户 ticket)，value 是一个 <code>RpcConnection</code> 对象的资源池。当 HBase 需要连接一个服务器时，首先会根据 <code>ConnectionId</code> 找到对应的连接池，然后从连接池中取出一个连接对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@InterfaceAudience</span>.Private</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PoolMap</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> PoolType poolType;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> poolMaxSize;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> Map&lt;K, Pool&lt;V&gt;&gt; pools = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">PoolMap</span><span class="params">(PoolType poolType)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.poolType = poolType;</span><br><span class="line">  &#125;</span><br><span class="line">  .....</span><br></pre></td></tr></table></figure><p>HBase 中提供了三种资源池的实现，分别是 <code>Reusable</code>，<code>RoundRobin</code> 和 <code>ThreadLocal</code>。具体实现可以通 <code>hbase.client.ipc.pool.type</code> 配置项指定，默认为 <code>Reusable</code>。连接池的大小也可以通过 <code>hbase.client.ipc.pool.size</code> 配置项指定，默认为 1，即每个 Server 1 个连接。也可以通过修改配置实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config.set(<span class="string">&quot;hbase.client.ipc.pool.type&quot;</span>,...);</span><br><span class="line">config.set(<span class="string">&quot;hbase.client.ipc.pool.size&quot;</span>,...);</span><br><span class="line">connection = ConnectionFactory.createConnection(config);</span><br></pre></td></tr></table></figure><p>由此可以看出 HBase 中 Connection 类已经实现了对连接的管理功能，所以我们不必在 Connection 上在做额外的管理。</p><p>另外，Connection 是线程安全的，但 Table 和 Admin 却不是线程安全的，因此正确的做法是一个进程共用一个 Connection 对象，而在不同的线程中使用单独的 Table 和 Admin 对象。Table 和 Admin 的获取操作 <code>getTable()</code> 和 <code>getAdmin()</code> 都是轻量级，所以不必担心性能的消耗，同时建议在使用完成后显示的调用 <code>close()</code> 方法来关闭它们。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://yq.aliyun.com/articles/581702?spm=a2c4e.11157919.spm-cont-list.1.146c27aeFxoMsN%20%E8%BF%9E%E6%8E%A5HBase%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF">连接 HBase 的正确姿势</a></li><li><a href="http://hbase.apache.org/book.htm">Apache HBase ™ Reference Guide</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring整合Mybatis+Phoenix(附springBoot)</title>
      <link href="/2021/10/25/SpringBoot%E6%95%B4%E5%90%88Mybatis+Phoenix/"/>
      <url>/2021/10/25/SpringBoot%E6%95%B4%E5%90%88Mybatis+Phoenix/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring整合Mybatis-Phoenix-附springBoot"><a href="#Spring整合Mybatis-Phoenix-附springBoot" class="headerlink" title="Spring整合Mybatis+Phoenix(附springBoot)"></a>Spring整合Mybatis+Phoenix(附springBoot)</h1><h2 id="一、前言"><a href="#一、前言" class="headerlink" title="一、前言"></a>一、前言</h2><p>使用 Spring+Mybatis 操作 Phoenix 和操作其他的关系型数据库（如 Mysql，Oracle）在配置上是基本相同的，下面会分别给出 Spring/Spring Boot 整合步骤，完整代码见本仓库：</p><ul><li><a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Phoenix/spring-mybatis-phoenix">Spring + Mybatis + Phoenix</a></li><li><a href="https://github.com/ihadyou/BigData-Notes/tree/master/code/Phoenix/spring-boot-mybatis-phoenix">SpringBoot + Mybatis + Phoenix</a></li></ul><h2 id="二、Spring-Mybatis-Phoenix"><a href="#二、Spring-Mybatis-Phoenix" class="headerlink" title="二、Spring + Mybatis + Phoenix"></a>二、Spring + Mybatis + Phoenix</h2><h3 id="2-1-项目结构"><a href="#2-1-项目结构" class="headerlink" title="2.1 项目结构"></a>2.1 项目结构</h3><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/spring-mybatis-phoenix.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/spring-mybatis-phoenix.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="2-2-主要依赖"><a href="#2-2-主要依赖" class="headerlink" title="2.2 主要依赖"></a>2.2 主要依赖</h3><p>除了 Spring 相关依赖外，还需要导入 <code>phoenix-core</code> 和对应的 Mybatis 依赖包</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--mybatis 依赖包--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--phoenix core--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-3-数据库配置文件"><a href="#2-3-数据库配置文件" class="headerlink" title="2.3  数据库配置文件"></a>2.3  数据库配置文件</h3><p>在数据库配置文件 <code>jdbc.properties</code>  中配置数据库驱动和 zookeeper 地址</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据库驱动</span></span><br><span class="line"><span class="meta">phoenix.driverClassName</span>=<span class="string">org.apache.phoenix.jdbc.PhoenixDriver</span></span><br><span class="line"><span class="comment"># zookeeper地址</span></span><br><span class="line"><span class="meta">phoenix.url</span>=<span class="string">jdbc:phoenix:192.168.0.105:2181</span></span><br></pre></td></tr></table></figure><h3 id="2-4-配置数据源和会话工厂"><a href="#2-4-配置数据源和会话工厂" class="headerlink" title="2.4  配置数据源和会话工厂"></a>2.4  配置数据源和会话工厂</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">beans</span> <span class="attr">xmlns</span>=<span class="string">&quot;http://www.springframework.org/schema/beans&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:xsi</span>=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xmlns:context</span>=<span class="string">&quot;http://www.springframework.org/schema/context&quot;</span> <span class="attr">xmlns:tx</span>=<span class="string">&quot;http://www.springframework.org/schema/tx&quot;</span></span></span><br><span class="line"><span class="tag">       <span class="attr">xsi:schemaLocation</span>=<span class="string">&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd</span></span></span><br><span class="line"><span class="string"><span class="tag">        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 开启注解包扫描--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">context:component-scan</span> <span class="attr">base-package</span>=<span class="string">&quot;com.ihadyou.*&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--指定配置文件的位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">context:property-placeholder</span> <span class="attr">location</span>=<span class="string">&quot;classpath:jdbc.properties&quot;</span>/&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置数据源--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;dataSource&quot;</span> <span class="attr">class</span>=<span class="string">&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--Phoenix 配置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;driverClassName&quot;</span> <span class="attr">value</span>=<span class="string">&quot;$&#123;phoenix.driverClassName&#125;&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;url&quot;</span> <span class="attr">value</span>=<span class="string">&quot;$&#123;phoenix.url&#125;&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--配置 mybatis 会话工厂 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">&quot;sqlSessionFactory&quot;</span> <span class="attr">class</span>=<span class="string">&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;dataSource&quot;</span> <span class="attr">ref</span>=<span class="string">&quot;dataSource&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定 mapper 文件所在的位置--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;mapperLocations&quot;</span> <span class="attr">value</span>=<span class="string">&quot;classpath*:/mappers/**/*.xml&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;configLocation&quot;</span> <span class="attr">value</span>=<span class="string">&quot;classpath:mybatisConfig.xml&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--扫描注册接口 --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--作用:从接口的基础包开始递归搜索，并将它们注册为 MapperFactoryBean(只有至少一种方法的接口才会被注册;, 具体类将被忽略)--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">class</span>=<span class="string">&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--指定会话工厂 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;sqlSessionFactoryBeanName&quot;</span> <span class="attr">value</span>=<span class="string">&quot;sqlSessionFactory&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 mybatis 接口所在的包 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">&quot;basePackage&quot;</span> <span class="attr">value</span>=<span class="string">&quot;com.ihadyou.dao&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">bean</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-5-Mybtais参数配置"><a href="#2-5-Mybtais参数配置" class="headerlink" title="2.5 Mybtais参数配置"></a>2.5 Mybtais参数配置</h3><p>新建 mybtais 配置文件，按照需求配置额外参数， 更多 settings 配置项可以参考<a href="http://www.mybatis.org/mybatis-3/zh/configuration.html">官方文档</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;</span></span><br><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">configuration</span></span></span><br><span class="line"><span class="meta">        <span class="meta-keyword">PUBLIC</span> <span class="meta-string">&quot;-//mybatis.org//DTD Config 3.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">        <span class="meta-string">&quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- mybatis 配置文件 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">settings</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 开启驼峰命名 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">&quot;mapUnderscoreToCamelCase&quot;</span> <span class="attr">value</span>=<span class="string">&quot;true&quot;</span>/&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 打印查询 sql --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">setting</span> <span class="attr">name</span>=<span class="string">&quot;logImpl&quot;</span> <span class="attr">value</span>=<span class="string">&quot;STDOUT_LOGGING&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">settings</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-6-查询接口"><a href="#2-6-查询接口" class="headerlink" title="2.6 查询接口"></a>2.6 查询接口</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">PopulationDao</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function">List&lt;USPopulation&gt; <span class="title">queryAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">save</span><span class="params">(USPopulation USPopulation)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function">USPopulation <span class="title">queryByStateAndCity</span><span class="params">(<span class="meta">@Param(&quot;state&quot;)</span> String state, <span class="meta">@Param(&quot;city&quot;)</span> String city)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">deleteByStateAndCity</span><span class="params">(<span class="meta">@Param(&quot;state&quot;)</span> String state, <span class="meta">@Param(&quot;city&quot;)</span> String city)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">mapper</span></span></span><br><span class="line"><span class="meta">        <span class="meta-keyword">PUBLIC</span> <span class="meta-string">&quot;-//mybatis.org//DTD Mapper 3.0//EN&quot;</span></span></span><br><span class="line"><span class="meta">        <span class="meta-string">&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">mapper</span> <span class="attr">namespace</span>=<span class="string">&quot;com.ihadyou.dao.PopulationDao&quot;</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;queryAll&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.ihadyou.bean.USPopulation&quot;</span>&gt;</span></span><br><span class="line">        SELECT * FROM us_population</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">insert</span> <span class="attr">id</span>=<span class="string">&quot;save&quot;</span>&gt;</span></span><br><span class="line">        UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; )</span><br><span class="line">    <span class="tag">&lt;/<span class="name">insert</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">select</span> <span class="attr">id</span>=<span class="string">&quot;queryByStateAndCity&quot;</span> <span class="attr">resultType</span>=<span class="string">&quot;com.ihadyou.bean.USPopulation&quot;</span>&gt;</span></span><br><span class="line">        SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">select</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">delete</span> <span class="attr">id</span>=<span class="string">&quot;deleteByStateAndCity&quot;</span>&gt;</span></span><br><span class="line">        DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">delete</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">mapper</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="2-7-单元测试"><a href="#2-7-单元测试" class="headerlink" title="2.7 单元测试"></a>2.7 单元测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@ContextConfiguration(&#123;&quot;classpath:springApplication.xml&quot;&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PopulationDaoTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> PopulationDao populationDao;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">queryAll</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll();</span><br><span class="line">        <span class="keyword">if</span> (USPopulationList != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (USPopulation USPopulation : USPopulationList) &#123;</span><br><span class="line">                System.out.println(USPopulation.getCity() + <span class="string">&quot; &quot;</span> + USPopulation.getPopulation());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">save</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> USPopulation(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>, <span class="number">66666</span>));</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> USPopulation(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>, <span class="number">99999</span>));</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.deleteByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="三、SpringBoot-Mybatis-Phoenix"><a href="#三、SpringBoot-Mybatis-Phoenix" class="headerlink" title="三、SpringBoot + Mybatis + Phoenix"></a>三、SpringBoot + Mybatis + Phoenix</h2><h3 id="3-1-项目结构"><a href="#3-1-项目结构" class="headerlink" title="3.1 项目结构"></a>3.1 项目结构</h3><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/spring-boot-mybatis-phoenix.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/spring-boot-mybatis-phoenix.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="3-2-主要依赖"><a href="#3-2-主要依赖" class="headerlink" title="3.2 主要依赖"></a>3.2 主要依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--spring 1.5 x 以上版本对应 mybatis 1.3.x (1.3.1)</span></span><br><span class="line"><span class="comment">        关于更多 spring-boot 与 mybatis 的版本对应可以参见 &lt;a href=&quot;http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/&quot;&gt;--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--phoenix core--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.phoenix<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>phoenix-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.14.0-cdh5.14.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>spring boot 与 mybatis 版本的对应关系：</p><table><thead><tr><th>MyBatis-Spring-Boot-Starter 版本</th><th>MyBatis-Spring 版本</th><th>Spring Boot 版本</th></tr></thead><tbody><tr><td><strong>1.3.x (1.3.1)</strong></td><td>1.3 or higher</td><td>1.5 or higher</td></tr><tr><td><strong>1.2.x (1.2.1)</strong></td><td>1.3 or higher</td><td>1.4 or higher</td></tr><tr><td><strong>1.1.x (1.1.1)</strong></td><td>1.3 or higher</td><td>1.3 or higher</td></tr><tr><td><strong>1.0.x (1.0.2)</strong></td><td>1.2 or higher</td><td>1.3 or higher</td></tr></tbody></table><h3 id="3-3-配置数据源"><a href="#3-3-配置数据源" class="headerlink" title="3.3 配置数据源"></a>3.3 配置数据源</h3><p>在 application.yml 中配置数据源，spring boot 2.x 版本默认采用 Hikari 作为数据库连接池，Hikari 是目前 java 平台性能最好的连接池，性能好于 druid。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">datasource:</span></span><br><span class="line">    <span class="comment">#zookeeper 地址</span></span><br><span class="line">    <span class="attr">url:</span> <span class="string">jdbc:phoenix:192.168.0.105:2181</span></span><br><span class="line">    <span class="attr">driver-class-name:</span> <span class="string">org.apache.phoenix.jdbc.PhoenixDriver</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果不想配置对数据库连接池做特殊配置的话,以下关于连接池的配置就不是必须的</span></span><br><span class="line">    <span class="comment"># spring-boot 2.X 默认采用高性能的 Hikari 作为连接池 更多配置可以参考 https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">com.zaxxer.hikari.HikariDataSource</span></span><br><span class="line">    <span class="attr">hikari:</span></span><br><span class="line">      <span class="comment"># 池中维护的最小空闲连接数</span></span><br><span class="line">      <span class="attr">minimum-idle:</span> <span class="number">10</span></span><br><span class="line">      <span class="comment"># 池中最大连接数，包括闲置和使用中的连接</span></span><br><span class="line">      <span class="attr">maximum-pool-size:</span> <span class="number">20</span></span><br><span class="line">      <span class="comment"># 此属性控制从池返回的连接的默认自动提交行为。默认为 true</span></span><br><span class="line">      <span class="attr">auto-commit:</span> <span class="literal">true</span></span><br><span class="line">      <span class="comment"># 允许最长空闲时间</span></span><br><span class="line">      <span class="attr">idle-timeout:</span> <span class="number">30000</span></span><br><span class="line">      <span class="comment"># 此属性表示连接池的用户定义名称，主要显示在日志记录和 JMX 管理控制台中，以标识池和池配置。 默认值：自动生成</span></span><br><span class="line">      <span class="attr">pool-name:</span> <span class="string">custom-hikari</span></span><br><span class="line">      <span class="comment">#此属性控制池中连接的最长生命周期，值 0 表示无限生命周期，默认 1800000 即 30 分钟</span></span><br><span class="line">      <span class="attr">max-lifetime:</span> <span class="number">1800000</span></span><br><span class="line">      <span class="comment"># 数据库连接超时时间,默认 30 秒，即 30000</span></span><br><span class="line">      <span class="attr">connection-timeout:</span> <span class="number">30000</span></span><br><span class="line">      <span class="comment"># 连接测试 sql 这个地方需要根据数据库方言差异而配置 例如 oracle 就应该写成  select 1 from dual</span></span><br><span class="line">      <span class="attr">connection-test-query:</span> <span class="string">SELECT</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mybatis 相关配置</span></span><br><span class="line"><span class="attr">mybatis:</span></span><br><span class="line">  <span class="attr">configuration:</span></span><br><span class="line">    <span class="comment"># 是否打印 sql 语句 调试的时候可以开启</span></span><br><span class="line">    <span class="attr">log-impl:</span> <span class="string">org.apache.ibatis.logging.stdout.StdOutImpl</span></span><br></pre></td></tr></table></figure><h3 id="3-4-新建查询接口"><a href="#3-4-新建查询接口" class="headerlink" title="3.4 新建查询接口"></a>3.4 新建查询接口</h3><p>上面 Spring+Mybatis 我们使用了 XML 的方式来写 SQL，为了体现 Mybatis 支持多种方式，这里使用注解的方式来写 SQL。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Mapper</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">PopulationDao</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Select(&quot;SELECT * from us_population&quot;)</span></span><br><span class="line">    <span class="function">List&lt;USPopulation&gt; <span class="title">queryAll</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Insert(&quot;UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; )&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">save</span><span class="params">(USPopulation USPopulation)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Select(&quot;SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;&quot;)</span></span><br><span class="line">    <span class="function">USPopulation <span class="title">queryByStateAndCity</span><span class="params">(String state, String city)</span></span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Delete(&quot;DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">deleteByStateAndCity</span><span class="params">(String state, String city)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-5-单元测试"><a href="#3-5-单元测试" class="headerlink" title="3.5 单元测试"></a>3.5 单元测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@SpringBootTest</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PopulationTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> PopulationDao populationDao;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">queryAll</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll();</span><br><span class="line">        <span class="keyword">if</span> (USPopulationList != <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (USPopulation USPopulation : USPopulationList) &#123;</span><br><span class="line">                System.out.println(USPopulation.getCity() + <span class="string">&quot; &quot;</span> + USPopulation.getPopulation());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">save</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> USPopulation(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>, <span class="number">66666</span>));</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.save(<span class="keyword">new</span> USPopulation(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>, <span class="number">99999</span>));</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        populationDao.deleteByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        USPopulation usPopulation = populationDao.queryByStateAndCity(<span class="string">&quot;TX&quot;</span>, <span class="string">&quot;Dallas&quot;</span>);</span><br><span class="line">        System.out.println(usPopulation);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="附：建表语句"><a href="#附：建表语句" class="headerlink" title="附：建表语句"></a>附：建表语句</h2><p>上面单元测试涉及到的测试表的建表语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> us_population (</span><br><span class="line">      state <span class="type">CHAR</span>(<span class="number">2</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">      city <span class="type">VARCHAR</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">      population <span class="type">BIGINT</span></span><br><span class="line">      <span class="keyword">CONSTRAINT</span> my_pk <span class="keyword">PRIMARY</span> KEY (state, city));</span><br><span class="line">      </span><br><span class="line"><span class="comment">-- 测试数据</span></span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;NY&#x27;</span>,<span class="string">&#x27;New York&#x27;</span>,<span class="number">8143197</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;CA&#x27;</span>,<span class="string">&#x27;Los Angeles&#x27;</span>,<span class="number">3844829</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;IL&#x27;</span>,<span class="string">&#x27;Chicago&#x27;</span>,<span class="number">2842518</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;TX&#x27;</span>,<span class="string">&#x27;Houston&#x27;</span>,<span class="number">2016582</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;PA&#x27;</span>,<span class="string">&#x27;Philadelphia&#x27;</span>,<span class="number">1463281</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;AZ&#x27;</span>,<span class="string">&#x27;Phoenix&#x27;</span>,<span class="number">1461575</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;TX&#x27;</span>,<span class="string">&#x27;San Antonio&#x27;</span>,<span class="number">1256509</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;CA&#x27;</span>,<span class="string">&#x27;San Diego&#x27;</span>,<span class="number">1255540</span>);</span><br><span class="line">UPSERT <span class="keyword">INTO</span> us_population <span class="keyword">VALUES</span>(<span class="string">&#x27;CA&#x27;</span>,<span class="string">&#x27;San Jose&#x27;</span>,<span class="number">912332</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase常用shell命令</title>
      <link href="/2021/10/25/Hbase%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/"/>
      <url>/2021/10/25/Hbase%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase-常用-Shell-命令"><a href="#Hbase-常用-Shell-命令" class="headerlink" title="Hbase 常用 Shell 命令"></a>Hbase 常用 Shell 命令</h1><h2 id="一、基本命令"><a href="#一、基本命令" class="headerlink" title="一、基本命令"></a>一、基本命令</h2><p>打开 Hbase Shell：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hbase shell</span></span><br></pre></td></tr></table></figure><h4 id="1-1-获取帮助"><a href="#1-1-获取帮助" class="headerlink" title="1.1 获取帮助"></a>1.1 获取帮助</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取帮助</span></span><br><span class="line">help</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取命令的详细信息</span></span><br><span class="line">help &#x27;status&#x27;</span><br></pre></td></tr></table></figure><h4 id="1-2-查看服务器状态"><a href="#1-2-查看服务器状态" class="headerlink" title="1.2 查看服务器状态"></a>1.2 查看服务器状态</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">status</span><br></pre></td></tr></table></figure><h4 id="1-3-查看版本信息"><a href="#1-3-查看版本信息" class="headerlink" title="1.3 查看版本信息"></a>1.3 查看版本信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">version</span><br></pre></td></tr></table></figure><h2 id="二、关于表的操作"><a href="#二、关于表的操作" class="headerlink" title="二、关于表的操作"></a>二、关于表的操作</h2><h4 id="2-1-查看所有表"><a href="#2-1-查看所有表" class="headerlink" title="2.1 查看所有表"></a>2.1 查看所有表</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list</span><br></pre></td></tr></table></figure><h4 id="2-2-创建表"><a href="#2-2-创建表" class="headerlink" title="2.2 创建表"></a>2.2 创建表</h4><p><strong>命令格式</strong>： create ‘表名称’, ‘列族名称 1’,’列族名称 2’,’列名称 N’</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建一张名为Student的表,包含基本信息（baseInfo）、学校信息（schoolInfo）两个列族</span></span><br><span class="line">create &#x27;Student&#x27;,&#x27;baseInfo&#x27;,&#x27;schoolInfo&#x27;</span><br></pre></td></tr></table></figure><h4 id="2-3-查看表的基本信息"><a href="#2-3-查看表的基本信息" class="headerlink" title="2.3 查看表的基本信息"></a>2.3 查看表的基本信息</h4><p><strong>命令格式</strong>：desc ‘表名’</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe &#x27;Student&#x27;</span><br></pre></td></tr></table></figure><h4 id="2-4-表的启用-禁用"><a href="#2-4-表的启用-禁用" class="headerlink" title="2.4 表的启用/禁用"></a>2.4 表的启用/禁用</h4><p>enable 和 disable 可以启用/禁用这个表,is_enabled 和 is_disabled 来检查表是否被禁用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 禁用表</span></span><br><span class="line">disable &#x27;Student&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查表是否被禁用</span></span><br><span class="line">is_disabled &#x27;Student&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用表</span></span><br><span class="line">enable &#x27;Student&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 检查表是否被启用</span></span><br><span class="line">is_enabled &#x27;Student&#x27;</span><br></pre></td></tr></table></figure><h4 id="2-5-检查表是否存在"><a href="#2-5-检查表是否存在" class="headerlink" title="2.5 检查表是否存在"></a>2.5 检查表是否存在</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exists &#x27;Student&#x27;</span><br></pre></td></tr></table></figure><h4 id="2-6-删除表"><a href="#2-6-删除表" class="headerlink" title="2.6 删除表"></a>2.6 删除表</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除表前需要先禁用表</span></span><br><span class="line">disable &#x27;Student&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除表</span></span><br><span class="line">drop &#x27;Student&#x27;</span><br></pre></td></tr></table></figure><h2 id="三、增删改"><a href="#三、增删改" class="headerlink" title="三、增删改"></a>三、增删改</h2><h4 id="3-1-添加列族"><a href="#3-1-添加列族" class="headerlink" title="3.1 添加列族"></a>3.1 添加列族</h4><p><strong>命令格式</strong>： alter ‘表名’, ‘列族名’</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter &#x27;Student&#x27;, &#x27;teacherInfo&#x27;</span><br></pre></td></tr></table></figure><h4 id="3-2-删除列族"><a href="#3-2-删除列族" class="headerlink" title="3.2 删除列族"></a>3.2 删除列族</h4><p><strong>命令格式</strong>：alter ‘表名’, {NAME =&gt; ‘列族名’, METHOD =&gt; ‘delete’}</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter &#x27;Student&#x27;, &#123;NAME =&gt; &#x27;teacherInfo&#x27;, METHOD =&gt; &#x27;delete&#x27;&#125;</span><br></pre></td></tr></table></figure><h4 id="3-3-更改列族存储版本的限制"><a href="#3-3-更改列族存储版本的限制" class="headerlink" title="3.3 更改列族存储版本的限制"></a>3.3 更改列族存储版本的限制</h4><p>默认情况下，列族只存储一个版本的数据，如果需要存储多个版本的数据，则需要修改列族的属性。修改后可通过 <code>desc</code> 命令查看。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter &#x27;Student&#x27;,&#123;NAME=&gt;&#x27;baseInfo&#x27;,VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure><h4 id="3-4-插入数据"><a href="#3-4-插入数据" class="headerlink" title="3.4 插入数据"></a>3.4 插入数据</h4><p><strong>命令格式</strong>：put ‘表名’, ‘行键’,’列族:列’,’值’</p><p><strong>注意：如果新增数据的行键值、列族名、列名与原有数据完全相同，则相当于更新操作</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:name&#x27;,&#x27;tom&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1990-01-09&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:age&#x27;,&#x27;29&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;Havard&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;Boston&#x27;</span><br><span class="line"></span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:name&#x27;,&#x27;jack&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1998-08-22&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:age&#x27;,&#x27;21&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;yale&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;New Haven&#x27;</span><br><span class="line"></span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;,&#x27;maike&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1995-01-22&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:age&#x27;,&#x27;24&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;yale&#x27;</span><br><span class="line">put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;New Haven&#x27;</span><br><span class="line"></span><br><span class="line">put &#x27;Student&#x27;, &#x27;wrowkey4&#x27;,&#x27;baseInfo:name&#x27;,&#x27;maike-jack&#x27;</span><br></pre></td></tr></table></figure><h4 id="3-5-获取指定行、指定行中的列族、列的信息"><a href="#3-5-获取指定行、指定行中的列族、列的信息" class="headerlink" title="3.5 获取指定行、指定行中的列族、列的信息"></a>3.5 获取指定行、指定行中的列族、列的信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中所有列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中指定列族下所有列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中指定列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;</span><br></pre></td></tr></table></figure><h4 id="3-6-删除指定行、指定行中的列"><a href="#3-6-删除指定行、指定行中的列" class="headerlink" title="3.6 删除指定行、指定行中的列"></a>3.6 删除指定行、指定行中的列</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除指定行</span></span><br><span class="line">delete &#x27;Student&#x27;,&#x27;rowkey3&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 删除指定行中指定列的数据</span></span><br><span class="line">delete &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;</span><br></pre></td></tr></table></figure><h2 id="四、查询"><a href="#四、查询" class="headerlink" title="四、查询"></a>四、查询</h2><p>hbase 中访问数据有两种基本的方式：</p><ul><li><p>按指定 rowkey 获取数据：get 方法；</p></li><li><p>按指定条件获取数据：scan 方法。</p></li></ul><p><code>scan</code> 可以设置 begin 和 end 参数来访问一个范围内所有的数据。get 本质上就是 begin 和 end 相等的一种特殊的 scan。</p><h4 id="4-1Get查询"><a href="#4-1Get查询" class="headerlink" title="4.1Get查询"></a>4.1Get查询</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中所有列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中指定列族下所有列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo&#x27;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 获取指定行中指定列的数据信息</span></span><br><span class="line">get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;</span><br></pre></td></tr></table></figure><h4 id="4-2-查询整表数据"><a href="#4-2-查询整表数据" class="headerlink" title="4.2 查询整表数据"></a>4.2 查询整表数据</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;Student&#x27;</span><br></pre></td></tr></table></figure><h4 id="4-3-查询指定列簇的数据"><a href="#4-3-查询指定列簇的数据" class="headerlink" title="4.3 查询指定列簇的数据"></a>4.3 查询指定列簇的数据</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, &#123;COLUMN=&gt;&#x27;baseInfo&#x27;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-4-条件查询"><a href="#4-4-条件查询" class="headerlink" title="4.4  条件查询"></a>4.4  条件查询</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查询指定列的数据</span></span><br><span class="line">scan &#x27;Student&#x27;, &#123;COLUMNS=&gt; &#x27;baseInfo:birthday&#x27;&#125;</span><br></pre></td></tr></table></figure><p>除了列 <code>（COLUMNS）</code> 修饰词外，HBase 还支持 <code>Limit</code>（限制查询结果行数），<code>STARTROW</code>（<code>ROWKEY</code> 起始行，会先根据这个 <code>key</code> 定位到 <code>region</code>，再向后扫描）、<code>STOPROW</code>(结束行)、<code>TIMERANGE</code>（限定时间戳范围）、<code>VERSIONS</code>（版本数）、和 <code>FILTER</code>（按条件过滤行）等。</p><p>如下代表从 <code>rowkey2</code> 这个 <code>rowkey</code> 开始，查找下两个行的最新 3 个版本的 name 列的数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, &#123;COLUMNS=&gt; &#x27;baseInfo:name&#x27;,STARTROW =&gt; &#x27;rowkey2&#x27;,STOPROW =&gt; &#x27;wrowkey4&#x27;,LIMIT=&gt;2, VERSIONS=&gt;3&#125;</span><br></pre></td></tr></table></figure><h4 id="4-5-条件过滤"><a href="#4-5-条件过滤" class="headerlink" title="4.5  条件过滤"></a>4.5  条件过滤</h4><p>Filter 可以设定一系列条件来进行过滤。如我们要查询值等于 24 的所有数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;ValueFilter(=,&#x27;binary:24&#x27;)&quot;</span><br></pre></td></tr></table></figure><p>值包含 yale 的所有数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;ValueFilter(=,&#x27;substring:yale&#x27;)&quot;</span><br></pre></td></tr></table></figure><p>列名中的前缀为 birth 的：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;birth&#x27;)&quot;</span><br></pre></td></tr></table></figure><p>FILTER 中支持多个过滤条件通过括号、AND 和 OR 进行组合：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 列名中的前缀为birth且列值中包含1998的数据</span></span><br><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;birth&#x27;) AND ValueFilter ValueFilter(=,&#x27;substring:1998&#x27;)&quot;</span><br></pre></td></tr></table></figure><p><code>PrefixFilter</code> 用于对 Rowkey 的前缀进行判断：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan &#x27;Student&#x27;, FILTER=&gt;&quot;PrefixFilter(&#x27;wr&#x27;)&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase集群搭建</title>
      <link href="/2021/10/25/Hbase%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/10/25/Hbase%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="HBase集群环境配置"><a href="#HBase集群环境配置" class="headerlink" title="HBase集群环境配置"></a>HBase集群环境配置</h1><h2 id="一、集群规划"><a href="#一、集群规划" class="headerlink" title="一、集群规划"></a>一、集群规划</h2><p>这里搭建一个 3 节点的 HBase 集群，其中三台主机上均为 <code>Regin Server</code>。同时为了保证高可用，除了在 hadoop001 上部署主 <code>Master</code> 服务外，还在 hadoop002 上部署备用的 <code>Master</code> 服务。Master 服务由 Zookeeper 集群进行协调管理，如果主 <code>Master</code> 不可用，则备用 <code>Master</code> 会成为新的主 <code>Master</code>。</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase集群规划.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase集群规划.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="二、前置条件"><a href="#二、前置条件" class="headerlink" title="二、前置条件"></a>二、前置条件</h2><p>HBase 的运行需要依赖 Hadoop 和 JDK(<code>HBase 2.0+</code> 对应 <code>JDK 1.8+</code>) 。同时为了保证高可用，这里我们不采用 HBase 内置的 Zookeeper 服务，而采用外置的 Zookeeper 集群。相关搭建步骤可以参阅：</p><ul><li><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 环境下 JDK 安装</a></li><li><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Zookeeper%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%92%8C%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Zookeeper 单机环境和集群环境搭建</a></li><li><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Hadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Hadoop 集群环境搭建</a></li></ul><h2 id="三、集群搭建"><a href="#三、集群搭建" class="headerlink" title="三、集群搭建"></a>三、集群搭建</h2><h3 id="3-1-下载并解压"><a href="#3-1-下载并解压" class="headerlink" title="3.1 下载并解压"></a>3.1 下载并解压</h3><p>下载并解压，这里我下载的是 CDH 版本 HBase，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tar -zxvf hbase-1.2.0-cdh5.15.2.tar.gz</span></span><br></pre></td></tr></table></figure><h3 id="3-2-配置环境变量"><a href="#3-2-配置环境变量" class="headerlink" title="3.2 配置环境变量"></a>3.2 配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/profile</span></span><br></pre></td></tr></table></figure><p>添加环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=usr/app/hbase-1.2.0-cdh5.15.2</span><br><span class="line">export PATH=$HBASE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>使得配置的环境变量立即生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure><h3 id="3-3-集群配置"><a href="#3-3-集群配置" class="headerlink" title="3.3 集群配置"></a>3.3 集群配置</h3><p>进入 <code>$&#123;HBASE_HOME&#125;/conf</code> 目录下，修改配置：</p><h4 id="1-hbase-env-sh"><a href="#1-hbase-env-sh" class="headerlink" title="1. hbase-env.sh"></a>1. hbase-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 配置JDK安装位置</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201</span><br><span class="line"><span class="meta">#</span><span class="bash"> 不使用内置的zookeeper服务</span></span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure><h4 id="2-hbase-site-xml"><a href="#2-hbase-site-xml" class="headerlink" title="2. hbase-site.xml"></a>2. hbase-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 hbase 以分布式集群的方式运行 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 hbase 在 HDFS 上的存储位置 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定 zookeeper 的地址--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop001:2181,hadoop002:2181,hadoop003:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="3-regionservers"><a href="#3-regionservers" class="headerlink" title="3. regionservers"></a>3. regionservers</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br></pre></td></tr></table></figure><h4 id="4-backup-masters"><a href="#4-backup-masters" class="headerlink" title="4. backup-masters"></a>4. backup-masters</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop002</span><br></pre></td></tr></table></figure><p><code> backup-masters</code> 这个文件是不存在的，需要新建，主要用来指明备用的 master 节点，可以是多个，这里我们以 1 个为例。</p><h3 id="3-4-HDFS客户端配置"><a href="#3-4-HDFS客户端配置" class="headerlink" title="3.4 HDFS客户端配置"></a>3.4 HDFS客户端配置</h3><p>这里有一个可选的配置：如果您在 Hadoop 集群上进行了 HDFS 客户端配置的更改，比如将副本系数 <code>dfs.replication</code> 设置成 5，则必须使用以下方法之一来使 HBase 知道，否则 HBase 将依旧使用默认的副本系数 3 来创建文件：</p><blockquote><ol><li>Add a pointer to your <code>HADOOP_CONF_DIR</code> to the <code>HBASE_CLASSPATH</code> environment variable in <em>hbase-env.sh</em>.</li><li>Add a copy of <em>hdfs-site.xml</em> (or <em>hadoop-site.xml</em>) or, better, symlinks, under <em>${HBASE_HOME}/conf</em>, or</li><li>if only a small set of HDFS client configurations, add them to <em>hbase-site.xml</em>.</li></ol></blockquote><p>以上是官方文档的说明，这里解释一下：</p><p><strong>第一种</strong> ：将 Hadoop 配置文件的位置信息添加到 <code>hbase-env.sh</code> 的 <code>HBASE_CLASSPATH</code> 属性，示例如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_CLASSPATH=usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop</span><br></pre></td></tr></table></figure><p><strong>第二种</strong> ：将 Hadoop 的 <code> hdfs-site.xml</code> 或 <code>hadoop-site.xml</code> 拷贝到  <code>$&#123;HBASE_HOME&#125;/conf </code> 目录下，或者通过符号链接的方式。如果采用这种方式的话，建议将两者都拷贝或建立符号链接，示例如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 拷贝</span></span><br><span class="line">cp core-site.xml hdfs-site.xml /usr/app/hbase-1.2.0-cdh5.15.2/conf/</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用符号链接</span></span><br><span class="line">ln -s   /usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop/core-site.xml</span><br><span class="line">ln -s   /usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop/hdfs-site.xml</span><br></pre></td></tr></table></figure><blockquote><p>注：<code>hadoop-site.xml</code> 这个配置文件现在叫做 <code>core-site.xml</code></p></blockquote><p><strong>第三种</strong> ：如果你只有少量更改，那么直接配置到 <code>hbase-site.xml</code> 中即可。</p><h3 id="3-5-安装包分发"><a href="#3-5-安装包分发" class="headerlink" title="3.5 安装包分发"></a>3.5 安装包分发</h3><p>将 HBase 的安装包分发到其他服务器，分发后建议在这两台服务器上也配置一下 HBase 的环境变量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /usr/app/hbase-1.2.0-cdh5.15.2/  hadoop002:usr/app/</span><br><span class="line">scp -r /usr/app/hbase-1.2.0-cdh5.15.2/  hadoop003:usr/app/</span><br></pre></td></tr></table></figure><h2 id="四、启动集群"><a href="#四、启动集群" class="headerlink" title="四、启动集群"></a>四、启动集群</h2><h3 id="4-1-启动ZooKeeper集群"><a href="#4-1-启动ZooKeeper集群" class="headerlink" title="4.1 启动ZooKeeper集群"></a>4.1 启动ZooKeeper集群</h3><p>分别到三台服务器上启动 ZooKeeper 服务：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="4-2-启动Hadoop集群"><a href="#4-2-启动Hadoop集群" class="headerlink" title="4.2 启动Hadoop集群"></a>4.2 启动Hadoop集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动dfs服务</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动yarn服务</span></span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h3 id="4-3-启动HBase集群"><a href="#4-3-启动HBase集群" class="headerlink" title="4.3 启动HBase集群"></a>4.3 启动HBase集群</h3><p>进入 hadoop001 的 <code>$&#123;HBASE_HOME&#125;/bin</code>，使用以下命令启动 HBase 集群。执行此命令后，会在 hadoop001 上启动 <code>Master</code> 服务，在 hadoop002 上启动备用 <code>Master</code> 服务，在 <code>regionservers</code> 文件中配置的所有节点启动 <code>region server</code> 服务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><h3 id="4-5-查看服务"><a href="#4-5-查看服务" class="headerlink" title="4.5 查看服务"></a>4.5 查看服务</h3><p>访问 HBase 的 Web-UI 界面，这里我安装的 HBase 版本为 1.2，访问端口为 <code>60010</code>，如果你安装的是 2.0 以上的版本，则访问端口号为 <code>16010</code>。可以看到 <code>Master</code> 在 hadoop001 上，三个 <code>Regin Servers</code> 分别在 hadoop001，hadoop002，和 hadoop003 上，并且还有一个 <code>Backup Matser</code> 服务在 hadoop002 上。</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-集群搭建1.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-集群搭建1.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><br/><p>hadoop002 上的 HBase 出于备用状态：</p><br/><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-集群搭建2.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-集群搭建2.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase环境搭建</title>
      <link href="/2021/10/25/Hbase%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/10/25/Hbase%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="HBase基本环境搭建"><a href="#HBase基本环境搭建" class="headerlink" title="HBase基本环境搭建"></a>HBase基本环境搭建</h1><h2 id="一、安装前置条件说明"><a href="#一、安装前置条件说明" class="headerlink" title="一、安装前置条件说明"></a>一、安装前置条件说明</h2><h3 id="1-1-JDK版本说明"><a href="#1-1-JDK版本说明" class="headerlink" title="1.1 JDK版本说明"></a>1.1 JDK版本说明</h3><p>HBase 需要依赖 JDK 环境，同时 HBase 2.0+ 以上版本不再支持 JDK 1.7 ，需要安装 JDK 1.8+ 。JDK 安装方式见本仓库：</p><blockquote><p><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 环境下 JDK 安装</a></p></blockquote><h3 id="1-2-Standalone模式和伪集群模式的区别"><a href="#1-2-Standalone模式和伪集群模式的区别" class="headerlink" title="1.2 Standalone模式和伪集群模式的区别"></a>1.2 Standalone模式和伪集群模式的区别</h3><ul><li>在 <code>Standalone</code> 模式下，所有守护进程都运行在一个 <code>jvm</code> 进程/实例中；</li><li>在伪分布模式下，HBase 仍然在单个主机上运行，但是每个守护进程 (HMaster，HRegionServer 和 ZooKeeper) 则分别作为一个单独的进程运行。</li></ul><p><strong>说明：两种模式任选其一进行部署即可，对于开发测试来说区别不大。</strong></p><h2 id="二、Standalone-模式"><a href="#二、Standalone-模式" class="headerlink" title="二、Standalone 模式"></a>二、Standalone 模式</h2><h3 id="2-1-下载并解压"><a href="#2-1-下载并解压" class="headerlink" title="2.1 下载并解压"></a>2.1 下载并解压</h3><p>从<a href="https://hbase.apache.org/downloads.html">官方网站</a> 下载所需要版本的二进制安装包，并进行解压：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tar -zxvf hbase-2.1.4-bin.tar.gz</span></span><br></pre></td></tr></table></figure><h3 id="2-2-配置环境变量"><a href="#2-2-配置环境变量" class="headerlink" title="2.2 配置环境变量"></a>2.2 配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/profile</span></span><br></pre></td></tr></table></figure><p>添加环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/usr/app/hbase-2.1.4</span><br><span class="line">export PATH=$HBASE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>使得配置的环境变量生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure><h3 id="2-3-进行HBase相关配置"><a href="#2-3-进行HBase相关配置" class="headerlink" title="2.3 进行HBase相关配置"></a>2.3 进行HBase相关配置</h3><p>修改安装目录下的 <code>conf/hbase-env.sh</code>,指定 JDK 的安装路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The java implementation to use.  Java 1.8+ required.</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201</span><br></pre></td></tr></table></figure><p>修改安装目录下的 <code>conf/hbase-site.xml</code>，增加如下配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///home/hbase/rootdir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/zookeeper/dataDir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.unsafe.stream.capability.enforce<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><code>hbase.rootdir</code>: 配置 hbase 数据的存储路径；</p><p><code>hbase.zookeeper.property.dataDir</code>: 配置 zookeeper 数据的存储路径；</p><p><code>hbase.unsafe.stream.capability.enforce</code>: 使用本地文件系统存储，不使用 HDFS 的情况下需要禁用此配置，设置为 false。</p><h3 id="2-4-启动HBase"><a href="#2-4-启动HBase" class="headerlink" title="2.4 启动HBase"></a>2.4 启动HBase</h3><p>由于已经将 HBase 的 bin 目录配置到环境变量，直接使用以下命令启动：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> start-hbase.sh</span></span><br></pre></td></tr></table></figure><h3 id="2-5-验证启动是否成功"><a href="#2-5-验证启动是否成功" class="headerlink" title="2.5 验证启动是否成功"></a>2.5 验证启动是否成功</h3><p>验证方式一 ：使用 <code>jps</code> 命令查看 HMaster 进程是否启动。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hbase-2.1.4]# jps</span><br><span class="line">16336 Jps</span><br><span class="line">15500 HMaster</span><br></pre></td></tr></table></figure><p>验证方式二 ：访问 HBaseWeb UI 页面，默认端口为 <code>16010</code> 。</p><div align="center"> <img src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-web-ui.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-web-ui.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="三、伪集群模式安装（Pseudo-Distributed）"><a href="#三、伪集群模式安装（Pseudo-Distributed）" class="headerlink" title="三、伪集群模式安装（Pseudo-Distributed）"></a>三、伪集群模式安装（Pseudo-Distributed）</h2><h3 id="3-1-Hadoop单机伪集群安装"><a href="#3-1-Hadoop单机伪集群安装" class="headerlink" title="3.1 Hadoop单机伪集群安装"></a>3.1 Hadoop单机伪集群安装</h3><p>这里我们采用 HDFS 作为 HBase 的存储方案，需要预先安装 Hadoop。Hadoop 的安装方式单独整理至：</p><blockquote><p><a href="https://github.com/ihadyou/BigData-Notes/blob/master/notes/installation/Hadoop%E5%8D%95%E6%9C%BA%E7%89%88%E6%9C%AC%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Hadoop 单机伪集群搭建</a></p></blockquote><h3 id="3-2-Hbase版本选择"><a href="#3-2-Hbase版本选择" class="headerlink" title="3.2 Hbase版本选择"></a>3.2 Hbase版本选择</h3><p>HBase 的版本必须要与 Hadoop 的版本兼容，不然会出现各种 Jar 包冲突。这里我 Hadoop 安装的版本为 <code>hadoop-2.6.0-cdh5.15.2</code>，为保持版本一致，选择的 HBase 版本为 <code>hbase-1.2.0-cdh5.15.2</code> 。所有软件版本如下：</p><ul><li>Hadoop 版本： hadoop-2.6.0-cdh5.15.2</li><li>HBase 版本： hbase-1.2.0-cdh5.15.2</li><li>JDK 版本：JDK 1.8</li></ul><h3 id="3-3-软件下载解压"><a href="#3-3-软件下载解压" class="headerlink" title="3.3 软件下载解压"></a>3.3 软件下载解压</h3><p>下载后进行解压，下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> tar -zxvf hbase-1.2.0-cdh5.15.2.tar.gz</span></span><br></pre></td></tr></table></figure><h3 id="3-4-配置环境变量"><a href="#3-4-配置环境变量" class="headerlink" title="3.4 配置环境变量"></a>3.4 配置环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim /etc/profile</span></span><br></pre></td></tr></table></figure><p>添加环境变量：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/usr/app/hbase-1.2.0-cdh5.15.2</span><br><span class="line">export PATH=$HBASE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>使得配置的环境变量生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">source</span> /etc/profile</span></span><br></pre></td></tr></table></figure><h3 id="3-5-进行HBase相关配置"><a href="#3-5-进行HBase相关配置" class="headerlink" title="3.5 进行HBase相关配置"></a>3.5 进行HBase相关配置</h3><p>1.修改安装目录下的 <code>conf/hbase-env.sh</code>,指定 JDK 的安装路径：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> The java implementation to use.  Java 1.7+ required.</span></span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201</span><br></pre></td></tr></table></figure><p>2.修改安装目录下的 <code>conf/hbase-site.xml</code>，增加如下配置 (hadoop001 为主机名)：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--指定 HBase 以分布式模式运行--&gt;</span>   </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--指定 HBase 数据存储路径为 HDFS 上的 hbase 目录,hbase 目录不需要预先创建，程序会自动创建--&gt;</span>   </span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop001:8020/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定 zookeeper 数据的存储位置--&gt;</span>   </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/zookeeper/dataDir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3.修改安装目录下的 <code>conf/regionservers</code>，指定 region  servers 的地址，修改后其内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop001</span><br></pre></td></tr></table></figure><h3 id="3-6-启动"><a href="#3-6-启动" class="headerlink" title="3.6 启动"></a>3.6 启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> bin/start-hbase.sh</span></span><br></pre></td></tr></table></figure><h3 id="3-7-验证启动是否成功"><a href="#3-7-验证启动是否成功" class="headerlink" title="3.7 验证启动是否成功"></a>3.7 验证启动是否成功</h3><p>验证方式一 ：使用 <code>jps</code> 命令查看进程。其中 <code>HMaster</code>，<code>HRegionServer</code> 是 HBase 的进程，<code>HQuorumPeer</code> 是 HBase 内置的 Zookeeper 的进程，其余的为 HDFS 和 YARN 的进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 conf]# jps</span><br><span class="line">28688 NodeManager</span><br><span class="line">25824 GradleDaemon</span><br><span class="line">10177 Jps</span><br><span class="line">22083 HRegionServer</span><br><span class="line">20534 DataNode</span><br><span class="line">20807 SecondaryNameNode</span><br><span class="line">18744 Main</span><br><span class="line">20411 NameNode</span><br><span class="line">21851 HQuorumPeer</span><br><span class="line">28573 ResourceManager</span><br><span class="line">21933 HMaster</span><br></pre></td></tr></table></figure><p>验证方式二 ：访问 HBase Web UI 界面，需要注意的是 1.2 版本的 HBase 的访问端口为 <code>60010</code></p><div align="center"> <img src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-60010.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-60010.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase架构及数据结构</title>
      <link href="/2021/10/25/Hbase%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
      <url>/2021/10/25/Hbase%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase系统架构及数据结构"><a href="#Hbase系统架构及数据结构" class="headerlink" title="Hbase系统架构及数据结构"></a>Hbase系统架构及数据结构</h1><h2 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h2><p>一个典型的 Hbase Table 表如下：</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-webtable.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-webtable.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="1-1-Row-Key-行键"><a href="#1-1-Row-Key-行键" class="headerlink" title="1.1 Row Key (行键)"></a>1.1 Row Key (行键)</h3><p><code>Row Key</code> 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式：</p><ul><li><p>通过指定的 <code>Row Key</code> 进行访问；</p></li><li><p>通过 Row Key 的 range 进行访问，即访问指定范围内的行；</p></li><li><p>进行全表扫描。</p></li></ul><p><code>Row Key</code> 可以是任意字符串，存储时数据按照 <code>Row Key</code> 的字典序进行排序。这里需要注意以下两点：</p><ul><li><p>因为字典序对 Int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。</p></li><li><p>行的一次读写操作时原子性的 (不论一次读写多少列)。</p></li></ul><h3 id="1-2-Column-Family（列族）"><a href="#1-2-Column-Family（列族）" class="headerlink" title="1.2 Column Family（列族）"></a>1.2 Column Family（列族）</h3><p>HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时进行定义。列族的所有列都以列族名作为前缀，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族。</p><h3 id="1-3-Column-Qualifier-列限定符"><a href="#1-3-Column-Qualifier-列限定符" class="headerlink" title="1.3 Column Qualifier (列限定符)"></a>1.3 Column Qualifier (列限定符)</h3><p>列限定符，你可以理解为是具体的列名，例如 <code>courses:history</code>，<code>courses:math</code> 都属于 <code>courses</code> 这个列族，它们的列限定符分别是 <code>history</code> 和 <code>math</code>。需要注意的是列限定符不是表 Schema 的一部分，你可以在插入数据的过程中动态创建列。</p><h3 id="1-4-Column-列"><a href="#1-4-Column-列" class="headerlink" title="1.4 Column(列)"></a>1.4 Column(列)</h3><p>HBase 中的列由列族和列限定符组成，它们由 <code>:</code>(冒号) 进行分隔，即一个完整的列名应该表述为 <code>列族名 ：列限定符</code>。</p><h3 id="1-5-Cell"><a href="#1-5-Cell" class="headerlink" title="1.5 Cell"></a>1.5 Cell</h3><p><code>Cell</code> 是行，列族和列限定符的组合，并包含值和时间戳。你可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，但不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。</p><h3 id="1-6-Timestamp-时间戳"><a href="#1-6-Timestamp-时间戳" class="headerlink" title="1.6 Timestamp(时间戳)"></a>1.6 Timestamp(时间戳)</h3><p>HBase 中通过 <code>row key</code> 和 <code>column</code> 确定的为一个存储单元称为 <code>Cell</code>。每个 <code>Cell</code> 都保存着同一份数据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入时自动赋值，也可以由客户显式指定。每个 <code>Cell</code> 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。</p><h2 id="二、存储结构"><a href="#二、存储结构" class="headerlink" title="二、存储结构"></a>二、存储结构</h2><h3 id="2-1-Regions"><a href="#2-1-Regions" class="headerlink" title="2.1 Regions"></a>2.1 Regions</h3><p>HBase Table 中的所有行按照 <code>Row Key</code> 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 <code>Region</code>, 一个 <code>Region</code> 包含了在 start key 和 end key 之间的所有行。</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig2.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig2.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p>每个表一开始只有一个 <code>Region</code>，随着数据不断增加，<code>Region</code> 会不断增大，当增大到一个阀值的时候，<code>Region</code> 就会等分为两个新的 <code>Region</code>。当 Table 中的行不断增多，就会有越来越多的 <code>Region</code>。</p><div align="center"> <img width="600px" src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-region-splite.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-region-splite.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p><code>Region</code> 是 HBase 中<strong>分布式存储和负载均衡的最小单元</strong>。这意味着不同的 <code>Region</code> 可以分布在不同的 <code>Region Server</code> 上。但一个 <code>Region</code> 是不会拆分到多个 Server 上的。</p><div align="center"> <img width="600px" src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-region-dis.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-region-dis.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="2-2-Region-Server"><a href="#2-2-Region-Server" class="headerlink" title="2.2 Region Server"></a>2.2 Region Server</h3><p><code>Region Server</code> 运行在 HDFS 的 DataNode 上。它具有以下组件：</p><ul><li>**WAL(Write Ahead Log，预写日志)**：用于存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。</li><li><strong>BlockCache</strong>：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 <code>最近最少使用原则</code> 清除多余的数据。</li><li><strong>MemStore</strong>：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。</li><li><strong>HFile</strong> ：将行数据按照 Key\Values 的形式存储在文件系统上。</li></ul><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-Region-Server.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-Region-Server.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p>Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 <code>Store</code> 实例，每个 <code>Store</code> 会有 0 个或多个 <code>StoreFile</code> 与之对应，每个 <code>StoreFile</code> 则对应一个 <code>HFile</code>，HFile 就是实际存储在 HDFS 上的文件。</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-hadoop.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase-hadoop.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="三、Hbase系统架构"><a href="#三、Hbase系统架构" class="headerlink" title="三、Hbase系统架构"></a>三、Hbase系统架构</h2><h3 id="3-1-系统架构"><a href="#3-1-系统架构" class="headerlink" title="3.1 系统架构"></a>3.1 系统架构</h3><p>HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成：</p><p><strong>Zookeeper</strong></p><ol><li><p>保证任何时候，集群中只有一个 Master；</p></li><li><p>存贮所有 Region 的寻址入口；</p></li><li><p>实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master；</p></li><li><p>存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。</p></li></ol><p><strong>Master</strong></p><ol><li><p>为 Region Server 分配 Region ；</p></li><li><p>负责 Region Server 的负载均衡 ；</p></li><li><p>发现失效的 Region Server 并重新分配其上的 Region；</p></li><li><p>GFS 上的垃圾文件回收；</p></li><li><p>处理 Schema 的更新请求。</p></li></ol><p><strong>Region Server</strong></p><ol><li><p>Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求；</p></li><li><p>Region Server 负责切分在运行过程中变得过大的 Region。</p></li></ol><div align="center"> <img width="600px" src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig1.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig1.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h3 id="3-2-组件间的协作"><a href="#3-2-组件间的协作" class="headerlink" title="3.2 组件间的协作"></a>3.2 组件间的协作</h3><p>HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务：</p><ul><li><p>每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server；</p></li><li><p>所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听；</p></li><li><p>如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。</p></li></ul><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig5.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig5.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><h2 id="四、数据的读写流程简述"><a href="#四、数据的读写流程简述" class="headerlink" title="四、数据的读写流程简述"></a>四、数据的读写流程简述</h2><h3 id="4-1-写入数据的流程"><a href="#4-1-写入数据的流程" class="headerlink" title="4.1 写入数据的流程"></a>4.1 写入数据的流程</h3><ol><li><p>Client 向 Region Server 提交写请求；</p></li><li><p>Region Server 找到目标 Region；</p></li><li><p>Region 检查数据是否与 Schema 一致；</p></li><li><p>如果客户端没有指定版本，则获取当前系统时间作为数据版本；</p></li><li><p>将更新写入 WAL Log；</p></li><li><p>将更新写入 Memstore；</p></li><li><p>判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。</p></li></ol><blockquote><p>更为详细写入流程可以参考：<a href="http://hbasefly.com/2016/03/23/hbase_writer/">HBase － 数据写入流程解析</a></p></blockquote><h3 id="4-2-读取数据的流程"><a href="#4-2-读取数据的流程" class="headerlink" title="4.2 读取数据的流程"></a>4.2 读取数据的流程</h3><p>以下是客户端首次读写 HBase 上数据的流程：</p><ol><li><p>客户端从 Zookeeper 获取 <code>META</code> 表所在的 Region Server；</p></li><li><p>客户端访问 <code>META</code> 表所在的 Region Server，从 <code>META</code> 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 <code>META</code> 表的位置；</p></li><li><p>客户端从行键所在的 Region Server 上获取数据。</p></li></ol><p>如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 <code>META</code> 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。</p><p>注：<code>META</code> 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息则存储在 ZooKeeper 上。</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig7.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBaseArchitecture-Blog-Fig7.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><blockquote><p>更为详细读取数据流程参考：</p><p><a href="http://hbasefly.com/2016/12/21/hbase-getorscan/">HBase 原理－数据读取流程解析</a></p><p><a href="http://hbasefly.com/2017/06/11/hbase-scan-2/">HBase 原理－迟到的‘数据读取流程部分细节</a></p></blockquote><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>本篇文章内容主要参考自官方文档和以下两篇博客，图片也主要引用自以下两篇博客：</p><ul><li><p><a href="https://mapr.com/blog/in-depth-look-hbase-architecture/#.VdMxvWSqqko">HBase Architectural Components</a></p></li><li><p><a href="https://www.open-open.com/lib/view/open1346821084631.html">Hbase 系统架构及数据结构</a></p></li></ul><p>官方文档：</p><ul><li><a href="https://hbase.apache.org/2.1/book.html">Apache HBase ™ Reference Guide</a></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase简介</title>
      <link href="/2021/10/25/Hbase%E7%AE%80%E4%BB%8B/"/>
      <url>/2021/10/25/Hbase%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="HBase简介"><a href="#HBase简介" class="headerlink" title="HBase简介"></a>HBase简介</h1><h2 id="一、Hadoop的局限"><a href="#一、Hadoop的局限" class="headerlink" title="一、Hadoop的局限"></a>一、Hadoop的局限</h2><p>HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。</p><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase.jpg" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hbase.jpg" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><p>要想明白为什么产生 HBase，就需要先了解一下 Hadoop 存在的限制？Hadoop 可以通过 HDFS 来存储结构化、半结构甚至非结构化的数据，它是传统数据库的补充，是海量数据存储的最佳方法，它针对大文件的存储，批量访问和流式访问都做了优化，同时也通过多副本解决了容灾问题。</p><p>但是 Hadoop 的缺陷在于它只能执行批处理，并且只能以顺序方式访问数据，这意味着即使是最简单的工作，也必须搜索整个数据集，无法实现对数据的随机访问。实现数据的随机访问是传统的关系型数据库所擅长的，但它们却不能用于海量数据的存储。在这种情况下，必须有一种新的方案来解决海量数据存储和随机访问的问题，HBase 就是其中之一 (HBase，Cassandra，couchDB，Dynamo 和 MongoDB 都能存储海量数据并支持随机访问)。</p><blockquote><p>注：数据结构分类：</p><ul><li>结构化数据：即以关系型数据库表形式管理的数据；</li><li>半结构化数据：非关系模型的，有基本固定结构模式的数据，例如日志文件、XML 文档、JSON 文档、Email 等；</li><li>非结构化数据：没有固定模式的数据，如 WORD、PDF、PPT、EXL，各种格式的图片、视频等。</li></ul></blockquote><h2 id="二、HBase简介"><a href="#二、HBase简介" class="headerlink" title="二、HBase简介"></a>二、HBase简介</h2><p>HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。</p><p>HBase 是一种类似于 <code>Google’s Big Table</code> 的数据模型，它是 Hadoop 生态系统的一部分，它将数据存储在 HDFS 上，客户端可以通过 HBase 实现对 HDFS 上数据的随机访问。它具有以下特性：</p><ul><li>不支持复杂的事务，只支持行级事务，即单行数据的读写都是原子性的；</li><li>由于是采用 HDFS 作为底层存储，所以和 HDFS 一样，支持结构化、半结构化和非结构化的存储；</li><li>支持通过增加机器进行横向扩展；</li><li>支持数据分片；</li><li>支持 RegionServers 之间的自动故障转移；</li><li>易于使用的 Java 客户端 API；</li><li>支持 BlockCache 和布隆过滤器；</li><li>过滤器支持谓词下推。</li></ul><h2 id="三、HBase-Table"><a href="#三、HBase-Table" class="headerlink" title="三、HBase Table"></a>三、HBase Table</h2><p>HBase 是一个面向 <code>列</code> 的数据库管理系统，这里更为确切的而说，HBase 是一个面向 <code>列族</code> 的数据库管理系统。表 schema 仅定义列族，表具有多个列族，每个列族可以包含任意数量的列，列由多个单元格（cell ）组成，单元格可以存储多个版本的数据，多个版本数据以时间戳进行区分。</p><p>下图为 HBase 中一张表的：</p><ul><li>RowKey 为行的唯一标识，所有行按照 RowKey 的字典序进行排序；</li><li>该表具有两个列族，分别是 personal 和 office;</li><li>其中列族 personal 拥有 name、city、phone 三个列，列族 office 拥有 tel、addres 两个列。</li></ul><div align="center"> <img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBase_table-iteblog.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/HBase_table-iteblog.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/> </div><blockquote><p><em>图片引用自 : HBase 是列式存储数据库吗</em> <em><a href="https://www.iteblog.com/archives/2498.html">https://www.iteblog.com/archives/2498.html</a></em></p></blockquote><p>Hbase 的表具有以下特点：</p><ul><li><p>容量大：一个表可以有数十亿行，上百万列；</p></li><li><p>面向列：数据是按照列存储，每一列都单独存放，数据即索引，在查询时可以只访问指定列的数据，有效地降低了系统的 I/O 负担；</p></li><li><p>稀疏性：空 (null) 列并不占用存储空间，表可以设计的非常稀疏  ；</p></li><li><p>数据多版本：每个单元中的数据可以有多个版本，按照时间戳排序，新的数据在最上面；</p></li><li><p>存储类型：所有数据的底层存储格式都是字节数组 (byte[])。</p></li></ul><h2 id="四、Phoenix"><a href="#四、Phoenix" class="headerlink" title="四、Phoenix"></a>四、Phoenix</h2><p><code>Phoenix</code> 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 <code>Phoenix</code> 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。<code>Phoenix</code> 的理念是 <code>we put sql SQL back in NOSQL</code>，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 <code>Spring Data  JPA</code> 或 <code>Mybatis</code> 等常用的持久层框架来操作 HBase。</p><p>其次 <code>Phoenix</code> 的性能表现也非常优异，<code>Phoenix</code> 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 <code>Phoenix</code> 成为了 HBase 最优秀的 SQL 中间层。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.tutorialspoint.com/hbase/hbase_overview.htm">HBase - Overview</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive条件与日期函数汇总</title>
      <link href="/2021/10/25/Hive%E6%9D%A1%E4%BB%B6%E4%B8%8E%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/"/>
      <url>/2021/10/25/Hive%E6%9D%A1%E4%BB%B6%E4%B8%8E%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="条件函数"><a href="#条件函数" class="headerlink" title="条件函数"></a>条件函数</h2><h3 id="assert-true-BOOLEAN-condition"><a href="#assert-true-BOOLEAN-condition" class="headerlink" title="assert_true(BOOLEAN condition)"></a>assert_true(BOOLEAN condition)</h3><ul><li>解释</li></ul><p>如果condition不为true，则抛出异常，否则返回null</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> assert_true(<span class="number">1</span><span class="operator">&lt;</span><span class="number">2</span>) <span class="comment">-- 返回null</span></span><br><span class="line"><span class="keyword">select</span> assert_true(<span class="number">1</span><span class="operator">&gt;</span><span class="number">2</span>) <span class="comment">-- 抛出异常</span></span><br></pre></td></tr></table></figure><h3 id="coalesce-T-v1-T-v2-…"><a href="#coalesce-T-v1-T-v2-…" class="headerlink" title="coalesce(T v1, T v2, …)"></a>coalesce(T v1, T v2, …)</h3><ul><li>解释</li></ul><p>返回第一个不为null的值，如果都为null，则返回null</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="keyword">null</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="keyword">null</span>)  <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="number">1</span>,<span class="keyword">null</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="keyword">null</span>,<span class="keyword">null</span>) <span class="comment">-- 返回null</span></span><br></pre></td></tr></table></figure><h3 id="if-BOOLEAN-testCondition-valueTrue-valueFalseOrNull"><a href="#if-BOOLEAN-testCondition-valueTrue-valueFalseOrNull" class="headerlink" title="if(BOOLEAN testCondition,valueTrue, valueFalseOrNull)"></a>if(BOOLEAN testCondition,valueTrue, valueFalseOrNull)</h3><ul><li>解释</li></ul><p>如果testCondition条件为true，则返回第一个值，否则返回第二个值</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> if(<span class="number">1</span> <span class="keyword">is</span> <span class="keyword">null</span>,<span class="number">0</span>,<span class="number">1</span>)  <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> if(<span class="keyword">null</span> <span class="keyword">is</span> <span class="keyword">null</span>,<span class="number">0</span>,<span class="number">1</span>) <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure><h3 id="isnotnull-a"><a href="#isnotnull-a" class="headerlink" title="isnotnull(a)"></a>isnotnull(a)</h3><ul><li>解释</li></ul><p>如果参数a不为null，则返回true，否则返回false</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> isnotnull(<span class="number">1</span>) <span class="comment">-- 返回true</span></span><br><span class="line"><span class="keyword">select</span> isnotnull(<span class="keyword">null</span>) <span class="comment">-- 返回false</span></span><br></pre></td></tr></table></figure><h3 id="isnull-a"><a href="#isnull-a" class="headerlink" title="isnull(a)"></a>isnull(a)</h3><ul><li>解释</li></ul><p>与isnotnull相反，如果参数a为null，则返回true，否则返回false</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> isnull(<span class="keyword">null</span>) <span class="comment">-- 返回true</span></span><br><span class="line"><span class="keyword">select</span> isnull(<span class="number">1</span>) <span class="comment">-- 返回false</span></span><br></pre></td></tr></table></figure><h3 id="nullif-a-b"><a href="#nullif-a-b" class="headerlink" title="nullif(a, b)"></a>nullif(a, b)</h3><ul><li>解释</li></ul><p>如果参数a=b，返回null，否则返回a值(Hive2.2.0版本)</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">nullif</span>(<span class="number">1</span>,<span class="number">2</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">nullif</span>(<span class="number">1</span>,<span class="number">1</span>) <span class="comment">-- 返回null</span></span><br></pre></td></tr></table></figure><h3 id="nvl-T-value-T-default-value"><a href="#nvl-T-value-T-default-value" class="headerlink" title="nvl(T value, T default_value)"></a>nvl(T value, T default_value)</h3><ul><li>解释</li></ul><p>如果value的值为null，则返回default_value默认值，否则返回value的值。在null值判断时，可以使用if函数给定默认值，也可以使用此函数给定默认值，使用该函数sql特别简洁。</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> nvl(<span class="number">1</span>,<span class="number">0</span>) <span class="comment">-- 返回1</span></span><br><span class="line"><span class="keyword">select</span> nvl(<span class="keyword">null</span>,<span class="number">0</span>) <span class="comment">-- 返回0</span></span><br></pre></td></tr></table></figure><h2 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h2><h3 id="add-months-DATE-STRING-TIMESTAMP-start-date-INT-num-months"><a href="#add-months-DATE-STRING-TIMESTAMP-start-date-INT-num-months" class="headerlink" title="add_months(DATE|STRING|TIMESTAMP start_date, INT num_months)"></a>add_months(DATE|STRING|TIMESTAMP start_date, INT num_months)</h3><ul><li>解释</li></ul><p>&emsp;&emsp;start_date参数可以是string, date 或者timestamp类型，num_months参数时int类型。返回一个日期，该日期是在start_date基础之上加上num_months个月，即start_date之后null_months个月的一个日期。如果start_date的时间部分的数据会被忽略。注意：如果start_date所在月份的天数大于结果日期月的天数，则返回结果月的最后一天的日期。</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> add_months(&quot;2020-05-20&quot;,<span class="number">2</span>); <span class="comment">-- 返回2020-07-20</span></span><br><span class="line"><span class="keyword">select</span> add_months(&quot;2020-05-20&quot;,<span class="number">8</span>); <span class="comment">-- 返回2021-01-20</span></span><br><span class="line"><span class="keyword">select</span> add_months(&quot;2020-05-31&quot;,<span class="number">1</span>); <span class="comment">-- 返回2020-06-30,5月有31天，6月只有30天，所以返回下一个月的最后一天</span></span><br></pre></td></tr></table></figure><h3 id="current-date"><a href="#current-date" class="headerlink" title="current_date"></a>current_date</h3><ul><li>解释</li></ul><p>返回查询时刻的当前日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">current_date</span>() <span class="comment">-- 返回当前查询日期2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="current-timestamp"><a href="#current-timestamp" class="headerlink" title="current_timestamp()"></a>current_timestamp()</h3><ul><li>解释</li></ul><p>返回查询时刻的当前时间</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">current_timestamp</span>() <span class="comment">-- 2020-05-20 14:40:47.273</span></span><br></pre></td></tr></table></figure><h3 id="datediff-STRING-enddate-STRING-startdate"><a href="#datediff-STRING-enddate-STRING-startdate" class="headerlink" title="datediff(STRING enddate, STRING startdate)"></a>datediff(STRING enddate, STRING startdate)</h3><ul><li>解释</li></ul><p>返回开始日期startdate与结束日期enddate之前相差的天数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> datediff(&quot;2020-05-20&quot;,&quot;2020-05-21&quot;); <span class="comment">-- 返回-1</span></span><br><span class="line"><span class="keyword">select</span> datediff(&quot;2020-05-21&quot;,&quot;2020-05-20&quot;); <span class="comment">-- 返回1</span></span><br></pre></td></tr></table></figure><h3 id="date-add-DATE-startdate-INT-days"><a href="#date-add-DATE-startdate-INT-days" class="headerlink" title="date_add(DATE startdate, INT days)"></a>date_add(DATE startdate, INT days)</h3><ul><li>解释</li></ul><p>在startdate基础上加上几天，然后返回加上几天之后的一个日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> date_add(&quot;2020-05-20&quot;,<span class="number">1</span>); <span class="comment">-- 返回2020-05-21,1表示加1天</span></span><br><span class="line"><span class="keyword">select</span> date_add(&quot;2020-05-20&quot;,<span class="number">-1</span>); <span class="comment">-- 返回2020-05-19，-1表示减一天</span></span><br></pre></td></tr></table></figure><h3 id="date-sub-DATE-startdate-INT-days"><a href="#date-sub-DATE-startdate-INT-days" class="headerlink" title="date_sub(DATE startdate, INT days)"></a>date_sub(DATE startdate, INT days)</h3><ul><li>解释</li></ul><p>在startdate基础上减去几天，然后返回减去几天之后的一个日期,功能与date_add很类似</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> date_sub(&quot;2020-05-20&quot;,<span class="number">1</span>); <span class="comment">-- 返回2020-05-19,1表示减1天</span></span><br><span class="line"><span class="keyword">select</span> date_sub(&quot;2020-05-20&quot;,<span class="number">-1</span>); <span class="comment">-- 返回2020-05-21，-1表示加1天</span></span><br></pre></td></tr></table></figure><h3 id="date-format-DATE-TIMESTAMP-STRING-ts-STRING-fmt"><a href="#date-format-DATE-TIMESTAMP-STRING-ts-STRING-fmt" class="headerlink" title="date_format(DATE|TIMESTAMP|STRING ts, STRING fmt)"></a>date_format(DATE|TIMESTAMP|STRING ts, STRING fmt)</h3><ul><li>解释</li></ul><p>将date/timestamp/string类型的值转换为一个具体格式化的字符串。支持java的SimpleDateFormat格式，第二个参数fmt必须是一个常量</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;yyyy&#x27;</span>); <span class="comment">-- 返回2020</span></span><br><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;MM&#x27;</span>); <span class="comment">-- 返回05</span></span><br><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;dd&#x27;</span>); <span class="comment">-- 返回20</span></span><br><span class="line"><span class="comment">-- 返回2020年05月20日 00时00分00秒</span></span><br><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;yyyy年MM月dd日 HH时mm分ss秒&#x27;</span>) ;</span><br><span class="line"><span class="keyword">select</span> date_format(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;yy/MM/dd&#x27;</span>) <span class="comment">-- 返回 20/05/20</span></span><br></pre></td></tr></table></figure><h3 id="dayofmonth-STRING-date"><a href="#dayofmonth-STRING-date" class="headerlink" title="dayofmonth(STRING date)"></a>dayofmonth(STRING date)</h3><ul><li>解释</li></ul><p>返回一个日期或时间的天,与day()函数功能相同</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> dayofmonth(<span class="string">&#x27;2020-05-20&#x27;</span>) <span class="comment">-- 返回20</span></span><br></pre></td></tr></table></figure><h3 id="extract-field-FROM-source"><a href="#extract-field-FROM-source" class="headerlink" title="extract(field FROM source)"></a>extract(field FROM source)</h3><ul><li>解释</li></ul><p>&emsp;&emsp;提取 day, dayofweek, hour, minute, month, quarter, second, week 或者year的值，field可以选择day, dayofweek, hour, minute, month, quarter, second, week 或者year，source必须是一个date、timestamp或者可以转为 date 、timestamp的字符串。注意：Hive 2.2.0版本之后支持该函数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">year</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回2020，年</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(quarter <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回2，季度</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">month</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回05，月份</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(week <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回21，同weekofyear，一年中的第几周</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(dayofweek <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回4,代表星期三</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">day</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回20，天</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">hour</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回15，小时</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">minute</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回21，分钟</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">extract</span>(<span class="keyword">second</span> <span class="keyword">from</span> <span class="string">&#x27;2020-05-20 15:21:34.467&#x27;</span>); <span class="comment">-- 返回34，秒</span></span><br></pre></td></tr></table></figure><h4 id="year-STRING-date"><a href="#year-STRING-date" class="headerlink" title="year(STRING date)"></a>year(STRING date)</h4><ul><li>解释</li></ul><p>返回时间的年份,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">year</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回2020</span></span><br></pre></td></tr></table></figure><h4 id="quarter-DATE-TIMESTAMP-STRING-a"><a href="#quarter-DATE-TIMESTAMP-STRING-a" class="headerlink" title="quarter(DATE|TIMESTAMP|STRING a)"></a>quarter(DATE|TIMESTAMP|STRING a)</h4><ul><li>解释</li></ul><p>返回给定时间或日期的季度，1至4个季度,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> quarter(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回2，第2季度</span></span><br></pre></td></tr></table></figure><h4 id="month-STRING-date"><a href="#month-STRING-date" class="headerlink" title="month(STRING date)"></a>month(STRING date)</h4><ul><li>解释</li></ul><p>返回时间的月份,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">month</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>) <span class="comment">-- 返回5</span></span><br></pre></td></tr></table></figure><h4 id="day-STRING-date"><a href="#day-STRING-date" class="headerlink" title="day(STRING date)"></a>day(STRING date)</h4><ul><li>解释</li></ul><p>返回一个日期或者时间的天,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(&quot;2020-05-20&quot;); <span class="comment">-- 返回20</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">day</span>(&quot;2020-05-20 15:05:27.5&quot;); <span class="comment">-- 返回20</span></span><br></pre></td></tr></table></figure><h4 id="hour-STRING-date"><a href="#hour-STRING-date" class="headerlink" title="hour(STRING date)"></a>hour(STRING date)</h4><ul><li>解释</li></ul><p>返回一个时间的小时,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">hour</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>);<span class="comment">-- 返回15</span></span><br></pre></td></tr></table></figure><h4 id="minute-STRING-date"><a href="#minute-STRING-date" class="headerlink" title="minute(STRING date)"></a>minute(STRING date)</h4><ul><li>解释</li></ul><p>返回一个时间的分钟值,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">minute</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回21</span></span><br></pre></td></tr></table></figure><h4 id="second-STRING-date"><a href="#second-STRING-date" class="headerlink" title="second(STRING date)"></a>second(STRING date)</h4><ul><li>解释</li></ul><p>返回一个时间的秒,可以用extract函数替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">second</span>(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">--返回34</span></span><br></pre></td></tr></table></figure><h3 id="from-unixtime-BIGINT-unixtime-STRING-format"><a href="#from-unixtime-BIGINT-unixtime-STRING-format" class="headerlink" title="from_unixtime(BIGINT unixtime [, STRING format])"></a>from_unixtime(BIGINT unixtime [, STRING format])</h3><ul><li>解释</li></ul><p>将Unix时间戳转换为字符串格式的时间(比如yyyy-MM-dd HH:mm:ss格式)</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>); <span class="comment">-- 返回2020-05-20 15:45:08</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>, <span class="string">&#x27;yyyy-MM-dd hh:mm:ss&#x27;</span>); <span class="comment">-- -- 返回2020-05-20 15:45:08</span></span><br><span class="line"><span class="keyword">select</span> from_unixtime(<span class="number">1589960708</span>, <span class="string">&#x27;yyyy-MM-dd&#x27;</span>); <span class="comment">-- 返回2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="from-utc-timestamp-T-a-STRING-timezone"><a href="#from-utc-timestamp-T-a-STRING-timezone" class="headerlink" title="from_utc_timestamp(T a, STRING timezone)"></a>from_utc_timestamp(T a, STRING timezone)</h3><ul><li>解释</li></ul><p>转换为特定时区的时间</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;PST&#x27;</span>); <span class="comment">-- 返回2020-05-20 08:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;GMT&#x27;</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;UTC&#x27;</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;DST&#x27;</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br><span class="line"><span class="keyword">select</span> from_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;CST&#x27;</span>); <span class="comment">-- 返回2020-05-20 10:21:34.0</span></span><br></pre></td></tr></table></figure><h3 id="last-day-STRING-date"><a href="#last-day-STRING-date" class="headerlink" title="last_day(STRING date)"></a>last_day(STRING date)</h3><ul><li>解释</li></ul><p>返回给定时间或日期所在月的最后一天，参数可以是’yyyy-MM-dd HH:mm:ss’ 或者 ‘yyyy-MM-dd’类型，时间部分会被忽略</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> last_day(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回2020-05-31</span></span><br><span class="line"><span class="keyword">select</span> last_day(<span class="string">&#x27;2020-05-20&#x27;</span>); <span class="comment">-- 返回2020-05-31</span></span><br></pre></td></tr></table></figure><h3 id="to-date-STRING-timestamp"><a href="#to-date-STRING-timestamp" class="headerlink" title="to_date(STRING timestamp)"></a>to_date(STRING timestamp)</h3><ul><li>解释</li></ul><p>返回一个字符串时间的日期部分，去掉时间部分，2.1.0之前版本返回的是string，2.1.0版本及之后返回的是date</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> to_date(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回2020-05-20</span></span><br><span class="line"><span class="keyword">select</span> to_date(<span class="string">&#x27;2020-05-20&#x27;</span>); <span class="comment">-- 返回2020-05-20</span></span><br></pre></td></tr></table></figure><h3 id="to-utc-timestamp-T-a-STRING-timezone"><a href="#to-utc-timestamp-T-a-STRING-timezone" class="headerlink" title="to_utc_timestamp(T a, STRING timezone)"></a>to_utc_timestamp(T a, STRING timezone)</h3><ul><li>解释</li></ul><p>转换为世界标准时间UTC的时间戳,与from_utc_timestamp类似</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> to_utc_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>, <span class="string">&#x27;GMT&#x27;</span>); <span class="comment">-- 返回2020-05-20 15:21:34.0</span></span><br></pre></td></tr></table></figure><h3 id="trunc-STRING-date-STRING-format"><a href="#trunc-STRING-date-STRING-format" class="headerlink" title="trunc(STRING date, STRING format)"></a>trunc(STRING date, STRING format)</h3><ul><li>解释</li></ul><p>截断日期到指定的日期精度，仅支持月（MONTH/MON/MM）或者年（YEAR/YYYY/YY）</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> trunc(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;YY&#x27;</span>);   <span class="comment">-- 返回2020-01-01，返回年的1月1日</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">&#x27;2020-05-20&#x27;</span>, <span class="string">&#x27;MM&#x27;</span>);   <span class="comment">-- 返回2020-05-01，返回月的第一天</span></span><br><span class="line"><span class="keyword">select</span> trunc(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>, <span class="string">&#x27;MM&#x27;</span>);   <span class="comment">-- 返回2020-05-01</span></span><br></pre></td></tr></table></figure><h3 id="unix-timestamp-STRING-date-STRING-pattern"><a href="#unix-timestamp-STRING-date-STRING-pattern" class="headerlink" title="unix_timestamp([STRING date [, STRING pattern]])"></a>unix_timestamp([STRING date [, STRING pattern]])</h3><ul><li>解释</li></ul><p>参数时可选的，当参数为空时，返回当前Unix是时间戳，精确到秒。可以指定一个具体的日期，转换为Unix时间戳格式</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 返回1589959294</span></span><br><span class="line"><span class="keyword">select</span> unix_timestamp(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd hh:mm:ss&#x27;</span>);</span><br><span class="line"><span class="comment">-- 返回1589904000</span></span><br><span class="line"><span class="keyword">select</span> unix_timestamp(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;yyyy-MM-dd&#x27;</span>);</span><br></pre></td></tr></table></figure><h3 id="weekofyear-STRING-date"><a href="#weekofyear-STRING-date" class="headerlink" title="weekofyear(STRING date)"></a>weekofyear(STRING date)</h3><ul><li>解释</li></ul><p>返回一个日期或时间在一年中的第几周，可以用extract替代</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> weekofyear(<span class="string">&#x27;2020-05-20 15:21:34&#x27;</span>); <span class="comment">-- 返回21，第21周</span></span><br><span class="line"><span class="keyword">select</span> weekofyear(<span class="string">&#x27;2020-05-20&#x27;</span>); <span class="comment">-- 返回21，第21周</span></span><br></pre></td></tr></table></figure><h3 id="next-day-STRING-start-date-STRING-day-of-week"><a href="#next-day-STRING-start-date-STRING-day-of-week" class="headerlink" title="next_day(STRING start_date, STRING day_of_week)"></a>next_day(STRING start_date, STRING day_of_week)</h3><ul><li>解释</li></ul><p>参数start_date可以是一个时间或日期，day_of_week表示星期几，比如Mo表示星期一，Tu表示星期二，Wed表示星期三，Thur表示星期四，Fri表示星期五，Sat表示星期六，Sun表示星期日。如果指定的星期几在该日期所在的周且在该日期之后，则返回当周的星期几日期，如果指定的星期几不在该日期所在的周，则返回下一个星期几对应的日期</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Mon&#x27;</span>);<span class="comment">-- 返回当前日期的下一个周一日期:2020-05-25</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Tu&#x27;</span>);<span class="comment">-- 返回当前日期的下一个周二日期:2020-05-26</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Wed&#x27;</span>);<span class="comment">-- 返回当前日期的下一个周三日期:2020-05-27</span></span><br><span class="line"><span class="comment">-- 2020-05-20为周三，指定的参数为周四，所以返回当周的周四就是2020-05-21</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Th&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Fri&#x27;</span>);<span class="comment">-- 返回周五日期2020-05-22</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Sat&#x27;</span>); <span class="comment">-- 返回周六日期2020-05-23</span></span><br><span class="line"><span class="keyword">select</span> next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;Sun&#x27;</span>); <span class="comment">-- 返回周六日期2020-05-24</span></span><br></pre></td></tr></table></figure><p>该函数比较重要：比如取当前日期所在的周一和周日，通过长用在按周进行汇总数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> date_add(next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;MO&#x27;</span>),<span class="number">-7</span>); <span class="comment">-- 返回当前日期的周一日期2020-05-18</span></span><br><span class="line"><span class="keyword">select</span> date_add(next_day(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;MO&#x27;</span>),<span class="number">-1</span>); <span class="comment">-- 返回当前日期的周日日期2020-05-24</span></span><br></pre></td></tr></table></figure><h3 id="months-between-DATE-TIMESTAMP-STRING-date1-…-date2"><a href="#months-between-DATE-TIMESTAMP-STRING-date1-…-date2" class="headerlink" title="months_between(DATE|TIMESTAMP|STRING date1, … date2)"></a>months_between(DATE|TIMESTAMP|STRING date1, … date2)</h3><ul><li>解释</li></ul><p>返回 date1 和 date2 的月份差。如果date1大于date2，返回正值，否则返回负值，如果是相减是整数月，则返回一个整数，否则会返回小数</p><ul><li>使用案例</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> months_between(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;2020-05-20&#x27;</span>); <span class="comment">-- 返回0</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">&#x27;2020-05-20&#x27;</span>,<span class="string">&#x27;2020-06-20&#x27;</span>); <span class="comment">-- 返回-1</span></span><br><span class="line"><span class="comment">-- 相差的整数月</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">&#x27;2020-06-30&#x27;</span>,<span class="string">&#x27;2020-05-31&#x27;</span>); <span class="comment">-- 返回1</span></span><br><span class="line"><span class="comment">-- 非整数月，一个月差一天</span></span><br><span class="line"><span class="keyword">select</span> months_between(<span class="string">&#x27;2020-06-29&#x27;</span>,<span class="string">&#x27;2020-05-31&#x27;</span>); <span class="comment">-- 返回0.93548387</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper使用场景</title>
      <link href="/2021/10/25/ZooKeeper%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/"/>
      <url>/2021/10/25/ZooKeeper%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/</url>
      
        <content type="html"><![CDATA[<h2 id="ZooKeeper-是什么"><a href="#ZooKeeper-是什么" class="headerlink" title="ZooKeeper 是什么"></a>ZooKeeper 是什么</h2><p>&emsp;&emsp;ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。<br>&emsp;&emsp;客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的 zookeeper 机器来处理。对于写请求，这些请求会同时发给其他 zookeeper 机器并且达成一致后，请求才会返回成功。因此，随着 zookeeper 的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。有序性是 zookeeper 中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为 zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个 zookeeper 最新的 zxid。</p><h2 id="Zookeeper-工作原理"><a href="#Zookeeper-工作原理" class="headerlink" title="Zookeeper 工作原理"></a>Zookeeper 工作原理</h2><p>&emsp;&emsp;Zookeeper 的核心是原子广播，这个机制保证了各个 Server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 Server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 Server 具有相同的系统状态。</p><h2 id="ZooKeeper-提供了什么"><a href="#ZooKeeper-提供了什么" class="headerlink" title="ZooKeeper 提供了什么"></a>ZooKeeper 提供了什么</h2><p>1、文件系统<br>2、通知机制</p><h3 id="Zookeeper-文件系统"><a href="#Zookeeper-文件系统" class="headerlink" title="Zookeeper 文件系统"></a>Zookeeper 文件系统</h3><p>&emsp;&emsp;Zookeeper 提供一个多层级的节点命名空间（节点称为 znode）。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper 为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为 1M。</p><h4 id="四种类型的-znode"><a href="#四种类型的-znode" class="headerlink" title="四种类型的 znode"></a>四种类型的 znode</h4><p>1、PERSISTENT-持久化目录节点<br>客户端与 zookeeper 断开连接后，该节点依旧存在<br>2、PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点<br>客户端与 zookeeper 断开连接后，该节点依旧存在，只是 Zookeeper 给该节点名称进行顺序编号<br>3、EPHEMERAL-临时目录节点<br>客户端与 zookeeper 断开连接后，该节点被删除<br>4、EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点</p><p>&emsp;&emsp;客户端与 zookeeper 断开连接后，该节点被删除，只是 Zookeeper 给该节点名称进行顺序编号</p><h3 id="Zookeeper-通知机制"><a href="#Zookeeper-通知机制" class="headerlink" title="Zookeeper 通知机制"></a>Zookeeper 通知机制</h3><p>&emsp;&emsp;client 端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 zk 的通知，然后 client 可以根据 znode 变化来做出业务上的改变等。</p><h4 id="zookeeper-watch-机制"><a href="#zookeeper-watch-机制" class="headerlink" title="zookeeper watch 机制"></a>zookeeper watch 机制</h4><p>&emsp;&emsp;Watch 机制官方声明：一个 Watch 事件是一个一次性的触发器，当被设置了 Watch 的数据发生了改变的时候，则服务器将这个改变发送给设置了 Watch 的客户端，以便通知它们。<br>Zookeeper 机制的特点：</p><ol><li>一次性触发数据发生改变时，一个 watcher event 会被发送到 client，但是 client 只会收到一次这样的信息。</li><li>watcher event 异步发送 watcher 的通知事件从 server 发送到 client 是异步的，这就存在一个问题，不同的客户端和服务器之间通过 socket 进行通信，由于网络延迟或其他因素导致客户端在不通的时刻监听到事件，由于 Zookeeper 本身提供了 ordering guarantee，即客户端监听事件后，才会感知它所监视 znode 发生了变化。所以我们使用 Zookeeper 不能期望能够监控到节点每次的变化。Zookeeper 只能保证最终的一致性，而无法保证强一致性。</li><li>数据监视 Zookeeper 有数据监视和子数据监视 getdata() and exists()设置数据监视，getchildren()设置了子节点监视。</li><li>注册 watcher getData、exists、getChildren</li><li>触发 watcher create、delete、setData</li><li>setData()会触发 znode 上设置的 data watch（如果 set 成功的话）。一个成功的 create() 操作会触发被创建的 znode 上的数据 watch，以及其父节点上的 child watch。而一个成功的 delete()操作将会同时触发一个 znode 的 data watch 和 child watch（因为这样就没有子节点了），同时也会触发其父节点的 childwatch。</li><li>当一个客户端连接到一个新的服务器上时，watch 将会被以任意会话事件触发。当与一个服务器失去连接的时候，是无法接收到 watch 的。而当 client 重新连接时，如果需要的话，所有先前注册过的 watch，都会被重新注册。通常这是完全透明的。只有在一个特殊情况下，watch 可能会丢失：对于一个未创建的 znode 的exist watch，如果在客户端断开连接期间被创建了，并且随后在客户端连接上之前又删除了，这种情况下，这个 watch 事件可能会被丢失。</li><li>Watch 是轻量级的，其实就是本地 JVM 的 Callback，服务器端只是存了是否有设置了 Watcher 的布尔类型</li></ol><h2 id="Zookeeper-做了什么"><a href="#Zookeeper-做了什么" class="headerlink" title="Zookeeper 做了什么"></a>Zookeeper 做了什么</h2><p>1、命名服务<br>2、配置管理<br>3、集群管理<br>4、分布式锁<br>5、队列管理</p><h3 id="zk-的命名服务（文件系统）"><a href="#zk-的命名服务（文件系统）" class="headerlink" title="zk 的命名服务（文件系统）"></a>zk 的命名服务（文件系统）</h3><p>&emsp;&emsp;命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局的路径，即是唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。</p><h3 id="zk-的配置管理（文件系统、通知机制）"><a href="#zk-的配置管理（文件系统、通知机制）" class="headerlink" title="zk 的配置管理（文件系统、通知机制）"></a>zk 的配置管理（文件系统、通知机制）</h3><p>&emsp;&emsp;程序分布式的部署在不同的机器上，将程序的配置信息放在 zk 的 znode 下，当有配置发生改变时，也就是znode 发生变化时，可以通过改变 zk 中某个目录节点的内容，利用 watcher 通知给各个客户端，从而更改配置。</p><h3 id="Zookeeper-集群管理（文件系统、通知机制）"><a href="#Zookeeper-集群管理（文件系统、通知机制）" class="headerlink" title="Zookeeper 集群管理（文件系统、通知机制）"></a>Zookeeper 集群管理（文件系统、通知机制）</h3><p>&emsp;&emsp;所谓集群管理无在乎两点：是否有机器退出和加入、选举 master。<br>&emsp;&emsp;对于第一点，所有机器约定在父目录下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与zookeeper 的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。<br>&emsp;&emsp;新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount 又有了，对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为 master 就好。</p><h3 id="Zookeeper-分布式锁（文件系统、通知机制）"><a href="#Zookeeper-分布式锁（文件系统、通知机制）" class="headerlink" title="Zookeeper 分布式锁（文件系统、通知机制）"></a>Zookeeper 分布式锁（文件系统、通知机制）</h3><p>&emsp;&emsp;有了 zookeeper 的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。<br>&emsp;&emsp;对于第一类，我们将 zookeeper 上的一个 znode 看作是一把锁，通过 createznode 的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的distribute_lock 节点就释放出锁。<br>&emsp;&emsp;对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master 一样，编号最小的获得锁，用完删除，依次方便。</p><h4 id="获取分布式锁的流程"><a href="#获取分布式锁的流程" class="headerlink" title="获取分布式锁的流程"></a>获取分布式锁的流程</h4><p>&emsp;&emsp;在获取分布式锁的时候在 locker 节点下创建临时顺序节点，释放锁的时候删除该临时节点。客户端调用createNode 方法在 locker 下创建临时顺序节点，然后调用 getChildren(“locker”)来获取 locker 下面的所有子节点，注意此时不用设置任何 Watcher。客户端获取到所有的子节点 path 之后，如果发现自己创建的节点在所有创建的子节点序号最小，那么就认为该客户端获取到了锁。如果发现自己创建的节点并非 locker 所有子节点中最小的，说明自己还没有获取到锁，此时客户端需要找到比自己小的那个节点，然后对其调用 exist()方法，同时对其注册事件监听器。之后，让这个被关注的节点删除，则客户端的 Watcher 会收到相应通知，此时再次判断自己创建的节点是否是 locker 子节点中序号最小的，如果是则获取到了锁，如果不是则重复以上步骤继续获取到比自己小的一个节点并注册监听。当前这个过程中还需要许多的逻辑判断。<br>&emsp;&emsp;代码的实现主要是基于互斥锁，获取分布式锁的重点逻辑在于BaseDistributedLock，实现了基于Zookeeper 实现分布式锁的细节。</p><h3 id="Zookeeper-队列管理（文件系统、通知机制）"><a href="#Zookeeper-队列管理（文件系统、通知机制）" class="headerlink" title="Zookeeper 队列管理（文件系统、通知机制）"></a>Zookeeper 队列管理（文件系统、通知机制）</h3><p>两种类型的队列：<br>1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。<br>2、队列按照 FIFO 方式进行入队和出队操作。<br>第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。<br>第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下创建PERSISTENT_SEQUENTIAL 节点，创建成功时 Watcher 通知等待的队列，队列删除序列号最小的节点用以消费。此场景下 Zookeeper 的 znode 用于消息存储，znode 存储的数据就是消息队列中的消息内容，<br>SEQUENTIAL 序列号就是消息的编号，按序取出即可。由于创建的节点是持久化的，所以不必担心队列消息的丢失问题。</p><h2 id="Zookeeper其他问题"><a href="#Zookeeper其他问题" class="headerlink" title="Zookeeper其他问题"></a>Zookeeper其他问题</h2><h3 id="Zookeeper-数据复制"><a href="#Zookeeper-数据复制" class="headerlink" title="Zookeeper 数据复制"></a>Zookeeper 数据复制</h3><p>Zookeeper 作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处：<br>1、容错：一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作；<br>2、提高系统的扩展能力 ：把负载分布到多个节点上，或者增加节点来提高系统的负载能力；<br>3、提高性能：让客户端本地访问就近的节点，提高用户访问速度。<br>从客户端读写访问的透明度来看，数据复制集群系统分下面两种：<br>1、写主(WriteMaster) ：对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离；<br>2、写任意(Write Any)：对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。</p><p>对 zookeeper 来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这也是它建立 observer 的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。</p><h3 id="zookeeper-保证事务的顺序一致性"><a href="#zookeeper-保证事务的顺序一致性" class="headerlink" title="zookeeper 保证事务的顺序一致性"></a>zookeeper 保证事务的顺序一致性</h3><p>&emsp;&emsp;zookeeper 采用了递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid，zxid 实际上是一个 64 位的数字，高 32 位是 epoch（时期; 纪元; 世; 新时代）用来标识 leader 是否发生改变，如果有新的 leader 产生出来，epoch 会自增，低 32 位用来递增计数。当新产生 proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。</p><h3 id="Zookeeper-下-Server-工作状态"><a href="#Zookeeper-下-Server-工作状态" class="headerlink" title="Zookeeper 下 Server 工作状态"></a>Zookeeper 下 Server 工作状态</h3><p>每个 Server 在工作过程中有三种状态：<br>LOOKING：当前 Server 不知道 leader 是谁，正在搜寻<br>LEADING：当前 Server 即为选举出来的 leader<br>FOLLOWING：leader 已经选举出来，当前 Server 与之同步</p><h3 id="zookeeper-如何选取主-leader"><a href="#zookeeper-如何选取主-leader" class="headerlink" title="zookeeper 如何选取主 leader"></a>zookeeper 如何选取主 leader</h3><p>当 leader 崩溃或者 leader 失去大多数的 follower，这时 zk 进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的 Server 都恢复到一个正确的状态。Zk 的选举算法有两种：一种是基于 basic paxos 实现的，另外一种是基于 fast paxos 算法实现的。系统默认的选举算法为 fast paxos。</p><p>1、Zookeeper 选主流程(basic paxos)<br>（1）选举线程由当前 Server 发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server；<br>（2）选举线程首先向所有 Server 发起一次询问(包括自己)；<br>（3）选举线程收到回复后，验证是否是自己发起的询问(验证 zxid 是否一致)，然后获取对方的 id(myid)，并存储到当前询问对象列表中，最后获取对方提议的 leader 相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中；<br>（4）收到所有 Server 回复以后，就计算出 zxid 最大的那个 Server，并将这个 Server 相关信息设置成下一次要投票的 Server；<br>（5）线程将当前 zxid 最大的 Server 设置为当前 Server 要推荐的 Leader，如果此时获胜的 Server 获得 n/2+ 1 的 Server 票数，设置当前推荐的 leader 为获胜的 Server，将根据获胜的 Server 相关信息设置自己的状态，否则，继续这个过程，直到 leader 被选举出来。 通过流程分析我们可以得出：要使 Leader 获得多数Server 的支持，则 Server 总数必须是奇数 2n+1，且存活的 Server 的数目不得少于 n+1. 每个 Server 启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的 server 还会从磁盘快照中恢复数据和会话信息，zk 会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。</p><p>2、Zookeeper 选主流程(basic paxos)<br>fast paxos 流程是在选举过程中，某 Server 首先向所有 Server 提议自己要成为 leader，当其它 Server 收到提议以后，解决 epoch 和 zxid 的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出 Leader。</p><h3 id="Zookeeper-同步流程"><a href="#Zookeeper-同步流程" class="headerlink" title="Zookeeper 同步流程"></a>Zookeeper 同步流程</h3><p>选完 Leader 以后，zk 就进入状态同步过程。<br>1、Leader 等待 server 连接；<br>2、Follower 连接 leader，将最大的 zxid 发送给 leader；<br>3、Leader 根据 follower 的 zxid 确定同步点；<br>4、完成同步后通知 follower 已经成为 uptodate 状态；<br>5、Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。</p><h4 id="机器中为什么会有-leader"><a href="#机器中为什么会有-leader" class="headerlink" title="机器中为什么会有 leader"></a>机器中为什么会有 leader</h4><p>在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行 leader 选举。</p><h3 id="zk-节点宕机如何处理"><a href="#zk-节点宕机如何处理" class="headerlink" title="zk 节点宕机如何处理"></a>zk 节点宕机如何处理</h3><p>Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。</p><ul><li>如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失；</li><li>如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader。</li></ul><p>ZK 集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在 ZK 节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。<br>所以<br>3 个节点的 cluster 可以挂掉 1 个节点(leader 可以得到 2 票&gt;1.5)<br>2 个节点的 cluster 就不能挂掉任何 1 个节点了(leader 可以得到 1 票&lt;=1)</p><h4 id="zookeeper-负载均衡和-nginx-负载均衡区别"><a href="#zookeeper-负载均衡和-nginx-负载均衡区别" class="headerlink" title="zookeeper 负载均衡和 nginx 负载均衡区别"></a>zookeeper 负载均衡和 nginx 负载均衡区别</h4><p>zk 的负载均衡是可以调控，nginx 只是能调权重，其他需要可控的都需要自己写插件；但是 nginx 的吞吐量比zk 大很多，应该说按业务选择用哪种方式。</p>]]></content>
      
      
      
        <tags>
            
            <tag> zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop一致性探讨</title>
      <link href="/2021/10/25/Sqoop%E4%B8%80%E8%87%B4%E6%80%A7%E6%8E%A2%E8%AE%A8/"/>
      <url>/2021/10/25/Sqoop%E4%B8%80%E8%87%B4%E6%80%A7%E6%8E%A2%E8%AE%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Sqoop导入导出Null存储一致性问题"><a href="#Sqoop导入导出Null存储一致性问题" class="headerlink" title="Sqoop导入导出Null存储一致性问题"></a>Sqoop导入导出Null存储一致性问题</h2><p>Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用–input-null-string和–input-null-non-string两个参数。导入数据时采用–null-string和–null-non-string。</p><h2 id="Sqoop数据导出一致性问题"><a href="#Sqoop数据导出一致性问题" class="headerlink" title="Sqoop数据导出一致性问题"></a>Sqoop数据导出一致性问题</h2><h3 id="场景1："><a href="#场景1：" class="headerlink" title="场景1："></a>场景1：</h3><p>如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。</p><h4 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h4><p>由于Sqoop将导出过程分解为多个事务，因此失败的导出作业可能会导致部分数据提交到数据库。在某些情况下，这可能进一步导致后续作业因插入冲突而失败，在其他情况下，这又可能导致数据重复。您可以通过–staging-table选项指定暂存表来解决此问题，该选项用作用于暂存导出数据的辅助表。最后，分阶段处理的数据将在单个事务中移至目标表</p><h5 id="命令："><a href="#命令：" class="headerlink" title="命令："></a>命令：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://192.168.137.10:3306/user_behavior \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table app_cource_study_report \</span><br><span class="line">--columns watch_video_cnt,complete_video_cnt,dt \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--export-dir &quot;/user/hive/warehouse/tmp.db/app_cource_study_analysis_$&#123;day&#125;&quot; \</span><br><span class="line">--staging-table app_cource_study_report_tmp \</span><br><span class="line">--clear-staging-table \</span><br><span class="line">--input-null-string &#x27;\N&#x27;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop常用命令及参数</title>
      <link href="/2021/10/25/Sqoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%8F%82%E6%95%B0/"/>
      <url>/2021/10/25/Sqoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%8F%82%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="Sqoop-常用命令及参数"><a href="#Sqoop-常用命令及参数" class="headerlink" title="Sqoop 常用命令及参数"></a>Sqoop 常用命令及参数</h2><h3 id="常用命令列举"><a href="#常用命令列举" class="headerlink" title="常用命令列举"></a>常用命令列举</h3><table><thead><tr><th align="left">序号</th><th align="left">命令</th><th align="left">类</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">import</td><td align="left">ImportTool</td><td align="left">将数据导入到集群</td></tr><tr><td align="left">2</td><td align="left">export</td><td align="left">ExportTool</td><td align="left">将集群数据导出</td></tr><tr><td align="left">3</td><td align="left">codegen</td><td align="left">CodeGenTool</td><td align="left">获取数据库中某张表数据生成Java 并打包Jar</td></tr><tr><td align="left">4</td><td align="left">create-hive-table</td><td align="left">CreateHiveTableTool</td><td align="left">创建 Hive 表</td></tr><tr><td align="left">5</td><td align="left">eval</td><td align="left">EvalSqlTool</td><td align="left">查看 SQL 执行结果</td></tr><tr><td align="left">6</td><td align="left">import-all-tables</td><td align="left">ImportAllTablesTool</td><td align="left">导入某个数据库下所有表到 HDFS 中</td></tr><tr><td align="left">7</td><td align="left">job</td><td align="left">JobTool</td><td align="left">用来生成一个 sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务</td></tr><tr><td align="left">8</td><td align="left">list-databases</td><td align="left">ListDatabasesTool</td><td align="left">列出所有数据库名</td></tr><tr><td align="left">9</td><td align="left">list-tables</td><td align="left">ListTablesTool</td><td align="left">列出某个数据库下所有表</td></tr><tr><td align="left">10</td><td align="left">merge</td><td align="left">MergeTool</td><td align="left">将 HDFS 中不同目录下面的数据合在一起，并存放在指定的目录中</td></tr><tr><td align="left">11</td><td align="left">metastore</td><td align="left">MetastoreTool</td><td align="left">记录 sqoop job 的元数据信息，如果不启动 metastore 实例，则默认的元数据存储目录为：~/.sqoop，如果要更改存储目录，可以 在 配 置 文 件sqoop-site.xml 中进行更改</td></tr></tbody></table><h3 id="命令-amp-参数详解"><a href="#命令-amp-参数详解" class="headerlink" title="命令&amp;参数详解"></a>命令&amp;参数详解</h3><p>对于不同的命令，有不同的参数.</p><h4 id="公用参数：数据库连接"><a href="#公用参数：数据库连接" class="headerlink" title="公用参数：数据库连接"></a>公用参数：数据库连接</h4><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">–connect</td><td align="left">连接关系型数据库的 URL</td></tr><tr><td align="left">2</td><td align="left">–connection-manager</td><td align="left">指定要使用的连接管理类</td></tr><tr><td align="left">3</td><td align="left">–driver</td><td align="left">Hadoop 根目录</td></tr><tr><td align="left">4</td><td align="left">–help</td><td align="left">打印帮助信息</td></tr><tr><td align="left">5</td><td align="left">–password</td><td align="left">连接数据库的密码</td></tr><tr><td align="left">6</td><td align="left">–username</td><td align="left">连接数据库的用户名</td></tr><tr><td align="left">7</td><td align="left">–verbose</td><td align="left">在控制台打印出详细信息</td></tr></tbody></table><h4 id="公用参数：import"><a href="#公用参数：import" class="headerlink" title="公用参数：import"></a>公用参数：import</h4><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">–enclosed-by <char></td><td align="left">给字段值前加上指定的字符</td></tr><tr><td align="left">2</td><td align="left">–escaped-by <char></td><td align="left">对字段中的双引号加转义符</td></tr><tr><td align="left">3</td><td align="left">–fields-terminated-by <char></td><td align="left">设定每个字段是以什么符号作为结束，默认为逗号</td></tr><tr><td align="left">4</td><td align="left">–lines-terminated-by <char></td><td align="left">设定每行记录之间的分隔符，默认是\n</td></tr><tr><td align="left">5</td><td align="left">–mysql-delimiters</td><td align="left">Mysql 默认的分隔符设置，字段之间以逗号分隔，行之间以\n 分隔，默认转义符是\，字段值以单引号包裹。</td></tr><tr><td align="left">6</td><td align="left">–optionally-enclosed-by <char></td><td align="left">给带有双引号或单引号的字段值前后加上指定字符。</td></tr></tbody></table><h4 id="公用参数：export"><a href="#公用参数：export" class="headerlink" title="公用参数：export"></a>公用参数：export</h4><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">–input-enclosed-by <char></td><td align="left">对字段值前后加上指定字符</td></tr><tr><td align="left">2</td><td align="left">–input-escaped-by <char></td><td align="left">对含有转移符的字段做转义处理</td></tr><tr><td align="left">3</td><td align="left">–input-fields-terminated-by <char></td><td align="left">字段之间的分隔符</td></tr><tr><td align="left">4</td><td align="left">–input-lines-terminated-by <char></td><td align="left">行之间的分隔符</td></tr><tr><td align="left">5</td><td align="left">–input-optionally-enclosed-by <char></td><td align="left">给带有双引号或单引号的字段前后加上指定字符</td></tr></tbody></table><h4 id="公用参数：hive"><a href="#公用参数：hive" class="headerlink" title="公用参数：hive"></a>公用参数：hive</h4><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">–hive-delims-replacement <arg></td><td align="left">用自定义的字符串替换掉数据中的\r\n和\013 \010等字符</td></tr><tr><td align="left">2</td><td align="left">–hive-drop-import-delims</td><td align="left">在导入数据到 hive 时，去掉数据中的\r\n\013\010 这样的字符</td></tr><tr><td align="left">3</td><td align="left">–map-column-hive <arg></td><td align="left">生成 hive 表时，可以更改生成字段的数据类型</td></tr><tr><td align="left">4</td><td align="left">–hive-partition-key</td><td align="left">创建分区，后面直接跟分区名，分区字段的默认类型为string</td></tr><tr><td align="left">5</td><td align="left">–hive-partition-value <v></td><td align="left">导入数据时，指定某个分区的值</td></tr><tr><td align="left">6</td><td align="left">–hive-home <dir></td><td align="left">hive 的安装目录，可以通过该参数覆盖之前默认配置的目录</td></tr><tr><td align="left">7</td><td align="left">–hive-import</td><td align="left">将数据从关系数据库中导入到 hive 表中</td></tr><tr><td align="left">8</td><td align="left">–hive-overwrite</td><td align="left">覆盖掉在 hive 表中已经存在的数据</td></tr><tr><td align="left">9</td><td align="left">–create-hive-table</td><td align="left">默认是 false，即，如果目标表已经存在了，那么创建任务失败。</td></tr><tr><td align="left">10</td><td align="left">–hive-table</td><td align="left">后面接要创建的 hive 表,默认使用 MySQL 的表名</td></tr><tr><td align="left">11</td><td align="left">–table</td><td align="left">指定关系数据库的表名</td></tr></tbody></table><h4 id="命令-amp-参数：import"><a href="#命令-amp-参数：import" class="headerlink" title="命令&amp;参数：import"></a>命令&amp;参数：import</h4><p>将关系型数据库中的数据导入到 HDFS（包括 Hive，HBase）中，如果导入的是 Hive，那么<br>当 Hive 中没有对应表时，则自动创建。</p><h5 id="1-命令："><a href="#1-命令：" class="headerlink" title="1) 命令："></a>1) 命令：</h5><h6 id="如：导入数据到-hive-中"><a href="#如：导入数据到-hive-中" class="headerlink" title="如：导入数据到 hive 中"></a>如：导入数据到 hive 中</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--hive-import</span></span><br></pre></td></tr></table></figure><h6 id="如：增量导入数据到-hive-中，mode-append"><a href="#如：增量导入数据到-hive-中，mode-append" class="headerlink" title="如：增量导入数据到 hive 中，mode=append"></a>如：增量导入数据到 hive 中，mode=append</h6><p>append 导入：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--num-mappers 1 \</span></span><br><span class="line"><span class="bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="bash">--target-dir /user/hive/warehouse/staff_hive \</span></span><br><span class="line"><span class="bash">--check-column id \</span></span><br><span class="line"><span class="bash">--incremental append \</span></span><br><span class="line"><span class="bash">--last-value 3</span></span><br></pre></td></tr></table></figure><p>尖叫提示：append 不能与–hive-等参数同时使用（Append mode for hive imports is not yet supported. Please remove the parameter –append-mode）</p><h6 id="如：增量导入数据到-hdfs-中，mode-lastmodified"><a href="#如：增量导入数据到-hdfs-中，mode-lastmodified" class="headerlink" title="如：增量导入数据到 hdfs 中，mode=lastmodified"></a>如：增量导入数据到 hdfs 中，mode=lastmodified</h6><p>先在 mysql 中建表并插入几条数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> company.staff_timestamp(id <span class="type">int</span>(<span class="number">4</span>), name <span class="type">varchar</span>(<span class="number">255</span>), sex <span class="type">varchar</span>(<span class="number">255</span>),</span><br><span class="line">last_modified <span class="type">timestamp</span> <span class="keyword">DEFAULT</span> <span class="built_in">CURRENT_TIMESTAMP</span> <span class="keyword">ON</span> UPDATE</span><br><span class="line"><span class="built_in">CURRENT_TIMESTAMP</span>);</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> company.staff_timestamp (id, name, sex) <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">&#x27;AAA&#x27;</span>, <span class="string">&#x27;female&#x27;</span>);</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> company.staff_timestamp (id, name, sex) <span class="keyword">values</span>(<span class="number">2</span>, <span class="string">&#x27;BBB&#x27;</span>, <span class="string">&#x27;female&#x27;</span>);</span><br></pre></td></tr></table></figure><p>先导入一部分数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff_timestamp \</span></span><br><span class="line"><span class="bash">--delete-target-dir \</span></span><br><span class="line"><span class="bash">--m 1</span></span><br></pre></td></tr></table></figure><p>再增量导入一部分数据：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> insert into company.staff_timestamp (id, name, sex) values(3, <span class="string">&#x27;CCC&#x27;</span>, <span class="string">&#x27;female&#x27;</span>);</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff_timestamp \</span></span><br><span class="line"><span class="bash">--check-column last_modified \</span></span><br><span class="line"><span class="bash">--incremental lastmodified \</span></span><br><span class="line"><span class="bash">--last-value <span class="string">&quot;2017-09-28 22:20:38&quot;</span> \</span></span><br><span class="line"><span class="bash">--m 1 \</span></span><br><span class="line"><span class="bash">--append</span></span><br></pre></td></tr></table></figure><p>尖叫提示：使用 lastmodified 方式导入数据要指定增量数据是要–append（追加）还是要–merge-key（合并）<br>尖叫提示：last-value 指定的值是会包含于增量导入的数据中</p><h5 id="2-参数："><a href="#2-参数：" class="headerlink" title="2) 参数："></a>2) 参数：</h5><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">- -append</td><td align="left">将数据追加到 HDFS 中已经存在的 DataSet 中，如果使用该参数，sqoop 会把数据先导入到临时文件目录，再合并。</td></tr><tr><td align="left">2</td><td align="left">- -as-avrodatafile</td><td align="left">将数据导入到一个 Avro 数据文件中</td></tr><tr><td align="left">3</td><td align="left">- -as-sequencefile</td><td align="left">将数据导入到一个 sequence文件中</td></tr><tr><td align="left">4</td><td align="left">- -as-textfile</td><td align="left">将数据导入到一个普通文本文件中</td></tr><tr><td align="left">5</td><td align="left">- -boundary-query <statement></td><td align="left">边界查询，导入的数据为该参数的值（一条 sql 语句）所执行的结果区间内的数据。</td></tr><tr><td align="left">6</td><td align="left">- -columns &lt;col1, col2, col3&gt;</td><td align="left">指定要导入的字段</td></tr><tr><td align="left">7</td><td align="left">- -direct</td><td align="left">直接导入模式，使用的是关系数据库自带的导入导出工具，以便加快导入导出过程。</td></tr><tr><td align="left">8</td><td align="left">- -direct-split-size</td><td align="left">在使用上面 direct 直接导入的基础上，对导入的流按字节分块，即达到该阈值就产生一个新的文件</td></tr><tr><td align="left">9</td><td align="left">- -inline-lob-limit</td><td align="left">设定大对象数据类型的最大值</td></tr><tr><td align="left">10</td><td align="left">–m 或–num-mappers</td><td align="left">启动 N 个 map 来并行导入数据，默认 4 个。</td></tr><tr><td align="left">11</td><td align="left">–query 或–e <statement></td><td align="left">将查询结果的数据导入，使用时必须伴随参–target-dir，–hive-table，如果查询中有where 条件，则条件后必须加上$CONDITIONS 关键字</td></tr><tr><td align="left">12</td><td align="left">–split-by <column-name></td><td align="left">按照某一列来切分表的工作单元，不能与–autoreset-to-one-mapper 连用</td></tr><tr><td align="left">13</td><td align="left">–table <table-name></td><td align="left">关系数据库的表名</td></tr><tr><td align="left">14</td><td align="left">–target-dir <dir></td><td align="left">指定 HDFS 路径</td></tr><tr><td align="left">15</td><td align="left">–warehouse-dir <dir></td><td align="left">与 14 参数不能同时使用，导入数据到 HDFS 时指定的目录</td></tr><tr><td align="left">16</td><td align="left">–where</td><td align="left">从关系数据库导入数据时的查询条件</td></tr><tr><td align="left">17</td><td align="left">–z 或–compress</td><td align="left">允许压缩</td></tr><tr><td align="left">18</td><td align="left">–compression-codec</td><td align="left">指定 hadoop 压缩编码类，默认为 gzip(Use Hadoop codecdefault gzip)</td></tr><tr><td align="left">19</td><td align="left">–null-string <null-string></td><td align="left">string 类型的列如果 null，替换为指定字符串</td></tr><tr><td align="left">20</td><td align="left">–null-non-string <null-string></td><td align="left">非 string 类型的列如果 null，替换为指定字符串</td></tr><tr><td align="left">21</td><td align="left">–check-column <col></td><td align="left">作为增量导入判断的列名</td></tr><tr><td align="left">22</td><td align="left">–incremental <mode></td><td align="left">mode：append 或 lastmodified</td></tr><tr><td align="left">23</td><td align="left">–last-value <value></td><td align="left">指定某一个值，用于标记增量导入的位置</td></tr></tbody></table><h4 id="命令-amp-参数：export"><a href="#命令-amp-参数：export" class="headerlink" title="命令&amp;参数：export"></a>命令&amp;参数：export</h4><p>从 HDFS（包括 Hive 和 HBase）中奖数据导出到关系型数据库中。</p><h5 id="1-命令：-1"><a href="#1-命令：-1" class="headerlink" title="1) 命令："></a>1) 命令：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop <span class="built_in">export</span> \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--export-dir /user/company \</span></span><br><span class="line"><span class="bash">--input-fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="bash">--num-mappers 1</span></span><br></pre></td></tr></table></figure><h5 id="2-参数：-1"><a href="#2-参数：-1" class="headerlink" title="2) 参数："></a>2) 参数：</h5><table><thead><tr><th>序号</th><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>1</td><td>–direct</td><td>利用数据库自带的导入导出工具，以便于提高效率</td></tr><tr><td>2</td><td>–export-dir <dir></td><td>存放数据的 HDFS 的源目录</td></tr><tr><td>3</td><td>m 或–num-mappers <n></td><td>启动 N 个 map 来并行导入数据，默认 4 个</td></tr><tr><td>4</td><td>–table <table-name></td><td>指定导出到哪个 RDBMS 中的表</td></tr><tr><td>5</td><td>–update-key <col-name></td><td>对某一列的字段进行更新操作</td></tr><tr><td>6</td><td>–update-mode <mode></td><td>updateonlyallowinsert(默认)</td></tr><tr><td>7</td><td>–input-null-string <null-string></td><td>请参考 import</td></tr><tr><td>8</td><td>–input-null-non-string <null-string></td><td>请参考 import</td></tr><tr><td>9</td><td>–staging-table <staging-table-name></td><td>创建一张临时表，用于存放所有事务的结果，然后将所有事务结果一次性导入到目标表中，防止错误。</td></tr><tr><td>10</td><td>–clear-staging-table</td><td>如果第 9 个参数非空，则可以在导出操作执行前，清空临时事务结果表</td></tr></tbody></table><h4 id="命令-amp-参数：codegen"><a href="#命令-amp-参数：codegen" class="headerlink" title="命令&amp;参数：codegen"></a>命令&amp;参数：codegen</h4><p>将关系型数据库中的表映射为一个 Java 类，在该类中有各列对应的各个字段。</p><h5 id="1-命令：-2"><a href="#1-命令：-2" class="headerlink" title="1) 命令："></a>1) 命令：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop codegen \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--bindir /home/admin/Desktop/staff \</span></span><br><span class="line"><span class="bash">--class-name Staff \</span></span><br><span class="line"><span class="bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span></span><br></pre></td></tr></table></figure><h5 id="2-参数：-2"><a href="#2-参数：-2" class="headerlink" title="2) 参数："></a>2) 参数：</h5><table><thead><tr><th>序号</th><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>1</td><td>–bindir <dir></td><td>指定生成的 Java 文件、编译成的 class 文件及将生成文件打包为 jar 的文件输出路径</td></tr><tr><td>2</td><td>–class-name <name></td><td>设定生成的 Java 文件指定的名称</td></tr><tr><td>3</td><td>–outdir <dir></td><td>生成 Java 文件存放的路径</td></tr><tr><td>4</td><td>–package-name <name></td><td>包名，如 com.z，就会生成 com和 z 两级目录</td></tr><tr><td>5</td><td>–input-null-non-string <null-str></td><td>在生成的 Java 文件中，可以将 null 字符串或者不存在的字符串设置为想要设定的值（例如空字符串）</td></tr><tr><td>6</td><td>–input-null-string <null-str></td><td>将 null 字符串替换成想要替换的值（一般与 5 同时使用）</td></tr><tr><td>7</td><td>–map-column-java <arg></td><td>数据库字段在生成的 Java 文件中会映射成各种属性，且默认的数据类型与数据库类型保持对应关系。该参数可以改变默认类型，例如：–map-column-java id=long,name=String</td></tr><tr><td>8</td><td>–null-non-string <null-str></td><td>在生成 Java 文件时，可以将不存在或者 null 的字符串设置为其他值</td></tr><tr><td>9</td><td>–null-string <null-str></td><td>在生成 Java 文件时，将 null字符串设置为其他值（一般与8 同时用）</td></tr><tr><td>10</td><td>–table <table-name></td><td>对应关系数据库中的表名，生成的 Java 文件中的各个属性与该表的各个字段一一对应</td></tr></tbody></table><h4 id="命令-amp-参数：create-hive-table"><a href="#命令-amp-参数：create-hive-table" class="headerlink" title="命令&amp;参数：create-hive-table"></a>命令&amp;参数：create-hive-table</h4><p>生成与关系数据库表结构对应的 hive 表结构。</p><h5 id="1-命令：-3"><a href="#1-命令：-3" class="headerlink" title="1) 命令："></a>1) 命令：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop create-hive-table \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--hive-table hive_staff</span></span><br></pre></td></tr></table></figure><h5 id="2-参数：-3"><a href="#2-参数：-3" class="headerlink" title="2) 参数："></a>2) 参数：</h5><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">–hive-home <dir></td><td align="left">Hive 的安装目录，可以通过该参数覆盖掉默认的 Hive 目录</td></tr><tr><td align="left">2</td><td align="left">–hive-overwrite</td><td align="left">覆盖掉在 Hive 表中已经存在的数据</td></tr><tr><td align="left">3</td><td align="left">–create-hive-table</td><td align="left">默认是 false，如果目标表已经存在了，那么创建任务会失败</td></tr><tr><td align="left">4</td><td align="left">–hive-table</td><td align="left">后面接要创建的 hive 表</td></tr><tr><td align="left">5</td><td align="left">–table</td><td align="left">指定关系数据库的表名</td></tr></tbody></table><h4 id="命令-amp-参数：eval"><a href="#命令-amp-参数：eval" class="headerlink" title="命令&amp;参数：eval"></a>命令&amp;参数：eval</h4><p>可以快速的使用 SQL 语句对关系型数据库进行操作，经常用于在 import 数据之前，了解一<br>下 SQL 语句是否正确，数据是否正常，并可以将结果显示在控制台。</p><h5 id="1-命令：-4"><a href="#1-命令：-4" class="headerlink" title="1) 命令："></a>1) 命令：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop <span class="built_in">eval</span> \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--query <span class="string">&quot;SELECT * FROM staff&quot;</span></span></span><br></pre></td></tr></table></figure><h5 id="2-参数：-4"><a href="#2-参数：-4" class="headerlink" title="2) 参数："></a>2) 参数：</h5><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">–query 或–e</td><td align="left">后跟查询的 SQL 语句</td></tr></tbody></table><h4 id="命令-amp-参数：import-all-tables"><a href="#命令-amp-参数：import-all-tables" class="headerlink" title="命令&amp;参数：import-all-tables"></a>命令&amp;参数：import-all-tables</h4><p>可以将 RDBMS 中的所有表导入到 HDFS 中，每一个表都对应一个 HDFS 目录</p><h5 id="1-命令：-5"><a href="#1-命令：-5" class="headerlink" title="1) 命令："></a>1) 命令：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import-all-tables \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--warehouse-dir /all_tables</span></span><br></pre></td></tr></table></figure><h5 id="2-参数"><a href="#2-参数" class="headerlink" title="2) 参数"></a>2) 参数</h5><p>这些参数的含义均和 import 对应的含义一致<br>| 序号   |  参数  |<br>| :———— | :———— |<br>| 1  | –as-avrodatafile   |<br>| 2  | –as-sequencefile  |<br>| 3  | –as-textfile  |<br>| 4  | –direct  |<br>| 5  | –direct-split-size <n>  |<br>| 6  | –inline-lob-limit <n>  |<br>| 7  | –m 或—num-mappers <n>  |<br>| 8  | –warehouse-dir <dir> |<br>| 9  | -z 或–compress  |<br>| 10  | –compression-codec  |</p><h4 id="命令-amp-参数：job"><a href="#命令-amp-参数：job" class="headerlink" title="命令&amp;参数：job"></a>命令&amp;参数：job</h4><p>用来生成一个 sqoop 任务，生成后不会立即执行，需要手动执行。</p><h5 id="1-命令：-6"><a href="#1-命令：-6" class="headerlink" title="1) 命令："></a>1) 命令：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop job \</span></span><br><span class="line"><span class="bash">--create myjob -- import-all-tables \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop job \</span></span><br><span class="line"><span class="bash">--list</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop job \</span></span><br><span class="line"><span class="bash">--<span class="built_in">exec</span> myjob</span></span><br></pre></td></tr></table></figure><p>尖叫提示：注意 import-all-tables 和它左边的–之间有一个空格<br>尖叫提示：如果需要连接 metastore，则–meta-connect jdbc:hsqldb:hsql://linux01:16000/sqoop</p><h5 id="2-参数-1"><a href="#2-参数-1" class="headerlink" title="2) 参数"></a>2) 参数</h5><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">–create <job-id></td><td align="left">创建 job 参数</td></tr><tr><td align="left">2</td><td align="left">–delete <job-id></td><td align="left">删除一个 job</td></tr><tr><td align="left">3</td><td align="left">–exec <job-id></td><td align="left">执行一个 job</td></tr><tr><td align="left">4</td><td align="left">–help</td><td align="left">显示 job 帮助</td></tr><tr><td align="left">5</td><td align="left">–list</td><td align="left">显示 job 列表</td></tr><tr><td align="left">6</td><td align="left">–meta-connect <jdbc-uri></td><td align="left">用来连接 metastore 服务</td></tr><tr><td align="left">7</td><td align="left">–show <job-id></td><td align="left">显示一个 job 的信息</td></tr><tr><td align="left">8</td><td align="left">–verbose</td><td align="left">打印命令运行时的详细信息</td></tr><tr><td align="left">尖叫提示：在执行一个 job 时，如果需要手动输入数据库密码，可以做如下优化</td><td align="left"></td><td align="left"></td></tr></tbody></table><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>sqoop.metastore.client.record.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, allow saved passwords in the metastore.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="命令-amp-参数：list-databases"><a href="#命令-amp-参数：list-databases" class="headerlink" title="命令&amp;参数：list-databases"></a>命令&amp;参数：list-databases</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop list-databases \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/ \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000</span></span><br></pre></td></tr></table></figure><h4 id="命令-amp-参数：list-tables"><a href="#命令-amp-参数：list-tables" class="headerlink" title="命令&amp;参数：list-tables"></a>命令&amp;参数：list-tables</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop list-tables \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000</span></span><br></pre></td></tr></table></figure><h4 id="命令-amp-参数：merge"><a href="#命令-amp-参数：merge" class="headerlink" title="命令&amp;参数：merge"></a>命令&amp;参数：merge</h4><p>将 HDFS 中不同目录下面的数据合并在一起并放入指定目录中</p><h5 id="数据环境："><a href="#数据环境：" class="headerlink" title="数据环境："></a>数据环境：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">new_staff</span><br><span class="line">1 AAA male</span><br><span class="line">2 BBB male</span><br><span class="line">3 CCC male</span><br><span class="line">4 DDD male</span><br><span class="line">old_staff</span><br><span class="line">1 AAA female</span><br><span class="line">2 CCC female</span><br><span class="line">3 BBB female</span><br><span class="line">6 DDD female</span><br></pre></td></tr></table></figure><p>尖叫提示：上边数据的列之间的分隔符应该为\t，行与行之间的分割符为\n，如果直接复制，请检查之。</p><h5 id="命令："><a href="#命令：" class="headerlink" title="命令："></a>命令：</h5><p>创建 JavaBean：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop codegen \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--bindir /home/admin/Desktop/staff \</span></span><br><span class="line"><span class="bash">--class-name Staff \</span></span><br><span class="line"><span class="bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span></span><br></pre></td></tr></table></figure><p>开始合并：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop merge \</span></span><br><span class="line"><span class="bash">--new-data /<span class="built_in">test</span>/new/ \</span></span><br><span class="line"><span class="bash">--onto /<span class="built_in">test</span>/old/ \</span></span><br><span class="line"><span class="bash">--target-dir /<span class="built_in">test</span>/merged \</span></span><br><span class="line"><span class="bash">--jar-file /home/admin/Desktop/staff/Staff.jar \</span></span><br><span class="line"><span class="bash">--class-name Staff \</span></span><br><span class="line"><span class="bash">--merge-key id</span></span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1 AAA MALE</span><br><span class="line">2 BBB MALE</span><br><span class="line">3 CCC MALE</span><br><span class="line">4 DDD MALE</span><br><span class="line">6 DDD FEMALE</span><br></pre></td></tr></table></figure><h5 id="参数："><a href="#参数：" class="headerlink" title="参数："></a>参数：</h5><table><thead><tr><th align="left">序号</th><th align="left">参数</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">–new-data <path></td><td align="left">HDFS 待合并的数据目录，合并后在新的数据集中保留</td></tr><tr><td align="left">2</td><td align="left">–onto <path></td><td align="left">HDFS 合并后，重复的部分在新的数据集中被覆盖</td></tr><tr><td align="left">3</td><td align="left">–merge-key <col></td><td align="left">合并键，一般是主键 ID</td></tr><tr><td align="left">4</td><td align="left">–jar-file <file></td><td align="left">合并时引入的 jar 包，该 jar包是通过 Codegen 工具生成的 jar 包</td></tr><tr><td align="left">5</td><td align="left">–class-name <class></td><td align="left">对应的表名或对象名，该class 类是包含在 jar 包中的</td></tr><tr><td align="left">6</td><td align="left">–target-dir <path></td><td align="left">合并后的数据在 HDFS 里存放的目录</td></tr></tbody></table><h4 id="命令-amp-参数：metastore"><a href="#命令-amp-参数：metastore" class="headerlink" title="命令&amp;参数：metastore"></a>命令&amp;参数：metastore</h4><p>记录了 Sqoop job 的元数据信息，如果不启动该服务，那么默认 job 元数据的存储目录为<br>~/.sqoop，可在 sqoop-site.xml 中修改。<br>启动 sqoop 的 metastore 服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop metastore</span></span><br></pre></td></tr></table></figure><p>关闭 sqoop 的 metastore 服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop metastore --shutdown</span>  </span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sqoop使用案例</title>
      <link href="/2021/10/25/Sqoop%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
      <url>/2021/10/25/Sqoop%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Sqoop-原理"><a href="#Sqoop-原理" class="headerlink" title="Sqoop 原理"></a>Sqoop 原理</h2><p>将导入或导出命令翻译成 mapreduce 程序来实现。<br>在翻译出的 mapreduce 中主要是对 inputformat 和 outputformat 进行定制。</p><h3 id="测试-Sqoop-是否能够成功连接数据库"><a href="#测试-Sqoop-是否能够成功连接数据库" class="headerlink" title="测试 Sqoop 是否能够成功连接数据库"></a>测试 Sqoop 是否能够成功连接数据库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/</span></span><br><span class="line">--username root --password 000000</span><br></pre></td></tr></table></figure><h2 id="Sqoop-的简单使用案例"><a href="#Sqoop-的简单使用案例" class="headerlink" title="Sqoop 的简单使用案例"></a>Sqoop 的简单使用案例</h2><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p>在 Sqoop 中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，<br>HBASE）中传输数据，叫做：导入，即使用 import 关键字。</p><h4 id="（1）全部导入"><a href="#（1）全部导入" class="headerlink" title="（1）全部导入"></a>（1）全部导入</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--target-dir /user/company \</span></span><br><span class="line"><span class="bash">--delete-target-dir \</span></span><br><span class="line"><span class="bash">--num-mappers 1 \</span></span><br><span class="line"><span class="bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span></span></span><br></pre></td></tr></table></figure><h4 id="（2）查询导入"><a href="#（2）查询导入" class="headerlink" title="（2）查询导入"></a>（2）查询导入</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--target-dir /user/company \</span></span><br><span class="line"><span class="bash">--delete-target-dir \</span></span><br><span class="line"><span class="bash">--num-mappers 1 \</span></span><br><span class="line"><span class="bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="bash">--query <span class="string">&#x27;select name,sex from staff where id &lt;=1 and $CONDITIONS;&#x27;</span></span></span><br></pre></td></tr></table></figure><p>提示：<code>must contain &#39;$CONDITIONS&#39; in WHERE clause</code>.<br>如果 query 后使用的是双引号，则$CONDITIONS 前必须加转移符，防止 shell 识别为自己的<br>变量。</p><h4 id="（3）导入指定列"><a href="#（3）导入指定列" class="headerlink" title="（3）导入指定列"></a>（3）导入指定列</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--target-dir /user/company \</span></span><br><span class="line"><span class="bash">--delete-target-dir \</span></span><br><span class="line"><span class="bash">--num-mappers 1 \</span></span><br><span class="line"><span class="bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="bash">--columns id,sex \</span></span><br><span class="line"><span class="bash">--table staff</span></span><br></pre></td></tr></table></figure><p>提示：columns 中如果涉及到多列，用逗号分隔，分隔时不要添加空格</p><h4 id="（4）使用-sqoop-关键字筛选查询导入数据"><a href="#（4）使用-sqoop-关键字筛选查询导入数据" class="headerlink" title="（4）使用 sqoop 关键字筛选查询导入数据"></a>（4）使用 sqoop 关键字筛选查询导入数据</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--target-dir /user/company \</span></span><br><span class="line"><span class="bash">--delete-target-dir \</span></span><br><span class="line"><span class="bash">--num-mappers 1 \</span></span><br><span class="line"><span class="bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--<span class="built_in">where</span> <span class="string">&quot;id=1&quot;</span></span></span><br></pre></td></tr></table></figure><h4 id="RDBMS-到-Hive"><a href="#RDBMS-到-Hive" class="headerlink" title="RDBMS 到 Hive"></a>RDBMS 到 Hive</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--num-mappers 1 \</span></span><br><span class="line"><span class="bash">--hive-import \</span></span><br><span class="line"><span class="bash">--fields-terminated-by <span class="string">&quot;\t&quot;</span> \</span></span><br><span class="line"><span class="bash">--hive-overwrite \</span></span><br><span class="line"><span class="bash">--hive-table staff_hive</span></span><br></pre></td></tr></table></figure><p>提示：该过程分为两步，第一步将数据导入到 HDFS，第二步将导入到 HDFS 的数据迁移到<br>Hive 仓库，第一步默认的临时目录是/user/atguigu/表名</p><h4 id="RDBMS-到-Hbase"><a href="#RDBMS-到-Hbase" class="headerlink" title="RDBMS 到 Hbase"></a>RDBMS 到 Hbase</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop import \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table company \</span></span><br><span class="line"><span class="bash">--columns <span class="string">&quot;id,name,sex&quot;</span> \</span></span><br><span class="line"><span class="bash">--column-family <span class="string">&quot;info&quot;</span> \</span></span><br><span class="line"><span class="bash">--hbase-create-table \</span></span><br><span class="line"><span class="bash">--hbase-row-key <span class="string">&quot;id&quot;</span> \</span></span><br><span class="line"><span class="bash">--hbase-table <span class="string">&quot;hbase_company&quot;</span> \</span></span><br><span class="line"><span class="bash">--num-mappers 1 \</span></span><br><span class="line"><span class="bash">--split-by id</span></span><br></pre></td></tr></table></figure><p>提示：sqoop1.4.6 只支持 HBase1.0.1 之前的版本的自动创建 HBase 表的功能<br>解决方案：手动创建 HBase 表</p><h3 id="导出数据"><a href="#导出数据" class="headerlink" title="导出数据"></a>导出数据</h3><p>在 Sqoop 中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群<br>（RDBMS）中传输数据，叫做：导出，即使用 export 关键字。</p><h4 id="HIVE-HDFS-到-RDBMS"><a href="#HIVE-HDFS-到-RDBMS" class="headerlink" title="HIVE/HDFS 到 RDBMS"></a>HIVE/HDFS 到 RDBMS</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop <span class="built_in">export</span> \</span></span><br><span class="line"><span class="bash">--connect jdbc:mysql://hadoop102:3306/company \</span></span><br><span class="line"><span class="bash">--username root \</span></span><br><span class="line"><span class="bash">--password 000000 \</span></span><br><span class="line"><span class="bash">--table staff \</span></span><br><span class="line"><span class="bash">--num-mappers 1 \</span></span><br><span class="line"><span class="bash">--export-dir /user/hive/warehouse/staff_hive \</span></span><br><span class="line"><span class="bash">--input-fields-terminated-by <span class="string">&quot;\t&quot;</span></span></span><br></pre></td></tr></table></figure><p>提示：Mysql 中如果表不存在，不会自动创建</p><h3 id="脚本打包"><a href="#脚本打包" class="headerlink" title="脚本打包"></a>脚本打包</h3><p>使用 opt 格式的文件打包 sqoop 命令，然后执行</p><h4 id="1-创建一个-opt-文件"><a href="#1-创建一个-opt-文件" class="headerlink" title="1) 创建一个.opt 文件"></a>1) 创建一个.opt 文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir opt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> touch opt/job_HDFS2RDBMS.opt</span></span><br></pre></td></tr></table></figure><h4 id="2-编写-sqoop-脚本"><a href="#2-编写-sqoop-脚本" class="headerlink" title="2) 编写 sqoop 脚本"></a>2) 编写 sqoop 脚本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> vi opt/job_HDFS2RDBMS.opt</span></span><br><span class="line">export</span><br><span class="line">--connect</span><br><span class="line">jdbc:mysql://hadoop102:3306/company</span><br><span class="line">--username</span><br><span class="line">root</span><br><span class="line">--password</span><br><span class="line">000000</span><br><span class="line">--table</span><br><span class="line">staff</span><br><span class="line">--num-mappers</span><br><span class="line">1</span><br><span class="line">--export-dir</span><br><span class="line">/user/hive/warehouse/staff_hive</span><br><span class="line">--input-fields-terminated-by</span><br><span class="line">&quot;\t&quot;</span><br></pre></td></tr></table></figure><h4 id="3-执行该脚本"><a href="#3-执行该脚本" class="headerlink" title="3) 执行该脚本"></a>3) 执行该脚本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> bin/sqoop --options-file opt/job_HDFS2RDBMS.opt</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive常用查询函数</title>
      <link href="/2021/10/15/hive%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%87%BD%E6%95%B0/"/>
      <url>/2021/10/15/hive%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="常用查询函数"><a href="#常用查询函数" class="headerlink" title="常用查询函数"></a>常用查询函数</h1><h2 id="1-空字段赋值NVL函数"><a href="#1-空字段赋值NVL函数" class="headerlink" title="1 空字段赋值NVL函数"></a>1 空字段赋值NVL函数</h2><p>1.函数说明<br>&emsp;&emsp;NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。</p><p>2.数据准备：采用员工表</p><p>3.查询：如果员工的comm为NULL，则用-1代替</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,-1) from emp;</span><br></pre></td></tr></table></figure><p>4.查询：如果员工的comm为NULL，则用领导id代替</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (default)&gt; select nvl(comm,mgr) from emp;</span><br></pre></td></tr></table></figure><h2 id="2-CASE-WHEN函数"><a href="#2-CASE-WHEN函数" class="headerlink" title="2.CASE WHEN函数"></a>2.CASE WHEN函数</h2><ol><li>数据准备</li></ol><table><thead><tr><th align="center">name</th><th align="center">dept_id</th><th align="center">sex</th></tr></thead><tbody><tr><td align="center">悟空</td><td align="center">A</td><td align="center">男</td></tr><tr><td align="center">大海</td><td align="center">A</td><td align="center">男</td></tr><tr><td align="center">宋宋</td><td align="center">B</td><td align="center">男</td></tr><tr><td align="center">凤姐</td><td align="center">A</td><td align="center">女</td></tr><tr><td align="center">婷姐</td><td align="center">B</td><td align="center">女</td></tr><tr><td align="center">婷婷</td><td align="center">B</td><td align="center">女</td></tr></tbody></table><p>2．需求<br>求出不同部门男女各多少人。结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A     2       1</span><br><span class="line">B     1       2</span><br></pre></td></tr></table></figure><p>3．创建本地emp_sex.txt，导入数据<br>[ihadu@hadoop102 datas]$ vi emp_sex.txt<br>悟空    A    男<br>大海    A    男<br>宋宋    B    男<br>凤姐    A    女<br>婷姐    B    女<br>婷婷    B    女</p><p>4．创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> emp_sex(</span><br><span class="line">name string,</span><br><span class="line">dept_id string,</span><br><span class="line">sex string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/emp_sex.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> emp_sex;</span><br></pre></td></tr></table></figure><p>5．按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  dept_id,</span><br><span class="line">  <span class="built_in">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;男&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) male_count,</span><br><span class="line">  <span class="built_in">sum</span>(<span class="keyword">case</span> sex <span class="keyword">when</span> <span class="string">&#x27;女&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) female_count</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  emp_sex</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  dept_id;</span><br></pre></td></tr></table></figure><h2 id="3-行转列"><a href="#3-行转列" class="headerlink" title="3.行转列"></a>3.行转列</h2><p>1．相关函数说明<br>&emsp;&emsp;CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;<br>&emsp;&emsp;CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;<br>&emsp;&emsp;COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p><p>2．数据准备</p><table><thead><tr><th align="center">name</th><th align="center">constellation</th><th align="center">blood_type</th></tr></thead><tbody><tr><td align="center">孙悟空</td><td align="center">白羊座</td><td align="center">A</td></tr><tr><td align="center">大海</td><td align="center">射手座</td><td align="center">A</td></tr><tr><td align="center">宋宋</td><td align="center">白羊座</td><td align="center">B</td></tr><tr><td align="center">猪八戒</td><td align="center">白羊座</td><td align="center">A</td></tr><tr><td align="center">凤姐</td><td align="center">射手座</td><td align="center">A</td></tr></tbody></table><p>3．需求<br>把星座和血型一样的人归类到一起。结果如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">射手座,A            大海|凤姐</span><br><span class="line">白羊座,A            孙悟空|猪八戒</span><br><span class="line">白羊座,B            宋宋</span><br></pre></td></tr></table></figure><p>4．创建本地constellation.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[ihadu@hadoop102 datas]$ vi person_info.txt</span><br><span class="line">孙悟空白羊座A</span><br><span class="line">大海 射手座A</span><br><span class="line">宋宋 白羊座B</span><br><span class="line">猪八戒   白羊座A</span><br><span class="line">凤姐  射手座A</span><br></pre></td></tr></table></figure><p>5．创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line">name string,</span><br><span class="line">constellation string,</span><br><span class="line">blood_type string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath “<span class="operator">/</span>opt<span class="operator">/</span><span class="keyword">module</span><span class="operator">/</span>datas<span class="operator">/</span>person_info.txt” <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure><p>6．按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.base,</span><br><span class="line">    concat_ws(<span class="string">&#x27;|&#x27;</span>, collect_set(t1.name)) name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (<span class="keyword">select</span></span><br><span class="line">        name,</span><br><span class="line">        concat(constellation, &quot;,&quot;, blood_type) base</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">        person_info) t1</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">    t1.base;</span><br></pre></td></tr></table></figure><h2 id="4-列转行"><a href="#4-列转行" class="headerlink" title="4.列转行"></a>4.列转行</h2><p>1．函数说明<br>Explode(col)：将hive一列中复杂的array或者map结构拆分成多行。</p><p>Lateral view<br>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias<br>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p>2．数据准备</p><table><thead><tr><th align="center">movie</th><th align="center">category</th></tr></thead><tbody><tr><td align="center">《金刚川》</td><td align="center">悬疑,动作,科幻,剧情</td></tr><tr><td align="center">《我和我的家乡》</td><td align="center">悬疑,警匪,动作,心理,剧情</td></tr><tr><td align="center">《心灵奇旅》</td><td align="center">战争,动作,灾难</td></tr></tbody></table><p>3．需求<br>将电影分类中的数组数据展开。结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《金刚川》      悬疑</span><br><span class="line">《金刚川》      动作</span><br><span class="line">《金刚川》      科幻</span><br><span class="line">《金刚川》      剧情</span><br><span class="line">《我和我的家乡》   悬疑</span><br><span class="line">《我和我的家乡》   警匪</span><br><span class="line">《我和我的家乡》   动作</span><br><span class="line">《我和我的家乡》   心理</span><br><span class="line">《我和我的家乡》   剧情</span><br><span class="line">《心灵奇旅》      战争</span><br><span class="line">《心灵奇旅》      动作</span><br><span class="line">《心灵奇旅》      灾难</span><br></pre></td></tr></table></figure><p>4．创建本地movie.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ihadu@hadoop102 datas]$ vi movie.txt</span><br><span class="line">《疑犯追踪》悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》战争,动作,灾难</span><br></pre></td></tr></table></figure><p>5．创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">    movie string,</span><br><span class="line">    category <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;</span><br><span class="line">collection items terminated <span class="keyword">by</span> &quot;,&quot;;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/opt/module/datas/movie.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure><p>6．按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> explode(category) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure><h2 id="5-窗口函数"><a href="#5-窗口函数" class="headerlink" title="5.窗口函数"></a>5.窗口函数</h2><p>1．相关函数说明<br>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化<br>CURRENT ROW：当前行<br>n PRECEDING：往前n行数据<br>n FOLLOWING：往后n行数据<br>UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点<br>LAG(col,n)：往前第n行数据<br>LEAD(col,n)：往后第n行数据<br>NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。</p><p>2．数据准备：name，orderdate，cost</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">jack,2017-01-01,10</span><br><span class="line">tony,2017-01-02,15</span><br><span class="line">jack,2017-02-03,23</span><br><span class="line">tony,2017-01-04,29</span><br><span class="line">jack,2017-01-05,46</span><br><span class="line">jack,2017-04-06,42</span><br><span class="line">tony,2017-01-07,50</span><br><span class="line">jack,2017-01-08,55</span><br><span class="line">mart,2017-04-08,62</span><br><span class="line">mart,2017-04-09,68</span><br><span class="line">neil,2017-05-10,12</span><br><span class="line">mart,2017-04-11,75</span><br><span class="line">neil,2017-06-12,80</span><br><span class="line">mart,2017-04-13,94</span><br></pre></td></tr></table></figure><p>3．需求<br>（1）查询在2017年4月份购买过的顾客及总人数<br>（2）查询顾客的购买明细及月购买总额<br>（3）上述的场景,要将cost按照日期进行累加<br>（4）查询顾客上次的购买时间<br>（5）查询前20%时间的订单信息</p><p>4．创建本地business.txt，导入数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ihadu@hadoop102 datas]$ vi business.txt</span><br></pre></td></tr></table></figure><p>5．创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> business(</span><br><span class="line">name string,</span><br><span class="line">orderdate string,</span><br><span class="line">cost <span class="type">int</span></span><br><span class="line">) <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/opt/module/datas/business.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> business;</span><br></pre></td></tr></table></figure><p>6．按需求查询数据<br>（1）查询在2017年4月份购买过的顾客及总人数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> name,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">over</span> ()</span><br><span class="line"><span class="keyword">from</span> business</span><br><span class="line"><span class="keyword">where</span> <span class="built_in">substring</span>(orderdate,<span class="number">1</span>,<span class="number">7</span>) <span class="operator">=</span> <span class="string">&#x27;2017-04&#x27;</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure><p>（2）查询顾客的购买明细及月购买总额</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">name,</span><br><span class="line">orderdate,</span><br><span class="line">cost,</span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> <span class="keyword">month</span>(orderdate))</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>（3）上述的场景,要将cost按照日期进行累加</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> name,orderdate,cost,</span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>() <span class="keyword">as</span> sample1,<span class="comment">--所有行相加</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name) <span class="keyword">as</span> sample2,<span class="comment">--按name分组，组内数据相加</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> sample3,<span class="comment">--按name分组，组内数据累加</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> UNBOUNDED PRECEDING <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span> ) <span class="keyword">as</span> sample4 ,<span class="comment">--和sample3一样,由起点到当前行的聚合</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> PRECEDING <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span>) <span class="keyword">as</span> sample5, <span class="comment">--当前行和前面一行做聚合</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> PRECEDING <span class="keyword">AND</span> <span class="number">1</span> FOLLOWING ) <span class="keyword">as</span> sample6,<span class="comment">--当前行和前边一行及后面一行</span></span><br><span class="line"><span class="built_in">sum</span>(cost) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="type">row</span> <span class="keyword">and</span> UNBOUNDED FOLLOWING ) <span class="keyword">as</span> sample7 <span class="comment">--当前行及后面所有行</span></span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>（4）查看顾客上次的购买时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> name,orderdate,cost,</span><br><span class="line"><span class="built_in">lag</span>(orderdate,<span class="number">1</span>,<span class="string">&#x27;1900-01-01&#x27;</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate ) <span class="keyword">as</span> time1,</span><br><span class="line"><span class="built_in">lag</span>(orderdate,<span class="number">2</span>) <span class="keyword">over</span> (<span class="keyword">partition</span> <span class="keyword">by</span> name <span class="keyword">order</span> <span class="keyword">by</span> orderdate) <span class="keyword">as</span> time2</span><br><span class="line"><span class="keyword">from</span> business;</span><br></pre></td></tr></table></figure><p>（5）查询前20%时间的订单信息</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> name,orderdate,cost, <span class="built_in">ntile</span>(<span class="number">5</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> orderdate) sorted</span><br><span class="line">    <span class="keyword">from</span> business</span><br><span class="line">) t</span><br><span class="line"><span class="keyword">where</span> sorted <span class="operator">=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>6.Rank函数<br>1．函数说明</p><ul><li>RANK() 排序相同时会重复，总数不会变</li><li>DENSE_RANK() 排序相同时会重复，总数会减少</li><li>ROW_NUMBER() 会根据顺序计算</li></ul><p>2．数据准备</p><table><thead><tr><th align="center">name</th><th align="center">subject</th><th align="center">score</th></tr></thead><tbody><tr><td align="center">孙悟空</td><td align="center">语文</td><td align="center">87</td></tr><tr><td align="center">孙悟空</td><td align="center">数学</td><td align="center">95</td></tr><tr><td align="center">孙悟空</td><td align="center">英语</td><td align="center">68</td></tr><tr><td align="center">大海</td><td align="center">语文</td><td align="center">94</td></tr><tr><td align="center">大海</td><td align="center">数学</td><td align="center">56</td></tr><tr><td align="center">大海</td><td align="center">英语</td><td align="center">84</td></tr><tr><td align="center">宋宋</td><td align="center">语文</td><td align="center">64</td></tr><tr><td align="center">宋宋</td><td align="center">数学</td><td align="center">86</td></tr><tr><td align="center">宋宋</td><td align="center">英语</td><td align="center">84</td></tr><tr><td align="center">婷婷</td><td align="center">语文</td><td align="center">65</td></tr><tr><td align="center">婷婷</td><td align="center">数学</td><td align="center">85</td></tr><tr><td align="center">婷婷</td><td align="center">英语</td><td align="center">78</td></tr></tbody></table><p>3．需求<br>计算每门学科成绩排名。</p><p>4．创建本地movie.txt，导入数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ihadu@hadoop102 datas]$ vi score.txt</span><br></pre></td></tr></table></figure><p>5．创建hive表并导入数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(</span><br><span class="line">name string,</span><br><span class="line">subject string, </span><br><span class="line">score <span class="type">int</span>) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/opt/module/datas/score.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> score;</span><br></pre></td></tr></table></figure><p>6．按需求查询数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> name,</span><br><span class="line">subject,</span><br><span class="line">score,</span><br><span class="line"><span class="built_in">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rp,</span><br><span class="line"><span class="built_in">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) drp,</span><br><span class="line"><span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rmp</span><br><span class="line"><span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><p>查询结果：</p><pre><code class="shell">name    subject score   rp      drp     rmp孙悟空  数学    95      1       1       1宋宋    数学    86      2       2       2婷婷    数学    85      3       3       3大海    数学    56      4       4       4宋宋    英语    84      1       1       1大海    英语    84      1       1       2婷婷    英语    78      3       2       3孙悟空  英语    68      4       3       4大海    语文    94      1       1       1孙悟空  语文    87      2       2       2婷婷    语文    65      3       3       3宋宋    语文    64      4       4       4</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive数据查询实战</title>
      <link href="/2021/10/15/Hive%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%AE%9E%E6%88%98/"/>
      <url>/2021/10/15/Hive%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h2 id="一、数据准备"><a href="#一、数据准备" class="headerlink" title="一、数据准备"></a>一、数据准备</h2><p>为了演示查询操作，这里需要预先创建三张表，并加载测试数据。</p><blockquote><p>数据文件 emp.txt 和 dept.txt 可以从本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录下载。</p></blockquote><h3 id="1-1-员工表"><a href="#1-1-员工表" class="headerlink" title="1.1 员工表"></a>1.1 员工表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">-- 建表语句</span></span><br><span class="line"> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">     empno <span class="type">INT</span>,     <span class="comment">-- 员工表编号</span></span><br><span class="line">     ename STRING,  <span class="comment">-- 员工姓名</span></span><br><span class="line">     job STRING,    <span class="comment">-- 职位类型</span></span><br><span class="line">     mgr <span class="type">INT</span>,</span><br><span class="line">     hiredate <span class="type">TIMESTAMP</span>,  <span class="comment">--雇佣日期</span></span><br><span class="line">     sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),  <span class="comment">--工资</span></span><br><span class="line">     comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">     deptno <span class="type">INT</span>)   <span class="comment">--部门编号</span></span><br><span class="line">    <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp;</span><br></pre></td></tr></table></figure><h3 id="1-2-部门表"><a href="#1-2-部门表" class="headerlink" title="1.2 部门表"></a>1.2 部门表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 建表语句</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> dept(</span><br><span class="line">    deptno <span class="type">INT</span>,   <span class="comment">--部门编号</span></span><br><span class="line">    dname STRING,  <span class="comment">--部门名称</span></span><br><span class="line">    loc STRING    <span class="comment">--部门所在的城市</span></span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/dept.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> dept;</span><br></pre></td></tr></table></figure><h3 id="1-3-分区表"><a href="#1-3-分区表" class="headerlink" title="1.3 分区表"></a>1.3 分区表</h3><p>这里需要额外创建一张分区表，主要是为了演示分区查询：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_ptn(</span><br><span class="line">      empno <span class="type">INT</span>,</span><br><span class="line">      ename STRING,</span><br><span class="line">      job STRING,</span><br><span class="line">      mgr <span class="type">INT</span>,</span><br><span class="line">      hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">      sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">      comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">--加载数据</span></span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">30</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">40</span>)</span><br><span class="line">LOAD DATA <span class="keyword">LOCAL</span> INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">50</span>)</span><br></pre></td></tr></table></figure><h2 id="二、单表查询"><a href="#二、单表查询" class="headerlink" title="二、单表查询"></a>二、单表查询</h2><h3 id="2-1-SELECT"><a href="#2-1-SELECT" class="headerlink" title="2.1 SELECT"></a>2.1 SELECT</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询表中全部数据</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure><h3 id="2-2-WHERE"><a href="#2-2-WHERE" class="headerlink" title="2.2 WHERE"></a>2.2 WHERE</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询 10 号部门中员工编号大于 7782 的员工信息 </span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> empno <span class="operator">&gt;</span> <span class="number">7782</span> <span class="keyword">AND</span> deptno <span class="operator">=</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure><h3 id="2-3-DISTINCT"><a href="#2-3-DISTINCT" class="headerlink" title="2.3  DISTINCT"></a>2.3  DISTINCT</h3><p>Hive 支持使用 DISTINCT 关键字去重。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询所有工作类型</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DISTINCT</span> job <span class="keyword">FROM</span> emp;</span><br></pre></td></tr></table></figure><h3 id="2-4-分区查询"><a href="#2-4-分区查询" class="headerlink" title="2.4 分区查询"></a>2.4 分区查询</h3><p>分区查询 (Partition Based Queries)，可以指定某个分区或者分区范围。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询分区表中部门编号在[20,40]之间的员工</span></span><br><span class="line"><span class="keyword">SELECT</span> emp_ptn.<span class="operator">*</span> <span class="keyword">FROM</span> emp_ptn</span><br><span class="line"><span class="keyword">WHERE</span> emp_ptn.deptno <span class="operator">&gt;=</span> <span class="number">20</span> <span class="keyword">AND</span> emp_ptn.deptno <span class="operator">&lt;=</span> <span class="number">40</span>;</span><br></pre></td></tr></table></figure><h3 id="2-5-LIMIT"><a href="#2-5-LIMIT" class="headerlink" title="2.5 LIMIT"></a>2.5 LIMIT</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询薪资最高的 5 名员工</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">ORDER</span> <span class="keyword">BY</span> sal <span class="keyword">DESC</span> LIMIT <span class="number">5</span>;</span><br></pre></td></tr></table></figure><h3 id="2-6-GROUP-BY"><a href="#2-6-GROUP-BY" class="headerlink" title="2.6 GROUP BY"></a>2.6 GROUP BY</h3><p>Hive 支持使用 GROUP BY 进行分组聚合操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 查询各个部门薪酬综合</span></span><br><span class="line"><span class="keyword">SELECT</span> deptno,<span class="built_in">SUM</span>(sal) <span class="keyword">FROM</span> emp <span class="keyword">GROUP</span> <span class="keyword">BY</span> deptno;</span><br></pre></td></tr></table></figure><p><code>hive.map.aggr</code> 控制程序如何进行聚合。默认值为 false。如果设置为 true，Hive 会在 map 阶段就执行一次聚合。这可以提高聚合效率，但需要消耗更多内存。</p><h3 id="2-7-ORDER-AND-SORT"><a href="#2-7-ORDER-AND-SORT" class="headerlink" title="2.7 ORDER AND SORT"></a>2.7 ORDER AND SORT</h3><p>可以使用 ORDER BY 或者 Sort BY 对查询结果进行排序，排序字段可以是整型也可以是字符串：如果是整型，则按照大小排序；如果是字符串，则按照字典序排序。ORDER BY 和 SORT BY 的区别如下：</p><ul><li>使用 ORDER BY 时会有一个 Reducer 对全部查询结果进行排序，可以保证数据的全局有序性；</li><li>使用 SORT BY 时只会在每个 Reducer 中进行排序，这可以保证每个 Reducer 的输出数据是有序的，但不能保证全局有序。</li></ul><p>由于 ORDER BY 的时间可能很长，如果你设置了严格模式 (hive.mapred.mode = strict)，则其后面必须再跟一个 <code>limit</code> 子句。</p><blockquote><p>注 ：hive.mapred.mode 默认值是 nonstrict ，也就是非严格模式。</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询员工工资，结果按照部门升序，按照工资降序排列</span></span><br><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp <span class="keyword">ORDER</span> <span class="keyword">BY</span> deptno <span class="keyword">ASC</span>, sal <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><h3 id="2-8-HAVING"><a href="#2-8-HAVING" class="headerlink" title="2.8 HAVING"></a>2.8 HAVING</h3><p>可以使用 HAVING 对分组数据进行过滤。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询工资总和大于 9000 的所有部门</span></span><br><span class="line"><span class="keyword">SELECT</span> deptno,<span class="built_in">SUM</span>(sal) <span class="keyword">FROM</span> emp <span class="keyword">GROUP</span> <span class="keyword">BY</span> deptno <span class="keyword">HAVING</span> <span class="built_in">SUM</span>(sal)<span class="operator">&gt;</span><span class="number">9000</span>;</span><br></pre></td></tr></table></figure><h3 id="2-9-DISTRIBUTE-BY"><a href="#2-9-DISTRIBUTE-BY" class="headerlink" title="2.9 DISTRIBUTE BY"></a>2.9 DISTRIBUTE BY</h3><p>如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这可以使用 DISTRIBUTE BY 字句。需要注意的是，DISTRIBUTE BY 虽然能把具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。情况如下：</p><p>把以下 5 个数据发送到两个 Reducer 上进行处理：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">k1</span></span><br><span class="line"><span class="attr">k2</span></span><br><span class="line"><span class="attr">k4</span></span><br><span class="line"><span class="attr">k3</span></span><br><span class="line"><span class="attr">k1</span></span><br></pre></td></tr></table></figure><p>Reducer1 得到如下乱序数据：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">k1</span></span><br><span class="line"><span class="attr">k2</span></span><br><span class="line"><span class="attr">k1</span></span><br></pre></td></tr></table></figure><p>Reducer2 得到数据如下：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">k4</span></span><br><span class="line"><span class="attr">k3</span></span><br></pre></td></tr></table></figure><p>如果想让 Reducer 上的数据是有序的，可以结合 <code>SORT BY</code> 使用 (示例如下)，或者使用下面我们将要介绍的 CLUSTER BY。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 将数据按照部门分发到对应的 Reducer 上处理</span></span><br><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp DISTRIBUTE <span class="keyword">BY</span> deptno SORT <span class="keyword">BY</span> deptno <span class="keyword">ASC</span>;</span><br></pre></td></tr></table></figure><h3 id="2-10-CLUSTER-BY"><a href="#2-10-CLUSTER-BY" class="headerlink" title="2.10 CLUSTER BY"></a>2.10 CLUSTER BY</h3><p>如果 <code>SORT BY</code> 和 <code>DISTRIBUTE BY</code> 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 <code>CLUSTER BY</code> 进行替换，同时 <code>CLUSTER BY</code> 可以保证数据在全局是有序的。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> empno, deptno, sal <span class="keyword">FROM</span> emp CLUSTER  <span class="keyword">BY</span> deptno ;</span><br></pre></td></tr></table></figure><h2 id="三、多表联结查询"><a href="#三、多表联结查询" class="headerlink" title="三、多表联结查询"></a>三、多表联结查询</h2><p>Hive 支持内连接，外连接，左外连接，右外连接，笛卡尔连接，这和传统数据库中的概念是一致的，可以参见下图。</p><p>需要特别强调：JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定，否则就会先做笛卡尔积，再过滤，这会导致你得不到预期的结果 (下面的演示会有说明)。</p><p><img src="https://pic.downk.cc/item/5ff40fc53ffa7d37b3b21b93.jpg" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40fc53ffa7d37b3b21b93.jpg" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="3-1-INNER-JOIN"><a href="#3-1-INNER-JOIN" class="headerlink" title="3.1 INNER JOIN"></a>3.1 INNER JOIN</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询员工编号为 7369 的员工的详细信息</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span> <span class="keyword">FROM</span> </span><br><span class="line">emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno </span><br><span class="line"><span class="keyword">WHERE</span> empno<span class="operator">=</span><span class="number">7369</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--如果是三表或者更多表连接，语法如下</span></span><br><span class="line"><span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key <span class="operator">=</span> b.key1) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key <span class="operator">=</span> b.key1)</span><br></pre></td></tr></table></figure><h3 id="3-2-LEFT-OUTER-JOIN"><a href="#3-2-LEFT-OUTER-JOIN" class="headerlink" title="3.2 LEFT OUTER  JOIN"></a>3.2 LEFT OUTER  JOIN</h3><p>LEFT OUTER  JOIN 和 LEFT  JOIN 是等价的。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 左连接</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">LEFT</span> <span class="keyword">OUTER</span>  <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure><h3 id="3-3-RIGHT-OUTER-JOIN"><a href="#3-3-RIGHT-OUTER-JOIN" class="headerlink" title="3.3 RIGHT OUTER  JOIN"></a>3.3 RIGHT OUTER  JOIN</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--右连接</span></span><br><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure><p>执行右连接后，由于 40 号部门下没有任何员工，所以此时员工信息为 NULL。这个查询可以很好的复述上面提到的——JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定。你可以把 ON 改成 WHERE，你会发现无论如何都查不出 40 号部门这条数据，因为笛卡尔运算不会有 (NULL, 40) 这种情况。</p><p><img src="https://pic.downk.cc/item/5ff40fc53ffa7d37b3b21b91.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40fc53ffa7d37b3b21b91.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="3-4-FULL-OUTER-JOIN"><a href="#3-4-FULL-OUTER-JOIN" class="headerlink" title="3.4 FULL OUTER  JOIN"></a>3.4 FULL OUTER  JOIN</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.<span class="operator">*</span>,d.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span>  dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure><h3 id="3-5-LEFT-SEMI-JOIN"><a href="#3-5-LEFT-SEMI-JOIN" class="headerlink" title="3.5 LEFT SEMI JOIN"></a>3.5 LEFT SEMI JOIN</h3><p>LEFT SEMI JOIN （左半连接）是 IN/EXISTS 子查询的一种更高效的实现。</p><ul><li>JOIN 子句中右边的表只能在 ON 子句中设置过滤条件;</li><li>查询结果只包含左边表的数据，所以只能 SELECT 左表中的列。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询在纽约办公的所有员工信息</span></span><br><span class="line"><span class="keyword">SELECT</span> emp.<span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> emp <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> dept </span><br><span class="line"><span class="keyword">ON</span> emp.deptno <span class="operator">=</span> dept.deptno <span class="keyword">AND</span> dept.loc<span class="operator">=</span>&quot;NEW YORK&quot;;</span><br><span class="line"></span><br><span class="line"><span class="comment">--上面的语句就等价于</span></span><br><span class="line"><span class="keyword">SELECT</span> emp.<span class="operator">*</span> <span class="keyword">FROM</span> emp</span><br><span class="line"><span class="keyword">WHERE</span> emp.deptno <span class="keyword">IN</span> (<span class="keyword">SELECT</span> deptno <span class="keyword">FROM</span> dept <span class="keyword">WHERE</span> loc<span class="operator">=</span>&quot;NEW YORK&quot;);</span><br></pre></td></tr></table></figure><h3 id="3-6-JOIN"><a href="#3-6-JOIN" class="headerlink" title="3.6 JOIN"></a>3.6 JOIN</h3><p>笛卡尔积连接，这个连接日常的开发中可能很少遇到，且性能消耗比较大，基于这个原因，如果在严格模式下 (hive.mapred.mode = strict)，Hive 会阻止用户执行此操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp <span class="keyword">JOIN</span> dept;</span><br></pre></td></tr></table></figure><h2 id="四、JOIN优化"><a href="#四、JOIN优化" class="headerlink" title="四、JOIN优化"></a>四、JOIN优化</h2><h3 id="4-1-STREAMTABLE"><a href="#4-1-STREAMTABLE" class="headerlink" title="4.1 STREAMTABLE"></a>4.1 STREAMTABLE</h3><p>在多表进行联结的时候，如果每个 ON 字句都使用到共同的列（如下面的 <code>b.key</code>），此时 Hive 会进行优化，将多表 JOIN 在同一个 map / reduce 作业上进行。同时假定查询的最后一个表（如下面的 c 表）是最大的一个表，在对每行记录进行 JOIN 操作时，它将尝试将其他的表缓存起来，然后扫描最后那个表进行计算。因此用户需要保证查询的表的大小从左到右是依次增加的。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`<span class="keyword">SELECT</span> a.val, b.val, c.val <span class="keyword">FROM</span> a <span class="keyword">JOIN</span> b <span class="keyword">ON</span> (a.key <span class="operator">=</span> b.key) <span class="keyword">JOIN</span> c <span class="keyword">ON</span> (c.key <span class="operator">=</span> b.key)`</span><br></pre></td></tr></table></figure><p>然后，用户并非需要总是把最大的表放在查询语句的最后面，Hive 提供了 <code>/*+ STREAMTABLE() */</code> 标志，用于标识最大的表，示例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ STREAMTABLE(d) */</span>  e.<span class="operator">*</span>,d.<span class="operator">*</span> </span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> job<span class="operator">=</span><span class="string">&#x27;CLERK&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="4-2-MAPJOIN"><a href="#4-2-MAPJOIN" class="headerlink" title="4.2 MAPJOIN"></a>4.2 MAPJOIN</h3><p>如果所有表中只有一张表是小表，那么 Hive 把这张小表加载到内存中。这时候程序会在 map 阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在 map 就进行了 JOIN 操作，从而可以省略 reduce 过程，这样效率可以提升很多。Hive 中提供了 <code>/*+ MAPJOIN() */</code> 来标记小表，示例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="comment">/*+ MAPJOIN(d) */</span> e.<span class="operator">*</span>,d.<span class="operator">*</span> </span><br><span class="line"><span class="keyword">FROM</span> emp e <span class="keyword">JOIN</span> dept d</span><br><span class="line"><span class="keyword">ON</span> e.deptno <span class="operator">=</span> d.deptno</span><br><span class="line"><span class="keyword">WHERE</span> job<span class="operator">=</span><span class="string">&#x27;CLERK&#x27;</span>;</span><br></pre></td></tr></table></figure><h2 id="五、SELECT的其他用途"><a href="#五、SELECT的其他用途" class="headerlink" title="五、SELECT的其他用途"></a>五、SELECT的其他用途</h2><p>查看当前数据库：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> current_database()</span><br></pre></td></tr></table></figure><h2 id="六、本地模式"><a href="#六、本地模式" class="headerlink" title="六、本地模式"></a>六、本地模式</h2><p>在上面演示的语句中，大多数都会触发 MapReduce, 少部分不会触发，比如 <code>select * from emp limit 5</code> 就不会触发 MR，此时 Hive 只是简单的读取数据文件中的内容，然后格式化后进行输出。在需要执行 MapReduce 的查询中，你会发现执行时间可能会很长，这时候你可以选择开启本地模式。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--本地模式默认关闭，需要手动开启此功能</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>启用后，Hive 将分析查询中每个 map-reduce 作业的大小，如果满足以下条件，则可以在本地运行它：</p><ul><li>作业的总输入大小低于：hive.exec.mode.local.auto.inputbytes.max（默认为 128MB）；</li><li>map-tasks 的总数小于：hive.exec.mode.local.auto.tasks.max（默认为 4）；</li><li>所需的 reduce 任务总数为 1 或 0。</li></ul><p>因为我们测试的数据集很小，所以你再次去执行上面涉及 MR 操作的查询，你会发现速度会有显著的提升。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select">LanguageManual Select</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins">LanguageManual Joins</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+GroupBy">LanguageManual GroupBy</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy">LanguageManual SortBy</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive 常用DML操作</title>
      <link href="/2021/10/15/Hive%E5%B8%B8%E7%94%A8DML%E6%93%8D%E4%BD%9C/"/>
      <url>/2021/10/15/Hive%E5%B8%B8%E7%94%A8DML%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="一、加载文件数据到表"><a href="#一、加载文件数据到表" class="headerlink" title="一、加载文件数据到表"></a>一、加载文件数据到表</h2><h3 id="1-1-语法"><a href="#1-1-语法" class="headerlink" title="1.1 语法"></a>1.1 语法</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA [LOCAL] INPATH &#x27;filepath&#x27; [OVERWRITE] </span><br><span class="line">INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</span><br></pre></td></tr></table></figure><ul><li><code>LOCAL</code> 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件：</li></ul><ul><li><p>从本地文件系统加载文件时， <code>filepath</code> 可以是绝对路径也可以是相对路径 (建议使用绝对路径)；</p></li><li><p>从 HDFS 加载文件时候，<code>filepath</code> 为文件完整的 URL 地址：如 <code>hdfs://namenode:port/user/hive/project/ data1</code></p></li></ul><ul><li><p><code>filepath</code> 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)；</p></li><li><p>如果使用 OVERWRITE 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入；</p></li><li><p>加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区；</p></li><li><p>加载文件的格式必须与建表时使用 <code> STORED AS</code> 指定的存储格式相同。</p></li></ul><blockquote><p>使用建议：</p><p><strong>不论是本地路径还是 URL 都建议使用完整的</strong>。虽然可以使用不完整的 URL 地址，此时 Hive 将使用 hadoop 中的 fs.default.name 配置来推断地址，但是为避免不必要的错误，建议使用完整的本地路径或 URL 地址；</p><p><strong>加载对象是分区表时建议显示指定分区</strong>。在 Hive 3.0 之后，内部将加载 (LOAD) 重写为 INSERT AS SELECT，此时如果不指定分区，INSERT AS SELECT 将假设最后一组列是分区列，如果该列不是表定义的分区，它将抛出错误。为避免错误，还是建议显示指定分区。</p></blockquote><h3 id="1-2-示例"><a href="#1-2-示例" class="headerlink" title="1.2 示例"></a>1.2 示例</h3><p>新建分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp_ptn(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED <span class="keyword">BY</span> (deptno <span class="type">INT</span>)   <span class="comment">-- 按照部门编号进行分区</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure><p>从 HDFS 上加载数据到分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LOAD DATA  INPATH &quot;hdfs://hadoop001:8020/mydir/emp.txt&quot; OVERWRITE <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>);</span><br></pre></td></tr></table></figure><blockquote><p>emp.txt 文件可在本仓库的 resources 目录中下载</p></blockquote><p>加载后表中数据如下,分区列 deptno 全部赋值成 20：</p><p><img src="https://pic.downk.cc/item/5ff40d263ffa7d37b3b02c55.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40d263ffa7d37b3b02c55.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="二、查询结果插入到表"><a href="#二、查询结果插入到表" class="headerlink" title="二、查询结果插入到表"></a>二、查询结果插入到表</h2><h3 id="2-1-语法"><a href="#2-1-语法" class="headerlink" title="2.1 语法"></a>2.1 语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]]   </span><br><span class="line">select_statement1 <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename1 [<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...)] </span><br><span class="line">select_statement1 <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure><ul><li><p>Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”=“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 <code>immutable</code> 属性的影响）;</p></li><li><p>可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区；</p></li><li><p>从 Hive 1.1.0 开始，TABLE 关键字是可选的；</p></li><li><p>从 Hive 1.2.0 开始 ，可以采用 INSERT INTO tablename(z，x，c1) 指明插入列；</p></li><li><p>可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> from_statement</span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename1 </span><br><span class="line">[<span class="keyword">PARTITION</span> (partcol1<span class="operator">=</span>val1, partcol2<span class="operator">=</span>val2 ...) [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement1</span><br><span class="line">[<span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ... [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>]] select_statement2]</span><br><span class="line">[<span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename2 [<span class="keyword">PARTITION</span> ...] select_statement2] ...;</span><br></pre></td></tr></table></figure></li></ul><h3 id="2-2-动态插入分区"><a href="#2-2-动态插入分区" class="headerlink" title="2.2 动态插入分区"></a>2.2 动态插入分区</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...) </span><br><span class="line">select_statement <span class="keyword">FROM</span> from_statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename <span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...) </span><br><span class="line">select_statement <span class="keyword">FROM</span> from_statement;</span><br></pre></td></tr></table></figure><p>在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 SELECT 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。</p><p>注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。以下是动态分区的相关配置：</p><table><thead><tr><th>配置</th><th>默认值</th><th>说明</th></tr></thead><tbody><tr><td><code>hive.exec.dynamic.partition</code></td><td><code>true</code></td><td>需要设置为 true 才能启用动态分区插入</td></tr><tr><td><code>hive.exec.dynamic.partition.mode</code></td><td><code>strict</code></td><td>在严格模式 (strict) 下，用户必须至少指定一个静态分区，以防用户意外覆盖所有分区，在非严格模式下，允许所有分区都是动态的</td></tr><tr><td><code>hive.exec.max.dynamic.partitions.pernode</code></td><td>100</td><td>允许在每个 mapper/reducer 节点中创建的最大动态分区数</td></tr><tr><td><code>hive.exec.max.dynamic.partitions</code></td><td>1000</td><td>允许总共创建的最大动态分区数</td></tr><tr><td><code>hive.exec.max.created.files</code></td><td>100000</td><td>作业中所有 mapper/reducer 创建的 HDFS 文件的最大数量</td></tr><tr><td><code>hive.error.on.empty.partition</code></td><td><code>false</code></td><td>如果动态分区插入生成空结果，是否抛出异常</td></tr></tbody></table><h3 id="2-3-示例"><a href="#2-3-示例" class="headerlink" title="2.3 示例"></a>2.3 示例</h3><ol><li>新建 emp 表，作为查询对象表</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp(</span><br><span class="line">    empno <span class="type">INT</span>,</span><br><span class="line">    ename STRING,</span><br><span class="line">    job STRING,</span><br><span class="line">    mgr <span class="type">INT</span>,</span><br><span class="line">    hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">    sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">    comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">    deptno <span class="type">INT</span>)</span><br><span class="line">    <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br><span class="line">    </span><br><span class="line"> <span class="comment">-- 加载数据到 emp 表中 这里直接从本地加载</span></span><br><span class="line">load data <span class="keyword">local</span> inpath &quot;/usr/file/emp.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> emp;</span><br></pre></td></tr></table></figure><p>​    完成后 <code>emp</code> 表中数据如下：<br><img src="https://pic.downk.cc/item/5ff40d263ffa7d37b3b02c49.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40d263ffa7d37b3b02c49.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><ol start="2"><li>为清晰演示，先清空 <code>emp_ptn</code> 表中加载的数据：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">TRUNCATE</span> <span class="keyword">TABLE</span> emp_ptn;</span><br></pre></td></tr></table></figure><ol start="3"><li>静态分区演示：从 <code>emp</code> 表中查询部门编号为 20 的员工数据，并插入 <code>emp_ptn</code> 表中，语句如下：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno<span class="operator">=</span><span class="number">20</span>) </span><br><span class="line"><span class="keyword">SELECT</span> empno,ename,job,mgr,hiredate,sal,comm <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno<span class="operator">=</span><span class="number">20</span>;</span><br></pre></td></tr></table></figure><p>​    完成后 <code>emp_ptn</code> 表中数据如下：</p><p><img src="https://pic.downk.cc/item/5ff40d263ffa7d37b3b02c4b.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40d263ffa7d37b3b02c4b.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><ol start="4"><li>接着演示动态分区：</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 由于我们只有一个分区，且还是动态分区，所以需要关闭严格默认。因为在严格模式下，用户必须至少指定一个静态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 动态分区   此时查询语句的最后一列为动态分区列，即 deptno</span></span><br><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> emp_ptn <span class="keyword">PARTITION</span> (deptno) </span><br><span class="line"><span class="keyword">SELECT</span> empno,ename,job,mgr,hiredate,sal,comm,deptno <span class="keyword">FROM</span> emp <span class="keyword">WHERE</span> deptno<span class="operator">=</span><span class="number">30</span>;</span><br></pre></td></tr></table></figure><p>​    完成后 <code>emp_ptn</code> 表中数据如下：</p><p><img src="https://pic.downk.cc/item/5ff40d263ffa7d37b3b02c50.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40d263ffa7d37b3b02c50.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="三、使用SQL语句插入值"><a href="#三、使用SQL语句插入值" class="headerlink" title="三、使用SQL语句插入值"></a>三、使用SQL语句插入值</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> tablename [<span class="keyword">PARTITION</span> (partcol1[<span class="operator">=</span>val1], partcol2[<span class="operator">=</span>val2] ...)] </span><br><span class="line"><span class="keyword">VALUES</span> ( <span class="keyword">value</span> [, <span class="keyword">value</span> ...] )</span><br></pre></td></tr></table></figure><ul><li>使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）；</li><li>如果目标表表支持 ACID 及其事务管理器，则插入后自动提交；</li><li>不支持支持复杂类型 (array, map, struct, union) 的插入。</li></ul><h2 id="四、更新和删除数据"><a href="#四、更新和删除数据" class="headerlink" title="四、更新和删除数据"></a>四、更新和删除数据</h2><h3 id="4-1-语法"><a href="#4-1-语法" class="headerlink" title="4.1 语法"></a>4.1 语法</h3><p>更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 更新</span></span><br><span class="line">UPDATE tablename <span class="keyword">SET</span> <span class="keyword">column</span> <span class="operator">=</span> <span class="keyword">value</span> [, <span class="keyword">column</span> <span class="operator">=</span> <span class="keyword">value</span> ...] [<span class="keyword">WHERE</span> expression]</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> tablename [<span class="keyword">WHERE</span> expression]</span><br></pre></td></tr></table></figure><h3 id="4-2-示例"><a href="#4-2-示例" class="headerlink" title="4.2 示例"></a>4.2 示例</h3><p><strong>1. 修改配置</strong></p><p>首先需要更改 <code>hive-site.xml</code>，添加如下配置，开启事务支持，配置完成后需要重启 Hive 服务。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.concurrency<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.enforce.bucketing<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.dynamic.partition.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nonstrict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.txn.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.compactor.initiator.on<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.in.test<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>2. 创建测试表</strong></p><p>创建用于测试的事务表，建表时候指定属性 <code>transactional = true</code> 则代表该表是事务表。需要注意的是，按照<a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions">官方文档</a> 的说明，目前 Hive 中的事务表有以下限制：</p><ul><li>必须是 buckets Table;</li><li>仅支持 ORC 文件格式；</li><li>不支持 LOAD DATA …语句。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> emp_ts(  </span><br><span class="line">  empno <span class="type">int</span>,  </span><br><span class="line">  ename String</span><br><span class="line">)</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (empno) <span class="keyword">INTO</span> <span class="number">2</span> BUCKETS STORED <span class="keyword">AS</span> ORC</span><br><span class="line">TBLPROPERTIES (&quot;transactional&quot;<span class="operator">=</span>&quot;true&quot;);</span><br></pre></td></tr></table></figure><p><strong>3. 插入测试数据</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_ts  <span class="keyword">VALUES</span> (<span class="number">1</span>,&quot;ming&quot;),(<span class="number">2</span>,&quot;hong&quot;);</span><br></pre></td></tr></table></figure><p>插入数据依靠的是 MapReduce 作业，执行成功后数据如下：</p><p><img src="https://pic.downk.cc/item/5ff40d2e3ffa7d37b3b033b2.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40d2e3ffa7d37b3b033b2.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><p><strong>4. 测试更新和删除</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--更新数据</span></span><br><span class="line">UPDATE emp_ts <span class="keyword">SET</span> ename <span class="operator">=</span> &quot;lan&quot;  <span class="keyword">WHERE</span>  empno<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--删除数据</span></span><br><span class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> emp_ts <span class="keyword">WHERE</span> empno<span class="operator">=</span><span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>更新和删除数据依靠的也是 MapReduce 作业，执行成功后数据如下：</p><p><img src="https://pic.downk.cc/item/5ff40d2e3ffa7d37b3b033b4.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40d2e3ffa7d37b3b033b4.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="五、查询结果写出到文件系统"><a href="#五、查询结果写出到文件系统" class="headerlink" title="五、查询结果写出到文件系统"></a>五、查询结果写出到文件系统</h2><h3 id="5-1-语法"><a href="#5-1-语法" class="headerlink" title="5.1 语法"></a>5.1 语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE [<span class="keyword">LOCAL</span>] DIRECTORY directory1</span><br><span class="line">  [<span class="type">ROW</span> FORMAT row_format] [STORED <span class="keyword">AS</span> file_format] </span><br><span class="line">  <span class="keyword">SELECT</span> ... <span class="keyword">FROM</span> ...</span><br></pre></td></tr></table></figure><ul><li><p>OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入；</p></li><li><p>和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的；</p></li><li><p>写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 定义列分隔符为&#x27;\t&#x27; </span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;./test-04&#x27;</span> </span><br><span class="line"><span class="type">row</span> format delimited </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line">COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;:&#x27;</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> src;</span><br></pre></td></tr></table></figure></li></ul><h3 id="5-2-示例"><a href="#5-2-示例" class="headerlink" title="5.2 示例"></a>5.2 示例</h3><p>这里我们将上面创建的 <code>emp_ptn</code> 表导出到本地文件系统，语句如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">LOCAL</span> DIRECTORY <span class="string">&#x27;/usr/file/ouput&#x27;</span></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp_ptn;</span><br></pre></td></tr></table></figure><p>导出结果如下：</p><p><img src="https://pic.downk.cc/item/5ff40d2e3ffa7d37b3b033b8.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff40d2e3ffa7d37b3b033b8.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive分区表和分桶表</title>
      <link href="/2021/10/15/Hive%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8/"/>
      <url>/2021/10/15/Hive%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="一、分区表"><a href="#一、分区表" class="headerlink" title="一、分区表"></a>一、分区表</h2><h3 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h3><p>Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。</p><p><strong>分区为 HDFS 上表目录的子目录</strong>，数据按照分区存储在子目录中。如果查询的 <code>where</code> 字句的中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。</p><blockquote><p>这里说明一下分区表并 Hive 独有的概念，实际上这个概念非常常见。比如在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。</p></blockquote><h3 id="1-2-使用场景"><a href="#1-2-使用场景" class="headerlink" title="1.2  使用场景"></a>1.2  使用场景</h3><p>通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。</p><h3 id="1-3-创建分区表"><a href="#1-3-创建分区表" class="headerlink" title="1.3 创建分区表"></a>1.3 创建分区表</h3><p>在 Hive 中可以使用 <code>PARTITIONED BY</code> 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_partition(</span><br><span class="line">   empno INT,</span><br><span class="line">   ename STRING,</span><br><span class="line">   job STRING,</span><br><span class="line">   mgr INT,</span><br><span class="line">   hiredate TIMESTAMP,</span><br><span class="line">   sal DECIMAL(7,2),</span><br><span class="line">   comm DECIMAL(7,2)</span><br><span class="line">   )</span><br><span class="line">   PARTITIONED BY (deptno INT)   -- 按照部门编号进行分区</span><br><span class="line">   ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\t&quot;</span><br><span class="line">   LOCATION &#x27;/hive/emp_partition&#x27;;</span><br></pre></td></tr></table></figure><h3 id="1-4-加载数据到分区表"><a href="#1-4-加载数据到分区表" class="headerlink" title="1.4 加载数据到分区表"></a>1.4 加载数据到分区表</h3><p>加载数据到分区表时候必须要指定数据所处的分区：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 加载部门编号为20的数据到表中</span></span><br><span class="line">LOAD DATA LOCAL INPATH &quot;/usr/file/emp20.txt&quot; OVERWRITE INTO TABLE emp_partition PARTITION (deptno=20)</span><br><span class="line"><span class="meta">#</span><span class="bash"> 加载部门编号为30的数据到表中</span></span><br><span class="line">LOAD DATA LOCAL INPATH &quot;/usr/file/emp30.txt&quot; OVERWRITE INTO TABLE emp_partition PARTITION (deptno=30)</span><br></pre></td></tr></table></figure><h3 id="1-5-查看分区目录"><a href="#1-5-查看分区目录" class="headerlink" title="1.5 查看分区目录"></a>1.5 查看分区目录</h3><p>这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 <code>deptno=20</code> 和 <code>deptno=30</code>,这就是分区目录，分区目录下才是我们加载的数据文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> hadoop fs -ls  hdfs://hadoop001:8020/hive/emp_partition/</span></span><br></pre></td></tr></table></figure><p>这时候当你的查询语句的 <code>where</code> 包含 <code>deptno=20</code>，则就去对应的分区目录下进行查找，而不用扫描全表。</p><p><img src="https://pic.downk.cc/item/5ff406b63ffa7d37b3ac2111.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff406b63ffa7d37b3ac2111.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="二、分桶表"><a href="#二、分桶表" class="headerlink" title="二、分桶表"></a>二、分桶表</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。</p><p>分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。</p><h3 id="1-2-理解分桶表"><a href="#1-2-理解分桶表" class="headerlink" title="1.2 理解分桶表"></a>1.2 理解分桶表</h3><p>单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。</p><p>当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图：</p><p><img src="https://pic.downk.cc/item/5ff408453ffa7d37b3ad0a67.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff408453ffa7d37b3ad0a67.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><blockquote><p>图片引用自：<a href="http://www.itcuties.com/java/hashmap-hashtable/">HashMap vs. Hashtable</a></p></blockquote><h3 id="1-3-创建分桶表"><a href="#1-3-创建分桶表" class="headerlink" title="1.3 创建分桶表"></a>1.3 创建分桶表</h3><p>在 Hive 中，我们可以通过 <code>CLUSTERED BY</code> 指定分桶列，并通过 <code>SORTED BY</code> 指定桶中数据的排序参考列。下面为分桶表建表语句示例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> emp_bucket(</span><br><span class="line">  empno <span class="type">INT</span>,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr <span class="type">INT</span>,</span><br><span class="line">  hiredate <span class="type">TIMESTAMP</span>,</span><br><span class="line">  sal <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  comm <span class="type">DECIMAL</span>(<span class="number">7</span>,<span class="number">2</span>),</span><br><span class="line">  deptno <span class="type">INT</span>)</span><br><span class="line">  CLUSTERED <span class="keyword">BY</span>(empno) SORTED <span class="keyword">BY</span>(empno <span class="keyword">ASC</span>) <span class="keyword">INTO</span> <span class="number">4</span> BUCKETS  <span class="comment">--按照员工编号散列到四个 bucket 中</span></span><br><span class="line">  <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;</span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_bucket&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="1-4-加载数据到分桶表"><a href="#1-4-加载数据到分桶表" class="headerlink" title="1.4 加载数据到分桶表"></a>1.4 加载数据到分桶表</h3><p>这里直接使用 <code>Load</code> 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。</p><p>这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下：</p><h4 id="1-设置强制分桶"><a href="#1-设置强制分桶" class="headerlink" title="1. 设置强制分桶"></a>1. 设置强制分桶</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing <span class="operator">=</span> <span class="literal">true</span>; <span class="comment">--Hive 2.x 不需要这一步</span></span><br></pre></td></tr></table></figure><p>在 Hive 0.x and 1.x 版本，必须使用设置 <code>hive.enforce.bucketing = true</code>，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by  column 来进行分桶。</p><h4 id="2-CTAS导入数据"><a href="#2-CTAS导入数据" class="headerlink" title="2. CTAS导入数据"></a>2. CTAS导入数据</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> emp_bucket <span class="keyword">SELECT</span> <span class="operator">*</span>  <span class="keyword">FROM</span> emp;  <span class="comment">--这里的 emp 表就是一张普通的雇员表</span></span><br></pre></td></tr></table></figure><p>可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致：</p><p><img src="https://pic.downk.cc/item/5ff406b63ffa7d37b3ac2118.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff406b63ffa7d37b3ac2118.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="1-5-查看分桶文件"><a href="#1-5-查看分桶文件" class="headerlink" title="1.5 查看分桶文件"></a>1.5 查看分桶文件</h3><p>bucket(桶) 本质上就是表目录下的具体文件：</p><p><img src="https://pic.downk.cc/item/5ff406b63ffa7d37b3ac2113.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff406b63ffa7d37b3ac2113.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="三、分区表和分桶表结合使用"><a href="#三、分区表和分桶表结合使用" class="headerlink" title="三、分区表和分桶表结合使用"></a>三、分区表和分桶表结合使用</h2><p>分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view_bucketed(</span><br><span class="line">viewTime <span class="type">INT</span>, </span><br><span class="line">    userid <span class="type">BIGINT</span>,</span><br><span class="line">    page_url STRING, </span><br><span class="line">    referrer_url STRING,</span><br><span class="line">    ip STRING )</span><br><span class="line"> PARTITIONED <span class="keyword">BY</span>(dt STRING)</span><br><span class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span></span><br><span class="line">   COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\002&#x27;</span></span><br><span class="line">   MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\003&#x27;</span></span><br><span class="line"> STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure><p>此时导入数据时需要指定分区：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE page_view_bucketed</span><br><span class="line">PARTITION (dt=&#x27;2009-02-25&#x27;)</span><br><span class="line">SELECT * FROM page_view WHERE dt=&#x27;2009-02-25&#x27;;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive常用DDL操作</title>
      <link href="/2021/10/15/Hive%E5%B8%B8%E7%94%A8DDL%E6%93%8D%E4%BD%9C/"/>
      <url>/2021/10/15/Hive%E5%B8%B8%E7%94%A8DDL%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Database"><a href="#一、Database" class="headerlink" title="一、Database"></a>一、Database</h2><h3 id="1-1-查看数据列表"><a href="#1-1-查看数据列表" class="headerlink" title="1.1 查看数据列表"></a>1.1 查看数据列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br></pre></td></tr></table></figure><h3 id="1-2-使用数据库"><a href="#1-2-使用数据库" class="headerlink" title="1.2 使用数据库"></a>1.2 使用数据库</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br></pre></td></tr></table></figure><h3 id="1-3-新建数据库"><a href="#1-3-新建数据库" class="headerlink" title="1.3 新建数据库"></a>1.3 新建数据库</h3><p>语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name   --DATABASE|SCHEMA 是等价的</span><br><span class="line">  [COMMENT database_comment] --数据库注释</span><br><span class="line">  [LOCATION hdfs_path] --存储在 HDFS 上的位置</span><br><span class="line">  [WITH DBPROPERTIES (property_name=property_value, ...)]; --指定额外属性</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE IF NOT EXISTS hive_test</span><br><span class="line">  COMMENT <span class="string">&#x27;hive database for test&#x27;</span></span><br><span class="line">  WITH DBPROPERTIES (<span class="string">&#x27;create&#x27;</span>=<span class="string">&#x27;heibaiying&#x27;</span>);</span><br></pre></td></tr></table></figure><h3 id="1-4-查看数据库信息"><a href="#1-4-查看数据库信息" class="headerlink" title="1.4 查看数据库信息"></a>1.4 查看数据库信息</h3><p>语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DESC DATABASE [EXTENDED] db_name; --EXTENDED 表示是否显示额外属性</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DESC DATABASE  EXTENDED hive_test;</span><br></pre></td></tr></table></figure><h3 id="1-5-删除数据库"><a href="#1-5-删除数据库" class="headerlink" title="1.5 删除数据库"></a>1.5 删除数据库</h3><p>语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure><ul><li>默认行为是 RESTRICT，如果数据库中存在表则删除失败。要想删除库及其中的表，可以使用 CASCADE 级联删除。</li></ul><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP DATABASE IF EXISTS hive_test CASCADE;</span><br></pre></td></tr></table></figure><h2 id="二、创建表"><a href="#二、创建表" class="headerlink" title="二、创建表"></a>二、创建表</h2><h3 id="2-1-建表语法"><a href="#2-1-建表语法" class="headerlink" title="2.1 建表语法"></a>2.1 建表语法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name     --表名</span><br><span class="line">  [(col_name data_type [COMMENT col_comment],</span><br><span class="line">    ... [constraint_specification])]  --列名 列数据类型</span><br><span class="line">  [COMMENT table_comment]   --表描述</span><br><span class="line">  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  --分区表分区规则</span><br><span class="line">  [</span><br><span class="line">    CLUSTERED BY (col_name, col_name, ...) </span><br><span class="line">   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS</span><br><span class="line">  ]  --分桶表分桶规则</span><br><span class="line">  [SKEWED BY (col_name, col_name, ...) ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)  </span><br><span class="line">   [STORED AS DIRECTORIES] </span><br><span class="line">  ]  --指定倾斜列和值</span><br><span class="line">  [</span><br><span class="line">   [ROW FORMAT row_format]    </span><br><span class="line">   [STORED AS file_format]</span><br><span class="line">     | STORED BY <span class="string">&#x27;storage.handler.class.name&#x27;</span> [WITH SERDEPROPERTIES (...)]  </span><br><span class="line">  ]  -- 指定行分隔符、存储文件格式或采用自定义存储格式</span><br><span class="line">  [LOCATION hdfs_path]  -- 指定表的存储位置</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]  --指定表的属性</span><br><span class="line">  [AS select_statement];   --从查询结果创建表</span><br></pre></td></tr></table></figure><h3 id="2-2-内部表"><a href="#2-2-内部表" class="headerlink" title="2.2 内部表"></a>2.2 内部表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE emp(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2),</span><br><span class="line">  deptno INT)</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">&quot;\t&quot;</span>;</span><br></pre></td></tr></table></figure><h3 id="2-3-外部表"><a href="#2-3-外部表" class="headerlink" title="2.3 外部表"></a>2.3 外部表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_external(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2),</span><br><span class="line">  deptno INT)</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">&quot;\t&quot;</span></span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_external&#x27;</span>;</span><br></pre></td></tr></table></figure><p>使用 <code>desc format  emp_external</code> 命令可以查看表的详细信息如下：</p><p><img src="https://pic.downk.cc/item/5ff404313ffa7d37b3aa9b1a.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff404313ffa7d37b3aa9b1a.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="2-4-分区表"><a href="#2-4-分区表" class="headerlink" title="2.4 分区表"></a>2.4 分区表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_partition(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2)</span><br><span class="line">  )</span><br><span class="line">  PARTITIONED BY (deptno INT)   -- 按照部门编号进行分区</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">&quot;\t&quot;</span></span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_partition&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="2-5-分桶表"><a href="#2-5-分桶表" class="headerlink" title="2.5 分桶表"></a>2.5 分桶表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_bucket(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2),</span><br><span class="line">  deptno INT)</span><br><span class="line">  CLUSTERED BY(empno) SORTED BY(empno ASC) INTO 4 BUCKETS  --按照员工编号散列到四个 bucket 中</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">&quot;\t&quot;</span></span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_bucket&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="2-6-倾斜表"><a href="#2-6-倾斜表" class="headerlink" title="2.6 倾斜表"></a>2.6 倾斜表</h3><p>通过指定一个或者多个列经常出现的值（严重偏斜），Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，这使得性能得到提升。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE emp_skewed(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2)</span><br><span class="line">  )</span><br><span class="line">  SKEWED BY (empno) ON (66,88,100)  --指定 empno 的倾斜值 66,88,100</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">&quot;\t&quot;</span></span><br><span class="line">  LOCATION <span class="string">&#x27;/hive/emp_skewed&#x27;</span>;   </span><br></pre></td></tr></table></figure><h3 id="2-7-临时表"><a href="#2-7-临时表" class="headerlink" title="2.7 临时表"></a>2.7 临时表</h3><p>临时表仅对当前 session 可见，临时表的数据将存储在用户的暂存目录中，并在会话结束后删除。如果临时表与永久表表名相同，则对该表名的任何引用都将解析为临时表，而不是永久表。临时表还具有以下两个限制：</p><ul><li>不支持分区列；</li><li>不支持创建索引。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY TABLE emp_temp(</span><br><span class="line">  empno INT,</span><br><span class="line">  ename STRING,</span><br><span class="line">  job STRING,</span><br><span class="line">  mgr INT,</span><br><span class="line">  hiredate TIMESTAMP,</span><br><span class="line">  sal DECIMAL(7,2),</span><br><span class="line">  comm DECIMAL(7,2)</span><br><span class="line">  )</span><br><span class="line">  ROW FORMAT DELIMITED FIELDS TERMINATED BY <span class="string">&quot;\t&quot;</span>;</span><br></pre></td></tr></table></figure><h3 id="2-8-CTAS创建表"><a href="#2-8-CTAS创建表" class="headerlink" title="2.8 CTAS创建表"></a>2.8 CTAS创建表</h3><p>支持从查询语句的结果创建表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE emp_copy AS SELECT * FROM emp WHERE deptno=<span class="string">&#x27;20&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="2-9-复制表结构"><a href="#2-9-复制表结构" class="headerlink" title="2.9 复制表结构"></a>2.9 复制表结构</h3><p>语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name  --创建表表名</span><br><span class="line">   LIKE existing_table_or_view_name  --被复制表的表名</span><br><span class="line">   [LOCATION hdfs_path]; --存储位置</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY EXTERNAL TABLE  IF NOT EXISTS  emp_co  LIKE emp</span><br></pre></td></tr></table></figure><h3 id="2-10-加载数据到表"><a href="#2-10-加载数据到表" class="headerlink" title="2.10 加载数据到表"></a>2.10 加载数据到表</h3><p>加载数据到表中属于 DML 操作，这里为了方便大家测试，先简单介绍一下加载本地数据到表中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 加载数据到 emp 表中</span><br><span class="line">load data <span class="built_in">local</span> inpath <span class="string">&quot;/usr/file/emp.txt&quot;</span> into table emp;</span><br></pre></td></tr></table></figure><p>其中 emp.txt 的内容如下，你可以直接复制使用，也可以到本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">7369SMITHCLERK79021980-12-17 00:00:00800.0020</span><br><span class="line">7499ALLENSALESMAN76981981-02-20 00:00:001600.00300.0030</span><br><span class="line">7521WARDSALESMAN76981981-02-22 00:00:001250.00500.0030</span><br><span class="line">7566JONESMANAGER78391981-04-02 00:00:002975.0020</span><br><span class="line">7654MARTINSALESMAN76981981-09-28 00:00:001250.001400.0030</span><br><span class="line">7698BLAKEMANAGER78391981-05-01 00:00:002850.0030</span><br><span class="line">7782CLARKMANAGER78391981-06-09 00:00:002450.0010</span><br><span class="line">7788SCOTTANALYST75661987-04-19 00:00:001500.0020</span><br><span class="line">7839KINGPRESIDENT1981-11-17 00:00:005000.0010</span><br><span class="line">7844TURNERSALESMAN76981981-09-08 00:00:001500.000.0030</span><br><span class="line">7876ADAMSCLERK77881987-05-23 00:00:001100.0020</span><br><span class="line">7900JAMESCLERK76981981-12-03 00:00:00950.0030</span><br><span class="line">7902FORDANALYST75661981-12-03 00:00:003000.0020</span><br><span class="line">7934MILLERCLERK77821982-01-23 00:00:001300.0010</span><br></pre></td></tr></table></figure><p>加载后可查询表中数据：</p><p><img src="https://pic.downk.cc/item/5ff404313ffa7d37b3aa9b1c.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff404313ffa7d37b3aa9b1c.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="三、修改表"><a href="#三、修改表" class="headerlink" title="三、修改表"></a>三、修改表</h2><h3 id="3-1-重命名表"><a href="#3-1-重命名表" class="headerlink" title="3.1 重命名表"></a>3.1 重命名表</h3><p>语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name RENAME TO new_table_name;</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE emp_temp RENAME TO new_emp; --把 emp_temp 表重命名为 new_emp</span><br></pre></td></tr></table></figure><h3 id="3-2-修改列"><a href="#3-2-修改列" class="headerlink" title="3.2 修改列"></a>3.2 修改列</h3><p>语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name [PARTITION partition_spec] CHANGE [COLUMN] col_old_name col_new_name column_type</span><br><span class="line">  [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT];</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- 修改字段名和类型</span><br><span class="line">ALTER TABLE emp_temp CHANGE empno empno_new INT;</span><br><span class="line"> </span><br><span class="line">-- 修改字段 sal 的名称 并将其放置到 empno 字段后</span><br><span class="line">ALTER TABLE emp_temp CHANGE sal sal_new decimal(7,2)  AFTER ename;</span><br><span class="line"></span><br><span class="line">-- 为字段增加注释</span><br><span class="line">ALTER TABLE emp_temp CHANGE mgr mgr_new INT COMMENT <span class="string">&#x27;this is column mgr&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="3-3-新增列"><a href="#3-3-新增列" class="headerlink" title="3.3 新增列"></a>3.3 新增列</h3><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE emp_temp ADD COLUMNS (address STRING COMMENT <span class="string">&#x27;home address&#x27;</span>);</span><br></pre></td></tr></table></figure><h2 id="四、清空表-删除表"><a href="#四、清空表-删除表" class="headerlink" title="四、清空表/删除表"></a>四、清空表/删除表</h2><h3 id="4-1-清空表"><a href="#4-1-清空表" class="headerlink" title="4.1 清空表"></a>4.1 清空表</h3><p>语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 清空整个表或表指定分区中的数据</span><br><span class="line">TRUNCATE TABLE table_name [PARTITION (partition_column = partition_col_value,  ...)];</span><br></pre></td></tr></table></figure><ul><li>目前只有内部表才能执行 TRUNCATE 操作，外部表执行时会抛出异常 <code>Cannot truncate non-managed table XXXX</code>。</li></ul><p>示例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TRUNCATE TABLE emp_mgt_ptn PARTITION (deptno=20);</span><br></pre></td></tr></table></figure><h3 id="4-2-删除表"><a href="#4-2-删除表" class="headerlink" title="4.2 删除表"></a>4.2 删除表</h3><p>语法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE [IF EXISTS] table_name [PURGE]; </span><br></pre></td></tr></table></figure><ul><li>内部表：不仅会删除表的元数据，同时会删除 HDFS 上的数据；</li><li>外部表：只会删除表的元数据，不会删除 HDFS 上的数据；</li><li>删除视图引用的表时，不会给出警告（但视图已经无效了，必须由用户删除或重新创建）。</li></ul><h2 id="五、其他命令"><a href="#五、其他命令" class="headerlink" title="五、其他命令"></a>五、其他命令</h2><h3 id="5-1-Describe"><a href="#5-1-Describe" class="headerlink" title="5.1 Describe"></a>5.1 Describe</h3><p>查看数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE|Desc DATABASE [EXTENDED] db_name;  --EXTENDED 是否显示额外属性</span><br></pre></td></tr></table></figure><p>查看表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DESCRIBE|Desc [EXTENDED|FORMATTED] table_name --FORMATTED 以友好的展现方式查看表详情</span><br></pre></td></tr></table></figure><h3 id="5-2-Show"><a href="#5-2-Show" class="headerlink" title="5.2 Show"></a>5.2 Show</h3><p><strong>1. 查看数据库列表</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- 语法</span><br><span class="line">SHOW (DATABASES|SCHEMAS) [LIKE <span class="string">&#x27;identifier_with_wildcards&#x27;</span>];</span><br><span class="line"></span><br><span class="line">-- 示例：</span><br><span class="line">SHOW DATABASES like <span class="string">&#x27;hive*&#x27;</span>;</span><br></pre></td></tr></table></figure><p>LIKE 子句允许使用正则表达式进行过滤，但是 SHOW 语句当中的 LIKE 子句只支持 <code>*</code>（通配符）和 <code>|</code>（条件或）两个符号。例如 <code>employees</code>，<code>emp *</code>，<code>emp * | * ees</code>，所有这些都将匹配名为 <code>employees</code> 的数据库。</p><p><strong>2. 查看表的列表</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- 语法</span><br><span class="line">SHOW TABLES [IN database_name] [<span class="string">&#x27;identifier_with_wildcards&#x27;</span>];</span><br><span class="line"></span><br><span class="line">-- 示例</span><br><span class="line">SHOW TABLES IN default;</span><br></pre></td></tr></table></figure><p><strong>3. 查看视图列表</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW VIEWS [IN/FROM database_name] [LIKE <span class="string">&#x27;pattern_with_wildcards&#x27;</span>];   --仅支持 Hive 2.2.0 +</span><br></pre></td></tr></table></figure><p><strong>4. 查看表的分区列表</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW PARTITIONS table_name;</span><br></pre></td></tr></table></figure><p><strong>5. 查看表/视图的创建语句</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW CREATE TABLE ([db_name.]table_name|view_name);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive CLI和Beeline命令行的基本使用</title>
      <link href="/2021/10/15/CLI%E5%92%8CBeeline%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
      <url>/2021/10/15/CLI%E5%92%8CBeeline%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="一、Hive-CLI"><a href="#一、Hive-CLI" class="headerlink" title="一、Hive CLI"></a>一、Hive CLI</h2><h3 id="1-1-Help"><a href="#1-1-Help" class="headerlink" title="1.1 Help"></a>1.1 Help</h3><p>使用 <code>hive -H</code> 或者 <code>hive --help</code> 命令可以查看所有命令的帮助，显示如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable subsitution to apply to hive </span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B  --定义用户自定义变量</span><br><span class="line">    --database &lt;databasename&gt;     Specify the database to use  -- 指定使用的数据库</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from <span class="built_in">command</span> line   -- 执行指定的 SQL</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files   --执行 SQL 脚本</span><br><span class="line"> -H,--<span class="built_in">help</span>                        Print <span class="built_in">help</span> information  -- 打印帮助信息</span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value <span class="keyword">for</span> given property    --自定义配置</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable subsitution to apply to hive  --自定义变量</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file  --在进入交互模式之前运行初始化脚本</span><br><span class="line"> -S,--silent                      Silent mode <span class="keyword">in</span> interactive shell    --静默模式</span><br><span class="line"> -v,--verbose                     Verbose mode (<span class="built_in">echo</span> executed SQL to the  console)  --详细模式</span><br></pre></td></tr></table></figure><h3 id="1-2-交互式命令行"><a href="#1-2-交互式命令行" class="headerlink" title="1.2 交互式命令行"></a>1.2 交互式命令行</h3><p>直接使用 <code>Hive</code> 命令，不加任何参数，即可进入交互式命令行。</p><h3 id="1-3-执行SQL命令"><a href="#1-3-执行SQL命令" class="headerlink" title="1.3 执行SQL命令"></a>1.3 执行SQL命令</h3><p>在不进入交互式命令行的情况下，可以使用 <code>hive -e </code> 执行 SQL 命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -e <span class="string">&#x27;select * from emp&#x27;</span>;</span><br></pre></td></tr></table></figure><p><img src="https://pic.downk.cc/item/5ff3fe683ffa7d37b3a6c955.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff3fe683ffa7d37b3a6c955.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="1-4-执行SQL脚本"><a href="#1-4-执行SQL脚本" class="headerlink" title="1.4 执行SQL脚本"></a>1.4 执行SQL脚本</h3><p>用于执行的 sql 脚本可以在本地文件系统，也可以在 HDFS 上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本地文件系统</span></span><br><span class="line">hive -f /usr/file/simple.sql;</span><br><span class="line"></span><br><span class="line"><span class="comment"># HDFS文件系统</span></span><br><span class="line">hive -f hdfs://hadoop001:8020/tmp/simple.sql;</span><br></pre></td></tr></table></figure><p>其中 <code>simple.sql</code> 内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from emp;</span><br></pre></td></tr></table></figure><h3 id="1-5-配置Hive变量"><a href="#1-5-配置Hive变量" class="headerlink" title="1.5 配置Hive变量"></a>1.5 配置Hive变量</h3><p>可以使用 <code>--hiveconf</code> 设置 Hive 运行时的变量。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive -e <span class="string">&#x27;select * from emp&#x27;</span> \</span><br><span class="line">--hiveconf hive.exec.scratchdir=/tmp/hive_scratch  \</span><br><span class="line">--hiveconf mapred.reduce.tasks=4;</span><br></pre></td></tr></table></figure><blockquote><p>hive.exec.scratchdir：指定 HDFS 上目录位置，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。</p></blockquote><h3 id="1-6-配置文件启动"><a href="#1-6-配置文件启动" class="headerlink" title="1.6 配置文件启动"></a>1.6 配置文件启动</h3><p>使用 <code>-i</code> 可以在进入交互模式之前运行初始化脚本，相当于指定配置文件启动。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive -i /usr/file/hive-init.conf;</span><br></pre></td></tr></table></figure><p>其中 <code>hive-init.conf</code> 的内容如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.mode.local.auto = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><blockquote><p>hive.exec.mode.local.auto 默认值为 false，这里设置为 true ，代表开启本地模式。</p></blockquote><h3 id="1-7-用户自定义变量"><a href="#1-7-用户自定义变量" class="headerlink" title="1.7 用户自定义变量"></a>1.7 用户自定义变量</h3><p><code>--define  </code> 和 <code>--hivevar   </code> 在功能上是等价的，都是用来实现自定义变量，这里给出一个示例:</p><p>定义变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive  --define  n=ename --hiveconf  --hivevar j=job;</span><br></pre></td></tr></table></figure><p>在查询中引用自定义变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下两条语句等价</span></span><br><span class="line">hive &gt; select <span class="variable">$&#123;n&#125;</span> from emp;</span><br><span class="line">hive &gt;  select <span class="variable">$&#123;hivevar:n&#125;</span> from emp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下两条语句等价</span></span><br><span class="line">hive &gt; select <span class="variable">$&#123;j&#125;</span> from emp;</span><br><span class="line">hive &gt;  select <span class="variable">$&#123;hivevar:j&#125;</span> from emp;</span><br></pre></td></tr></table></figure><p>结果如下：</p><p><img src="https://pic.downk.cc/item/5ff3fe683ffa7d37b3a6c957.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff3fe683ffa7d37b3a6c957.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="二、Beeline"><a href="#二、Beeline" class="headerlink" title="二、Beeline"></a>二、Beeline</h2><h3 id="2-1-HiveServer2"><a href="#2-1-HiveServer2" class="headerlink" title="2.1 HiveServer2"></a>2.1 HiveServer2</h3><p>Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，所以产生了 HiveServer2。</p><p>HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web  服务器。</p><p>HiveServer2 拥有自己的 CLI(Beeline)，Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于  HiveServer2 是 Hive 开发维护的重点 (Hive0.15 后就不再支持 hiveserver)，所以 Hive CLI  已经不推荐使用了，官方更加推荐使用 Beeline。</p><h3 id="2-1-Beeline"><a href="#2-1-Beeline" class="headerlink" title="2.1 Beeline"></a>2.1 Beeline</h3><p>Beeline 拥有更多可使用参数，可以使用 <code>beeline --help</code> 查看。</p><h3 id="2-2-常用参数"><a href="#2-2-常用参数" class="headerlink" title="2.2 常用参数"></a>2.2 常用参数</h3><p>在 Hive CLI 中支持的参数，Beeline 都支持，常用的参数如下。更多参数说明可以参见官方文档 <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93NewCommandLineShell">Beeline Command Options</a></p><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>**-u **</td><td>数据库地址</td></tr><tr><td>**-n **</td><td>用户名</td></tr><tr><td>**-p **</td><td>密码</td></tr><tr><td>**-d **</td><td>驱动 (可选)</td></tr><tr><td>**-e **</td><td>执行 SQL 命令</td></tr><tr><td>**-f **</td><td>执行 SQL 脚本</td></tr><tr><td>**-i  (or)–init  **</td><td>在进入交互模式之前运行初始化脚本</td></tr><tr><td>**–property-file **</td><td>指定配置文件</td></tr><tr><td><strong>–hiveconf</strong> <em>property</em>*=*<em>value</em></td><td>指定配置属性</td></tr><tr><td><strong>–hivevar</strong> <em>name</em>*=*<em>value</em></td><td>用户自定义属性，在会话级别有效</td></tr></tbody></table><p>示例： 使用用户名和密码连接 Hive</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ beeline -u jdbc:hive2://localhost:10000  -n username -p password </span><br></pre></td></tr></table></figure><h2 id="三、Hive配置"><a href="#三、Hive配置" class="headerlink" title="三、Hive配置"></a>三、Hive配置</h2><p>可以通过三种方式对 Hive 的相关属性进行配置，分别介绍如下：</p><h3 id="3-1-配置文件"><a href="#3-1-配置文件" class="headerlink" title="3.1 配置文件"></a>3.1 配置文件</h3><p>方式一为使用配置文件，使用配置文件指定的配置是永久有效的。Hive 有以下三个可选的配置文件：</p><ul><li>hive-site.xml ：Hive 的主要配置文件；</li><li>hivemetastore-site.xml： 关于元数据的配置；</li><li>hiveserver2-site.xml：关于 HiveServer2 的配置。</li></ul><p>示例如下,在 hive-site.xml 配置 <code>hive.exec.scratchdir</code>：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/mydir<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>Scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="3-2-hiveconf"><a href="#3-2-hiveconf" class="headerlink" title="3.2 hiveconf"></a>3.2 hiveconf</h3><p>方式二为在启动命令行 (Hive CLI / Beeline) 的时候使用 <code>--hiveconf</code> 指定配置，这种方式指定的配置作用于整个 Session。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --hiveconf hive.exec.scratchdir=/tmp/mydir</span><br></pre></td></tr></table></figure><h3 id="3-3-set"><a href="#3-3-set" class="headerlink" title="3.3 set"></a>3.3 set</h3><p>方式三为在交互式环境下 (Hive CLI / Beeline)，使用 set 命令指定。这种设置的作用范围也是 Session 级别的，配置对于执行该命令后的所有命令生效。set 兼具设置参数和查看参数的功能。如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir=/tmp/mydir;</span><br><span class="line">No rows affected (0.025 seconds)</span><br><span class="line">0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir;</span><br><span class="line">+----------------------------------+--+</span><br><span class="line">|               set                |</span><br><span class="line">+----------------------------------+--+</span><br><span class="line">| hive.exec.scratchdir=/tmp/mydir  |</span><br><span class="line">+----------------------------------+--+</span><br></pre></td></tr></table></figure><h3 id="3-4-配置优先级"><a href="#3-4-配置优先级" class="headerlink" title="3.4 配置优先级"></a>3.4 配置优先级</h3><p>配置的优先顺序如下 (由低到高)：<br><code>hive-site.xml</code> - &gt;<code>hivemetastore-site.xml</code>- &gt; <code>hiveserver2-site.xml</code> - &gt;<code> -- hiveconf</code>- &gt; <code>set</code></p><h3 id="3-5-配置参数"><a href="#3-5-配置参数" class="headerlink" title="3.5 配置参数"></a>3.5 配置参数</h3><p>Hive 可选的配置参数非常多，在用到时查阅官方文档即可<a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration">AdminManual Configuration</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive安装</title>
      <link href="/2021/10/15/hive%E5%AE%89%E8%A3%85/"/>
      <url>/2021/10/15/hive%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="一、安装Hive"><a href="#一、安装Hive" class="headerlink" title="一、安装Hive"></a>一、安装Hive</h2><h3 id="1-1-下载并解压"><a href="#1-1-下载并解压" class="headerlink" title="1.1 下载并解压"></a>1.1 下载并解压</h3><p>下载所需版本的 Hive，这里我下载版本为 <code>cdh5.15.2</code>。下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载后进行解压</span></span><br><span class="line"> tar -zxvf hive-1.1.0-cdh5.15.2.tar.gz</span><br></pre></td></tr></table></figure><h3 id="1-2-配置环境变量"><a href="#1-2-配置环境变量" class="headerlink" title="1.2 配置环境变量"></a>1.2 配置环境变量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/profile</span></span><br></pre></td></tr></table></figure><p>添加环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HIVE_HOME=/usr/app/hive-1.1.0-cdh5.15.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$HIVE_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure><p>使得配置的环境变量立即生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># source /etc/profile</span></span><br></pre></td></tr></table></figure><h3 id="1-3-修改配置"><a href="#1-3-修改配置" class="headerlink" title="1.3 修改配置"></a>1.3 修改配置</h3><p><strong>1. hive-env.sh</strong></p><p>进入安装目录下的 <code>conf/</code> 目录，拷贝 Hive 的环境配置模板 <code>flume-env.sh.template</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure><p>修改 <code>hive-env.sh</code>，指定 Hadoop 的安装路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br></pre></td></tr></table></figure><p><strong>2. hive-site.xml</strong></p><p>新建 hive-site.xml 文件，内容如下，主要是配置存放元数据的 MySQL 的地址、驱动、用户名和密码等信息：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop001:3306/hadoop_hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="1-4-拷贝数据库驱动"><a href="#1-4-拷贝数据库驱动" class="headerlink" title="1.4 拷贝数据库驱动"></a>1.4 拷贝数据库驱动</h3><p>将 MySQL 驱动包拷贝到 Hive 安装目录的 <code>lib</code> 目录下, MySQL 驱动的下载地址为：<a href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a>  , 在本仓库的<a href="https://github.com/heibaiying/BigData-Notes/tree/master/resources">resources</a> 目录下我也上传了一份，有需要的可以自行下载。</p><p><img src="https://pic.downk.cc/item/5ff3fb793ffa7d37b3a5056c.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff3fb793ffa7d37b3a5056c.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="1-5-初始化元数据库"><a href="#1-5-初始化元数据库" class="headerlink" title="1.5 初始化元数据库"></a>1.5 初始化元数据库</h3><ul><li><p>当使用的 hive 是 1.x 版本时，可以不进行初始化操作，Hive 会在第一次启动的时候会自动进行初始化，但不会生成所有的元数据信息表，只会初始化必要的一部分，在之后的使用中用到其余表时会自动创建；</p></li><li><p>当使用的 hive 是 2.x 版本时，必须手动初始化元数据库。初始化命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># schematool 命令在安装目录的 bin 目录下，由于上面已经配置过环境变量，在任意位置执行即可</span></span><br><span class="line">schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure></li></ul><p>这里我使用的是 CDH 的 <code>hive-1.1.0-cdh5.15.2.tar.gz</code>，对应 <code>Hive 1.1.0</code> 版本，可以跳过这一步。</p><h3 id="1-6-启动"><a href="#1-6-启动" class="headerlink" title="1.6 启动"></a>1.6 启动</h3><p>由于已经将 Hive 的 bin 目录配置到环境变量，直接使用以下命令启动，成功进入交互式命令行后执行 <code>show databases</code> 命令，无异常则代表搭建成功。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hive</span></span><br></pre></td></tr></table></figure><p><img src="https://pic.downk.cc/item/5ff3fb793ffa7d37b3a5056e.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff3fb793ffa7d37b3a5056e.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><p>在 Mysql 中也能看到 Hive 创建的库和存放元数据信息的表</p><p><img src="https://pic.downk.cc/item/5ff3fb793ffa7d37b3a50572.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff3fb793ffa7d37b3a50572.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="二、HiveServer2-beeline"><a href="#二、HiveServer2-beeline" class="headerlink" title="二、HiveServer2/beeline"></a>二、HiveServer2/beeline</h2><p>Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是  HiveServer 不能处理多个客户端的并发请求，因此产生了  HiveServer2。HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive  提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务。</p><p>HiveServer2 拥有自己的 CLI 工具——Beeline。Beeline 是一个基于 SQLLine 的 JDBC  客户端。由于目前 HiveServer2 是 Hive 开发维护的重点，所以官方更加推荐使用 Beeline 而不是 Hive  CLI。以下主要讲解 Beeline 的配置方式。</p><h3 id="2-1-修改Hadoop配置"><a href="#2-1-修改Hadoop配置" class="headerlink" title="2.1 修改Hadoop配置"></a>2.1 修改Hadoop配置</h3><p>修改 hadoop 集群的 core-site.xml 配置文件，增加如下配置，指定 hadoop 的 root 用户可以代理本机上所有的用户。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>之所以要配置这一步，是因为 hadoop 2.0 以后引入了安全伪装机制，使得 hadoop 不允许上层系统（如  hive）直接将实际用户传递到 hadoop 层，而应该将实际用户传递给一个超级代理，由该代理在 hadoop  上执行操作，以避免任意客户端随意操作 hadoop。如果不配置这一步，在之后的连接中可能会抛出 <code>AuthorizationException</code> 异常。</p><blockquote><p>关于 Hadoop 的用户代理机制，可以参考：<a href="https://blog.csdn.net/u012948976/article/details/49904675#%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E8%A7%A3%E8%AF%BB">hadoop 的用户代理机制</a> 或 <a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/Superusers.html">Superusers Acting On Behalf Of Other Users</a></p></blockquote><h3 id="2-2-启动hiveserver2"><a href="#2-2-启动hiveserver2" class="headerlink" title="2.2 启动hiveserver2"></a>2.2 启动hiveserver2</h3><p>由于上面已经配置过环境变量，这里直接启动即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nohup hiveserver2 &amp;</span></span><br></pre></td></tr></table></figure><h3 id="2-3-使用beeline"><a href="#2-3-使用beeline" class="headerlink" title="2.3 使用beeline"></a>2.3 使用beeline</h3><p>可以使用以下命令进入 beeline 交互式命令行，出现 <code>Connected</code> 则代表连接成功。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># beeline -u jdbc:hive2://hadoop001:10000 -n root</span></span><br></pre></td></tr></table></figure><p><img src="https://pic.downk.cc/item/5ff3fb793ffa7d37b3a50575.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff3fb793ffa7d37b3a50575.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive的简介及核心概念</title>
      <link href="/2021/10/15/hive%E7%9A%84%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/"/>
      <url>/2021/10/15/hive%E7%9A%84%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。</p><p><strong>特点</strong>：</p><ol><li>简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析；</li><li>灵活性高，可以自定义用户函数 (UDF) 和存储格式；</li><li>为超大的数据集设计的计算和存储能力，集群扩展容易;</li><li>统一的元数据管理，可与 presto／impala／sparksql 等共享数据；</li><li>执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。</li></ol><h2 id="二、Hive的体系架构"><a href="#二、Hive的体系架构" class="headerlink" title="二、Hive的体系架构"></a>二、Hive的体系架构</h2><p><img src="https://pic.downk.cc/item/5ff3dc473ffa7d37b3928f6e.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff3dc473ffa7d37b3928f6e.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="Hive的体系架构"></p><h3 id="2-1-command-line-shell-amp-thrift-jdbc"><a href="#2-1-command-line-shell-amp-thrift-jdbc" class="headerlink" title="2.1 command-line shell &amp; thrift/jdbc"></a>2.1 command-line shell &amp; thrift/jdbc</h3><p>可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据：</p><ul><li><strong>command-line shell</strong>：通过 hive 命令行的的方式来操作数据；</li><li><strong>thrift／jdbc</strong>：通过 thrift 协议按照标准的 JDBC 的方式操作数据。</li></ul><h3 id="2-2-Metastore"><a href="#2-2-Metastore" class="headerlink" title="2.2 Metastore"></a>2.2 Metastore</h3><p>在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby  数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替  derby。</p><p>Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql  中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql  中创建一张表，在 Hive 中也可以直接使用。</p><h3 id="2-3-HQL的执行流程"><a href="#2-3-HQL的执行流程" class="headerlink" title="2.3 HQL的执行流程"></a>2.3 HQL的执行流程</h3><p>Hive 在执行一条 HQL 的时候，会经过以下步骤：</p><ol><li>语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree；</li><li>语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock；</li><li>生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree；</li><li>优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量；</li><li>生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务；</li><li>优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。</li></ol><blockquote><p>关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章：<a href="https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html">Hive SQL 的编译过程</a></p></blockquote><h2 id="三、数据类型"><a href="#三、数据类型" class="headerlink" title="三、数据类型"></a>三、数据类型</h2><h3 id="3-1-基本数据类型"><a href="#3-1-基本数据类型" class="headerlink" title="3.1 基本数据类型"></a>3.1 基本数据类型</h3><p>Hive 表中的列支持以下基本数据类型：</p><table><thead><tr><th>大类</th><th>类型</th></tr></thead><tbody><tr><td><strong>Integers（整型）</strong></td><td>TINYINT—1 字节的有符号整数  SMALLINT—2 字节的有符号整数  INT—4 字节的有符号整数  BIGINT—8 字节的有符号整数</td></tr><tr><td><strong>Boolean（布尔型）</strong></td><td>BOOLEAN—TRUE/FALSE</td></tr><tr><td><strong>Floating point numbers（浮点型）</strong></td><td>FLOAT— 单精度浮点型  DOUBLE—双精度浮点型</td></tr><tr><td><strong>Fixed point numbers（定点数）</strong></td><td>DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2)</td></tr><tr><td><strong>String types（字符串）</strong></td><td>STRING—指定字符集的字符序列  VARCHAR—具有最大长度限制的字符序列  CHAR—固定长度的字符序列</td></tr><tr><td><strong>Date and time types（日期时间类型）</strong></td><td>TIMESTAMP —  时间戳  TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度  DATE—日期类型</td></tr><tr><td><strong>Binary types（二进制类型）</strong></td><td>BINARY—字节序列</td></tr></tbody></table><blockquote><p>TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下：</p><ul><li><strong>TIMESTAMP WITH LOCAL TIME ZONE</strong>：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。</li><li><strong>TIMESTAMP</strong> ：提交什么时间就保存什么时间，查询时也不做任何转换。</li></ul></blockquote><h3 id="3-2-隐式转换"><a href="#3-2-隐式转换" class="headerlink" title="3.2 隐式转换"></a>3.2 隐式转换</h3><p>Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。</p><p><img src="https://pic.downk.cc/item/5ff3dc473ffa7d37b3928f71.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5ff3dc473ffa7d37b3928f71.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="基本数据类型"></p><h3 id="3-3-复杂类型"><a href="#3-3-复杂类型" class="headerlink" title="3.3 复杂类型"></a>3.3 复杂类型</h3><table><thead><tr><th>类型</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td><strong>STRUCT</strong></td><td>类似于对象，是字段的集合，字段的类型可以不同，可以使用 <code>名称.字段名</code> 方式进行访问</td><td>STRUCT (‘xiaoming’, 12 , ‘2018-12-12’)</td></tr><tr><td><strong>MAP</strong></td><td>键值对的集合，可以使用 <code> 名称[key]</code> 的方式访问对应的值</td><td>map(‘a’, 1, ‘b’, 2)</td></tr><tr><td><strong>ARRAY</strong></td><td>数组是一组具有相同类型和名称的变量的集合，可以使用 <code> 名称[index]</code> 访问对应的值</td><td>ARRAY(‘a’, ‘b’, ‘c’, ‘d’)</td></tr></tbody></table><h3 id="3-4-示例"><a href="#3-4-示例" class="headerlink" title="3.4 示例"></a>3.4 示例</h3><p>如下给出一个基本数据类型和复杂数据类型的使用示例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> students(</span><br><span class="line">  name      STRING,   <span class="comment">-- 姓名</span></span><br><span class="line">  age       <span class="type">INT</span>,      <span class="comment">-- 年龄</span></span><br><span class="line">  subject   <span class="keyword">ARRAY</span><span class="operator">&lt;</span>STRING<span class="operator">&gt;</span>,   <span class="comment">--学科</span></span><br><span class="line">  score     MAP<span class="operator">&lt;</span>STRING,<span class="type">FLOAT</span><span class="operator">&gt;</span>,  <span class="comment">--各个学科考试成绩</span></span><br><span class="line">  address   STRUCT<span class="operator">&lt;</span>houseNumber:<span class="type">int</span>, street:STRING, city:STRING, province：STRING<span class="operator">&gt;</span>  <span class="comment">--家庭居住地址</span></span><br><span class="line">) <span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> &quot;\t&quot;;</span><br></pre></td></tr></table></figure><h2 id="四、内容格式"><a href="#四、内容格式" class="headerlink" title="四、内容格式"></a>四、内容格式</h2><p>当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated  Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。</p><p>所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示。</p><table><thead><tr><th>分隔符</th><th>描述</th></tr></thead><tbody><tr><td><strong>\n</strong></td><td>对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录</td></tr><tr><td><strong>^A (Ctrl+A)</strong></td><td>分割字段 (列)，在 CREATE TABLE 语句中也可以使用八进制编码 <code>\001</code> 来表示</td></tr><tr><td><strong>^B</strong></td><td>用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割， 在 CREATE TABLE 语句中也可以使用八进制编码 <code>\002</code> 表示</td></tr><tr><td><strong>^C</strong></td><td>用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 <code>\003</code> 表示</td></tr></tbody></table><p>使用示例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="type">INT</span>, userid <span class="type">BIGINT</span>)</span><br><span class="line"> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\001&#x27;</span></span><br><span class="line">   COLLECTION ITEMS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\002&#x27;</span></span><br><span class="line">   MAP KEYS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\003&#x27;</span></span><br><span class="line"> STORED <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure><h2 id="五、存储格式"><a href="#五、存储格式" class="headerlink" title="五、存储格式"></a>五、存储格式</h2><h3 id="5-1-支持的存储格式"><a href="#5-1-支持的存储格式" class="headerlink" title="5.1 支持的存储格式"></a>5.1 支持的存储格式</h3><p>Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式：</p><table><thead><tr><th>格式</th><th>说明</th></tr></thead><tbody><tr><td><strong>TextFile</strong></td><td>存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。</td></tr><tr><td><strong>SequenceFile</strong></td><td>SequenceFile 是 Hadoop API  提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的  Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的  SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value  存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。</td></tr><tr><td><strong>RCFile</strong></td><td>RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。</td></tr><tr><td><strong>ORC Files</strong></td><td>ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。</td></tr><tr><td><strong>Avro Files</strong></td><td>Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。</td></tr><tr><td><strong>Parquet</strong></td><td>Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。</td></tr></tbody></table><blockquote><p>以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。</p></blockquote><h3 id="5-2-指定存储格式"><a href="#5-2-指定存储格式" class="headerlink" title="5.2 指定存储格式"></a>5.2 指定存储格式</h3><p>通常在创建表的时候使用 <code>STORED AS</code> 参数指定：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE page_view(viewTime INT, userid BIGINT)</span><br><span class="line"> ROW FORMAT DELIMITED</span><br><span class="line">   FIELDS TERMINATED BY &#x27;\001&#x27;</span><br><span class="line">   COLLECTION ITEMS TERMINATED BY &#x27;\002&#x27;</span><br><span class="line">   MAP KEYS TERMINATED BY &#x27;\003&#x27;</span><br><span class="line"> STORED AS SEQUENCEFILE;</span><br></pre></td></tr></table></figure><p>各个存储文件类型指定方式如下：</p><ul><li>STORED AS TEXTFILE</li><li>STORED AS SEQUENCEFILE</li><li>STORED AS ORC</li><li>STORED AS PARQUET</li><li>STORED AS AVRO</li><li>STORED AS RCFILE</li></ul><h2 id="六、内部表和外部表"><a href="#六、内部表和外部表" class="headerlink" title="六、内部表和外部表"></a>六、内部表和外部表</h2><p>内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下：</p><table><thead><tr><th></th><th>内部表</th><th>外部表</th></tr></thead><tbody><tr><td>数据存储位置</td><td>内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 <code>/user/hive/warehouse/数据库名.db/表名/</code>  目录下</td><td>外部表数据的存储位置创建表时由 <code>Location</code> 参数指定；</td></tr><tr><td>导入数据</td><td>在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理</td><td>外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置</td></tr><tr><td>删除表</td><td>删除元数据（metadata）和文件</td><td>只删除元数据（metadata）</td></tr></tbody></table>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于ZooKeeper搭建Hadoop</title>
      <link href="/2021/10/15/%E5%9F%BA%E4%BA%8EZooKeeper%E6%90%AD%E5%BB%BAHadoop/"/>
      <url>/2021/10/15/%E5%9F%BA%E4%BA%8EZooKeeper%E6%90%AD%E5%BB%BAHadoop/</url>
      
        <content type="html"><![CDATA[<h2 id="一、高可用简介"><a href="#一、高可用简介" class="headerlink" title="一、高可用简介"></a>一、高可用简介</h2><p>Hadoop 高可用 (High Availability) 分为 HDFS 高可用和 YARN 高可用，两者的实现基本类似，但 HDFS NameNode 对数据存储及其一致性的要求比 YARN ResourceManger 高得多，所以它的实现也更加复杂，故下面先进行讲解：</p><h3 id="1-1-高可用整体架构"><a href="#1-1-高可用整体架构" class="headerlink" title="1.1 高可用整体架构"></a>1.1 高可用整体架构</h3><p>HDFS 高可用架构如下：</p><p><img src="https://camo.githubusercontent.com/b0b8e7bfcf3d75fc2048d0fb1671e7c7ffa234f8/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f484446532d48412d4172636869746563747572652d45647572656b612e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/b0b8e7bfcf3d75fc2048d0fb1671e7c7ffa234f8/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f484446532d48412d4172636869746563747572652d45647572656b612e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><blockquote><p><em>图片引用自：<a href="https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/">https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/</a></em></p></blockquote><p>HDFS 高可用架构主要由以下组件所构成：</p><ul><li><strong>Active NameNode 和 Standby NameNode</strong>：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。</li><li><strong>主备切换控制器 ZKFailoverController</strong>：ZKFailoverController  作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode  的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于  Zookeeper 的手动主备切换。</li><li><strong>Zookeeper 集群</strong>：为主备切换控制器提供主备选举支持。</li><li><strong>共享存储系统</strong>：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了  NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和 NameNode  通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</li><li><strong>DataNode 节点</strong>：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备  NameNode 上报数据块的位置信息。</li></ul><h3 id="1-2-基于-QJM-的共享存储系统的数据同步机制分析"><a href="#1-2-基于-QJM-的共享存储系统的数据同步机制分析" class="headerlink" title="1.2 基于 QJM 的共享存储系统的数据同步机制分析"></a>1.2 基于 QJM 的共享存储系统的数据同步机制分析</h3><p>目前 Hadoop 支持使用 Quorum Journal Manager (QJM) 或 Network File System  (NFS) 作为共享的存储系统，这里以 QJM 集群为例进行说明：Active NameNode 首先把 EditLog 提交到  JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog，当  Active NameNode  宕机后， Standby  NameNode 在确认元数据完全同步之后就可以对外提供服务。</p><p>需要说明的是向 JournalNode 集群写入 EditLog 是遵循 “过半写入则成功” 的策略，所以你至少要有 3 个  JournalNode 节点，当然你也可以继续增加节点数量，但是应该保证节点总数是奇数。同时如果有 2N+1 台  JournalNode，那么根据过半写的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。</p><p><img src="https://camo.githubusercontent.com/3f5274ffc21109f2390bdd2e1ec37d3e837b4cc1/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d514a4d2de5908ce6ada5e69cbae588b62e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/3f5274ffc21109f2390bdd2e1ec37d3e837b4cc1/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d514a4d2de5908ce6ada5e69cbae588b62e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><h3 id="1-3-NameNode-主备切换"><a href="#1-3-NameNode-主备切换" class="headerlink" title="1.3 NameNode 主备切换"></a>1.3 NameNode 主备切换</h3><p>NameNode 实现主备切换的流程下图所示：</p><p><img src="https://camo.githubusercontent.com/33bb0d7c919a1ef21c959460dc3e5e116070b5bb/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d6e616d656e6f6465e4b8bbe5a487e58887e68da22e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/33bb0d7c919a1ef21c959460dc3e5e116070b5bb/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d6e616d656e6f6465e4b8bbe5a487e58887e68da22e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><p>\1. HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。 2. HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。 3. 如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。 4. ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。 5. ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。 6. ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。</p><h3 id="1-4-YARN高可用"><a href="#1-4-YARN高可用" class="headerlink" title="1.4 YARN高可用"></a>1.4 YARN高可用</h3><p>YARN ResourceManager 的高可用与 HDFS NameNode 的高可用类似，但是 ResourceManager 不像 NameNode ，没有那么多的元数据信息需要维护，所以它的状态信息可以直接写到 Zookeeper 上，并依赖 Zookeeper  来进行主备选举。</p><p><img src="https://camo.githubusercontent.com/9e7d4c0e0db498ab5f8663d58ad38deeaa330218/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d726d2d68612d6f766572766965772e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/9e7d4c0e0db498ab5f8663d58ad38deeaa330218/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d726d2d68612d6f766572766965772e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><h2 id="二、集群规划"><a href="#二、集群规划" class="headerlink" title="二、集群规划"></a>二、集群规划</h2><p>按照高可用的设计目标：需要保证至少有两个 NameNode (一主一备)  和 两个 ResourceManager (一主一备)   ，同时为满足“过半写入则成功”的原则，需要至少要有 3 个 JournalNode 节点。这里使用三台主机进行搭建，集群规划如下：</p><p><img src="https://camo.githubusercontent.com/9d815ddad1f979ac693a6d96f2e59c0962a72c14/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4e8a784e588922e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/9d815ddad1f979ac693a6d96f2e59c0962a72c14/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4e8a784e588922e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><h2 id="三、前置条件"><a href="#三、前置条件" class="headerlink" title="三、前置条件"></a>三、前置条件</h2><ul><li>所有服务器都安装有 JDK，安装步骤可以参见：<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 下 JDK 的安装</a>；</li><li>搭建好 ZooKeeper 集群，搭建步骤可以参见：<a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Zookeeper%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%92%8C%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA.md">Zookeeper 单机环境和集群环境搭建</a></li><li>所有服务器之间都配置好 SSH 免密登录。</li></ul><h2 id="四、集群配置"><a href="#四、集群配置" class="headerlink" title="四、集群配置"></a>四、集群配置</h2><h3 id="4-1-下载并解压"><a href="#4-1-下载并解压" class="headerlink" title="4.1 下载并解压"></a>4.1 下载并解压</h3><p>下载 Hadoop。这里我下载的是 CDH 版本 Hadoop，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz </span><br></pre></td></tr></table></figure><h3 id="4-2-配置环境变量"><a href="#4-2-配置环境变量" class="headerlink" title="4.2 配置环境变量"></a>4.2 配置环境变量</h3><p>编辑 <code>profile</code> 文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/profile</span><br></pre></td></tr></table></figure><p>增加如下配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br><span class="line">export  PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure><p>执行 <code>source</code> 命令，使得配置立即生效：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="4-3-修改配置"><a href="#4-3-修改配置" class="headerlink" title="4.3 修改配置"></a>4.3 修改配置</h3><p>进入 <code>$&#123;HADOOP_HOME&#125;/etc/hadoop</code> 目录下，修改配置文件。各个配置文件内容如下：</p><h4 id="1-hadoop-env-sh"><a href="#1-hadoop-env-sh" class="headerlink" title="1. hadoop-env.sh"></a>1. hadoop-env.sh</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 指定JDK的安装位置</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201/</span><br></pre></td></tr></table></figure><h4 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2.  core-site.xml"></a>2.  core-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 指定 namenode 的 hdfs 协议文件系统的通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 指定 hadoop 集群存储临时文件的目录 --&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- ZooKeeper 集群的地址 --&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop002:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- ZKFC 连接到 ZooKeeper 超时时长 --&gt;</span><br><span class="line">        &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="3-hdfs-site-xml"><a href="#3-hdfs-site-xml" class="headerlink" title="3. hdfs-site.xml"></a>3. hdfs-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 指定 HDFS 副本的数量 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- namenode 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/namenode/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- datanode 节点数据（即数据块）的存放位置 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/datanode/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 集群服务的逻辑名称 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mycluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- NameNode ID 列表--&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- nn1 的 RPC 通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- nn2 的 RPC 通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop002:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- nn1 的 http 通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- nn2 的 http 通信地址 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop002:50070&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- NameNode 元数据在 JournalNode 上的共享存储目录 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/mycluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- Journal Edit Files 的存储目录 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/journalnode/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 配置隔离机制，确保在任何给定时间只有一个 NameNode 处于活动状态 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 使用 sshfence 机制时需要 ssh 免密登录 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- SSH 超时时间 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;30000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 访问代理类，用于确定当前处于 Active 状态的 NameNode --&gt;</span><br><span class="line">        &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 开启故障自动转移 --&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="4-yarn-site-xml"><a href="#4-yarn-site-xml" class="headerlink" title="4. yarn-site.xml"></a>4. yarn-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 是否启用日志聚合 (可选) --&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 聚合日志的保存时间 (可选) --&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;86400&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 启用 RM HA --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM 集群标识 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;my-yarn-cluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM 的逻辑 ID 列表 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;rm1,rm2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM1 的服务地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop002&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM2 的服务地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop003&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM1 Web 应用程序的地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop002:8088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- RM2 Web 应用程序的地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop003:8088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- ZooKeeper 集群的地址 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 启用自动恢复 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!-- 用于进行持久化存储的类 --&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="5-mapred-site-xml"><a href="#5-mapred-site-xml" class="headerlink" title="5.  mapred-site.xml"></a>5.  mapred-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--指定 mapreduce 作业运行在 yarn 上--&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="5-slaves"><a href="#5-slaves" class="headerlink" title="5. slaves"></a>5. slaves</h4><p>配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 <code>DataNode</code> 服务和 <code>NodeManager</code> 服务都会被启动。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br></pre></td></tr></table></figure><h3 id="4-4-分发程序"><a href="#4-4-分发程序" class="headerlink" title="4.4 分发程序"></a>4.4 分发程序</h3><p>将 Hadoop 安装包分发到其他两台服务器，分发后建议在这两台服务器上也配置一下 Hadoop 的环境变量。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 将安装包分发到hadoop002</span><br><span class="line">scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/  hadoop002:/usr/app/</span><br><span class="line"># 将安装包分发到hadoop003</span><br><span class="line">scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/  hadoop003:/usr/app/</span><br></pre></td></tr></table></figure><h2 id="五、启动集群"><a href="#五、启动集群" class="headerlink" title="五、启动集群"></a>五、启动集群</h2><h3 id="5-1-启动ZooKeeper"><a href="#5-1-启动ZooKeeper" class="headerlink" title="5.1 启动ZooKeeper"></a>5.1 启动ZooKeeper</h3><p>分别到三台服务器上启动 ZooKeeper 服务：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="5-2-启动Journalnode"><a href="#5-2-启动Journalnode" class="headerlink" title="5.2 启动Journalnode"></a>5.2 启动Journalnode</h3><p>分别到三台服务器的的 <code>$&#123;HADOOP_HOME&#125;/sbin</code> 目录下，启动 <code>journalnode</code> 进程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><h3 id="5-3-初始化NameNode"><a href="#5-3-初始化NameNode" class="headerlink" title="5.3 初始化NameNode"></a>5.3 初始化NameNode</h3><p>在 <code>hadop001</code> 上执行 <code>NameNode</code> 初始化命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><p>执行初始化命令后，需要将 <code>NameNode</code> 元数据目录的内容，复制到其他未格式化的 <code>NameNode</code> 上。元数据存储目录就是我们在 <code>hdfs-site.xml</code> 中使用 <code>dfs.namenode.name.dir</code> 属性指定的目录。这里我们需要将其复制到 <code>hadoop002</code> 上：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /home/hadoop/namenode/data hadoop002:/home/hadoop/namenode/</span><br></pre></td></tr></table></figure><h3 id="5-4-初始化HA状态"><a href="#5-4-初始化HA状态" class="headerlink" title="5.4 初始化HA状态"></a>5.4 初始化HA状态</h3><p>在任意一台 <code>NameNode</code> 上使用以下命令来初始化 ZooKeeper 中的 HA 状态：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc -formatZK</span><br></pre></td></tr></table></figure><h3 id="5-5-启动HDFS"><a href="#5-5-启动HDFS" class="headerlink" title="5.5 启动HDFS"></a>5.5 启动HDFS</h3><p>进入到 <code>hadoop001</code> 的 <code>$&#123;HADOOP_HOME&#125;/sbin</code> 目录下，启动 HDFS。此时 <code>hadoop001</code> 和 <code>hadoop002</code> 上的 <code>NameNode</code> 服务，和三台服务器上的 <code>DataNode</code> 服务都会被启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="5-6-启动YARN"><a href="#5-6-启动YARN" class="headerlink" title="5.6 启动YARN"></a>5.6 启动YARN</h3><p>进入到 <code>hadoop002</code> 的 <code>$&#123;HADOOP_HOME&#125;/sbin</code> 目录下，启动 YARN。此时 <code>hadoop002</code> 上的 <code>ResourceManager</code> 服务，和三台服务器上的 <code>NodeManager</code> 服务都会被启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>需要注意的是，这个时候 <code>hadoop003</code> 上的 <code>ResourceManager</code> 服务通常是没有启动的，需要手动启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure><h2 id="六、查看集群"><a href="#六、查看集群" class="headerlink" title="六、查看集群"></a>六、查看集群</h2><h3 id="6-1-查看进程"><a href="#6-1-查看进程" class="headerlink" title="6.1 查看进程"></a>6.1 查看进程</h3><p>成功启动后，每台服务器上的进程应该如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 sbin]# jps</span><br><span class="line">4512 DFSZKFailoverController</span><br><span class="line">3714 JournalNode</span><br><span class="line">4114 NameNode</span><br><span class="line">3668 QuorumPeerMain</span><br><span class="line">5012 DataNode</span><br><span class="line">4639 NodeManager</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop002 sbin]# jps</span><br><span class="line">4499 ResourceManager</span><br><span class="line">4595 NodeManager</span><br><span class="line">3465 QuorumPeerMain</span><br><span class="line">3705 NameNode</span><br><span class="line">3915 DFSZKFailoverController</span><br><span class="line">5211 DataNode</span><br><span class="line">3533 JournalNode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@hadoop003 sbin]# jps</span><br><span class="line">3491 JournalNode</span><br><span class="line">3942 NodeManager</span><br><span class="line">4102 ResourceManager</span><br><span class="line">4201 DataNode</span><br><span class="line">3435 QuorumPeerMain</span><br></pre></td></tr></table></figure><h3 id="6-2-查看Web-UI"><a href="#6-2-查看Web-UI" class="headerlink" title="6.2 查看Web UI"></a>6.2 查看Web UI</h3><p>HDFS 和 YARN 的端口号分别为 <code>50070</code> 和 <code>8080</code>，界面应该如下：</p><p>此时 hadoop001 上的 <code>NameNode</code> 处于可用状态：</p><p><img src="https://camo.githubusercontent.com/0dfee81cee0e0ea059224ad0a0df2b04b91339ef/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4312e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/0dfee81cee0e0ea059224ad0a0df2b04b91339ef/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4312e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><p>而 hadoop002 上的 <code>NameNode</code> 则处于备用状态：</p><p><img src="https://camo.githubusercontent.com/f0adb502fb1dc9f1b4d524b7ee954bb3e9ca04a2/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4332e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/f0adb502fb1dc9f1b4d524b7ee954bb3e9ca04a2/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4332e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><p>hadoop002 上的 <code>ResourceManager</code> 处于可用状态：</p><p><img src="https://camo.githubusercontent.com/afbb05472c97edf7c1f06b33c65f97a80171a1de/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4342e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/afbb05472c97edf7c1f06b33c65f97a80171a1de/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4342e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><p>hadoop003 上的 <code>ResourceManager</code> 则处于备用状态：</p><p><img src="https://camo.githubusercontent.com/dbed5c91d8834f4203f1ea9206767d3efc3b1491/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4352e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/dbed5c91d8834f4203f1ea9206767d3efc3b1491/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4352e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><p>同时界面上也有 <code>Journal Manager</code> 的相关信息：</p><p><img src="https://camo.githubusercontent.com/00e35f7ac2eb2e8638bc6013be956ce728df2283/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4322e706e67" class="lazyload placeholder" data-srcset="https://camo.githubusercontent.com/00e35f7ac2eb2e8638bc6013be956ce728df2283/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f70e9ab98e58fafe794a8e99b86e7bea4322e706e67" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" alt="img"></p><p>## 七、集群的二次启动</p><p>上面的集群初次启动涉及到一些必要初始化操作，所以过程略显繁琐。但是集群一旦搭建好后，想要再次启用它是比较方便的，步骤如下（首选需要确保 ZooKeeper 集群已经启动）：</p><p>在 <code> hadoop001</code> 启动 HDFS，此时会启动所有与 HDFS 高可用相关的服务，包括 NameNode，DataNode 和 JournalNode：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure><p>在 <code>hadoop002</code> 启动 YARN：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>这个时候 <code>hadoop003</code> 上的 <code>ResourceManager</code> 服务通常还是没有启动的，需要手动启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn-daemon.sh start resourcemanager</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS JAVA API</title>
      <link href="/2021/10/15/HDFS%20API/"/>
      <url>/2021/10/15/HDFS%20API/</url>
      
        <content type="html"><![CDATA[<h2 id="一、-简介"><a href="#一、-简介" class="headerlink" title="一、 简介"></a>一、 简介</h2><p>想要使用 HDFS API，需要导入依赖 <code>hadoop-client</code>。如果是 CDH 版本的 Hadoop，还需要额外指明其仓库地址：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="line">&lt;project xmlns=<span class="string">&quot;http://maven.apache.org/POM/4.0.0&quot;</span></span><br><span class="line">         xmlns:xsi=<span class="string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class="line">         xsi:schemaLocation=<span class="string">&quot;http://maven.apache.org/POM/4.0.0 </span></span><br><span class="line"><span class="string">         http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</span>&gt;</span><br><span class="line">    &lt;modelVersion&gt;<span class="number">4.0</span><span class="number">.0</span>&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">    &lt;groupId&gt;com.ihadu&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;hdfs-java-api&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">1.0</span>&lt;/version&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;properties&gt;</span><br><span class="line">        &lt;project.build.sourceEncoding&gt;UTF-<span class="number">8</span>&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">        &lt;hadoop.version&gt;<span class="number">2.6</span><span class="number">.0</span>-cdh5<span class="number">.15</span><span class="number">.2</span>&lt;/hadoop.version&gt;</span><br><span class="line">    &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;!---配置 CDH 仓库地址--&gt;</span><br><span class="line">    &lt;repositories&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">            &lt;url&gt;https:<span class="comment">//repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;</span></span><br><span class="line">        &lt;/repository&gt;</span><br><span class="line">    &lt;/repositories&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &lt;dependencies&gt;</span><br><span class="line">        &lt;!--Hadoop-client--&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">            &lt;version&gt;<span class="number">4.12</span>&lt;/version&gt;</span><br><span class="line">            &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">    &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure><h2 id="二、API的使用"><a href="#二、API的使用" class="headerlink" title="二、API的使用"></a>二、API的使用</h2><h3 id="2-1-FileSystem"><a href="#2-1-FileSystem" class="headerlink" title="2.1 FileSystem"></a>2.1 FileSystem</h3><p>FileSystem 是所有 HDFS 操作的主入口。由于之后的每个单元测试都需要用到它，这里使用 <code>@Before</code> 注解进行标注。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_PATH = <span class="string">&quot;hdfs://192.168.0.106:8020&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_USER = <span class="string">&quot;root&quot;</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 这里我启动的是单节点的 Hadoop,所以副本系数设置为 1,默认值为 3</span></span><br><span class="line">        configuration.set(<span class="string">&quot;dfs.replication&quot;</span>, <span class="string">&quot;1&quot;</span>);</span><br><span class="line">        fileSystem = FileSystem.get(<span class="keyword">new</span> URI(HDFS_PATH), configuration, HDFS_USER);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (URISyntaxException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@After</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">destroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    fileSystem = <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-2-创建目录"><a href="#2-2-创建目录" class="headerlink" title="2.2 创建目录"></a>2.2 创建目录</h3><p>支持递归创建目录：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkDir</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test0/&quot;</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-创建指定权限的目录"><a href="#2-3-创建指定权限的目录" class="headerlink" title="2.3 创建指定权限的目录"></a>2.3 创建指定权限的目录</h3><p><code>FsPermission(FsAction u, FsAction g, FsAction o)</code> 的三个参数分别对应：创建者权限，同组其他用户权限，其他用户权限，权限值定义在 <code>FsAction</code> 枚举类中。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">mkDirWithPermission</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test1/&quot;</span>),</span><br><span class="line">            <span class="keyword">new</span> FsPermission(FsAction.READ_WRITE, FsAction.READ, FsAction.READ));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-4-创建文件，并写入内容"><a href="#2-4-创建文件，并写入内容" class="headerlink" title="2.4 创建文件，并写入内容"></a>2.4 创建文件，并写入内容</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">create</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 如果文件存在，默认会覆盖, 可以通过第二个参数进行控制。第三个参数可以控制使用缓冲区的大小</span></span><br><span class="line">    FSDataOutputStream out = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>),</span><br><span class="line">                                               <span class="keyword">true</span>, <span class="number">4096</span>);</span><br><span class="line">    out.write(<span class="string">&quot;hello hadoop!&quot;</span>.getBytes());</span><br><span class="line">    out.write(<span class="string">&quot;hello spark!&quot;</span>.getBytes());</span><br><span class="line">    out.write(<span class="string">&quot;hello flink!&quot;</span>.getBytes());</span><br><span class="line">    <span class="comment">// 强制将缓冲区中内容刷出</span></span><br><span class="line">    out.flush();</span><br><span class="line">    out.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-5-判断文件是否存在"><a href="#2-5-判断文件是否存在" class="headerlink" title="2.5 判断文件是否存在"></a>2.5 判断文件是否存在</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">exist</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> exists = fileSystem.exists(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>));</span><br><span class="line">    System.out.println(exists);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-6-查看文件内容"><a href="#2-6-查看文件内容" class="headerlink" title="2.6 查看文件内容"></a>2.6 查看文件内容</h3><p>查看小文本文件的内容，直接转换成字符串后输出：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readToString</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>));</span><br><span class="line">    String context = inputStreamToString(inputStream, <span class="string">&quot;utf-8&quot;</span>);</span><br><span class="line">    System.out.println(context);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>inputStreamToString</code> 是一个自定义方法，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 把输入流转换为指定编码的字符</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> inputStream 输入流</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> encode      指定编码类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> String <span class="title">inputStreamToString</span><span class="params">(InputStream inputStream, String encode)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (encode == <span class="keyword">null</span> || (<span class="string">&quot;&quot;</span>.equals(encode))) &#123;</span><br><span class="line">            encode = <span class="string">&quot;utf-8&quot;</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(inputStream, encode));</span><br><span class="line">        StringBuilder builder = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        String str = <span class="string">&quot;&quot;</span>;</span><br><span class="line">        <span class="keyword">while</span> ((str = reader.readLine()) != <span class="keyword">null</span>) &#123;</span><br><span class="line">            builder.append(str).append(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> builder.toString();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-7-文件重命名"><a href="#2-7-文件重命名" class="headerlink" title="2.7 文件重命名"></a>2.7 文件重命名</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">rename</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Path oldPath = <span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/a.txt&quot;</span>);</span><br><span class="line">    Path newPath = <span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/b.txt&quot;</span>);</span><br><span class="line">    <span class="keyword">boolean</span> result = fileSystem.rename(oldPath, newPath);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-8-删除目录或文件"><a href="#2-8-删除目录或文件" class="headerlink" title="2.8 删除目录或文件"></a>2.8 删除目录或文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">delete</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     *  第二个参数代表是否递归删除</span></span><br><span class="line"><span class="comment">     *    +  如果 path 是一个目录且递归删除为 true, 则删除该目录及其中所有文件;</span></span><br><span class="line"><span class="comment">     *    +  如果 path 是一个目录但递归删除为 false,则会则抛出异常。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">boolean</span> result = fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/b.txt&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">    System.out.println(result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-9-上传文件到HDFS"><a href="#2-9-上传文件到HDFS" class="headerlink" title="2.9 上传文件到HDFS"></a>2.9 上传文件到HDFS</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    <span class="comment">// 如果指定的是目录，则会把目录及其中的文件都复制到指定目录下</span></span><br><span class="line">    Path src = <span class="keyword">new</span> Path(<span class="string">&quot;D:\\BigData-Notes\\notes\\installation&quot;</span>);</span><br><span class="line">    Path dst = <span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/&quot;</span>);</span><br><span class="line">    fileSystem.copyFromLocalFile(src, dst);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-10-上传大文件并显示上传进度"><a href="#2-10-上传大文件并显示上传进度" class="headerlink" title="2.10 上传大文件并显示上传进度"></a>2.10 上传大文件并显示上传进度</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyFromLocalBigFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        File file = <span class="keyword">new</span> File(<span class="string">&quot;D:\\kafka.tgz&quot;</span>);</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">float</span> fileSize = file.length();</span><br><span class="line">        InputStream in = <span class="keyword">new</span> BufferedInputStream(<span class="keyword">new</span> FileInputStream(file));</span><br><span class="line"></span><br><span class="line">        FSDataOutputStream out = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/kafka5.tgz&quot;</span>),</span><br><span class="line">                <span class="keyword">new</span> Progressable() &#123;</span><br><span class="line">                  <span class="keyword">long</span> fileCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">                  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">progress</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                     fileCount++;</span><br><span class="line">                     <span class="comment">// progress 方法每上传大约 64KB 的数据后就会被调用一次</span></span><br><span class="line">                     System.out.println(<span class="string">&quot;上传进度：&quot;</span> + (fileCount * <span class="number">64</span> * <span class="number">1024</span> / fileSize) * <span class="number">100</span> + <span class="string">&quot; %&quot;</span>);</span><br><span class="line">                   &#125;</span><br><span class="line">                &#125;);</span><br><span class="line"></span><br><span class="line">        IOUtils.copyBytes(in, out, <span class="number">4096</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="2-11-从HDFS上下载文件"><a href="#2-11-从HDFS上下载文件" class="headerlink" title="2.11 从HDFS上下载文件"></a>2.11 从HDFS上下载文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">copyToLocalFile</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    Path src = <span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/kafka.tgz&quot;</span>);</span><br><span class="line">    Path dst = <span class="keyword">new</span> Path(<span class="string">&quot;D:\\app\\&quot;</span>);</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 第一个参数控制下载完成后是否删除源文件,默认是 true,即删除;</span></span><br><span class="line"><span class="comment">     * 最后一个参数表示是否将 RawLocalFileSystem 用作本地文件系统;</span></span><br><span class="line"><span class="comment">     * RawLocalFileSystem 默认为 false,通常情况下可以不设置,</span></span><br><span class="line"><span class="comment">     * 但如果你在执行时候抛出 NullPointerException 异常,则代表你的文件系统与程序可能存在不兼容的情况 (window 下常见),</span></span><br><span class="line"><span class="comment">     * 此时可以将 RawLocalFileSystem 设置为 true</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    fileSystem.copyToLocalFile(<span class="keyword">false</span>, src, dst, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-12-查看指定目录下所有文件的信息"><a href="#2-12-查看指定目录下所有文件的信息" class="headerlink" title="2.12 查看指定目录下所有文件的信息"></a>2.12 查看指定目录下所有文件的信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFiles</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    FileStatus[] statuses = fileSystem.listStatus(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api&quot;</span>));</span><br><span class="line">    <span class="keyword">for</span> (FileStatus fileStatus : statuses) &#123;</span><br><span class="line">        <span class="comment">//fileStatus 的 toString 方法被重写过，直接打印可以看到所有信息</span></span><br><span class="line">        System.out.println(fileStatus.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>FileStatus</code> 中包含了文件的基本信息，比如文件路径，是否是文件夹，修改时间，访问时间，所有者，所属组，文件权限，是否是符号链接等，输出内容示例如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">FileStatus&#123;</span><br><span class="line">path=hdfs://192.168.0.106:8020/hdfs-api/test; </span><br><span class="line">isDirectory=true; </span><br><span class="line">modification_time=1556680796191; </span><br><span class="line">access_time=0; </span><br><span class="line">owner=root; </span><br><span class="line">group=supergroup; </span><br><span class="line">permission=rwxr-xr-x; </span><br><span class="line">isSymlink=false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-13-递归查看指定目录下所有文件的信息"><a href="#2-13-递归查看指定目录下所有文件的信息" class="headerlink" title="2.13 递归查看指定目录下所有文件的信息"></a>2.13 递归查看指定目录下所有文件的信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listFilesRecursive</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">    RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">&quot;/hbase&quot;</span>), <span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">while</span> (files.hasNext()) &#123;</span><br><span class="line">        System.out.println(files.next());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和上面输出类似，只是多了文本大小，副本系数，块大小信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">LocatedFileStatus&#123;</span><br><span class="line">path=hdfs://192.168.0.106:8020/hbase/hbase.version; </span><br><span class="line">isDirectory=false; </span><br><span class="line">length=7; </span><br><span class="line">replication=1; </span><br><span class="line">blocksize=134217728; </span><br><span class="line">modification_time=1554129052916; </span><br><span class="line">access_time=1554902661455; </span><br><span class="line">owner=root; </span><br><span class="line">group=supergroup;</span><br><span class="line">permission=rw-r--r--; </span><br><span class="line">isSymlink=false&#125;</span><br></pre></td></tr></table></figure><h3 id="2-14-查看文件的块信息"><a href="#2-14-查看文件的块信息" class="headerlink" title="2.14 查看文件的块信息"></a>2.14 查看文件的块信息</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getFileBlockLocations</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">    FileStatus fileStatus = fileSystem.getFileStatus(<span class="keyword">new</span> Path(<span class="string">&quot;/hdfs-api/test/kafka.tgz&quot;</span>));</span><br><span class="line">    BlockLocation[] blocks = fileSystem.getFileBlockLocations(fileStatus, <span class="number">0</span>, fileStatus.getLen());</span><br><span class="line">    <span class="keyword">for</span> (BlockLocation block : blocks) &#123;</span><br><span class="line">        System.out.println(block);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>块输出信息有三个值，分别是文件的起始偏移量 (offset)，文件大小 (length)，块所在的主机名 (hosts)。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0,57028557,hadoop001</span><br></pre></td></tr></table></figure><p>这里我上传的文件只有 57M(小于 128M)，且程序中设置了副本系数为 1，所有只有一个块信息。</p>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS常用shell命令</title>
      <link href="/2021/10/15/HDFS%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/"/>
      <url>/2021/10/15/HDFS%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p><strong>1. 显示当前目录结构</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归显示当前目录结构</span></span><br><span class="line">hadoop fs -ls  -R  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 显示根目录下内容</span></span><br><span class="line">hadoop fs -ls  /</span><br></pre></td></tr></table></figure><p><strong>2. 创建目录</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建目录</span></span><br><span class="line">hadoop fs -mkdir  &lt;path&gt; </span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归创建目录</span></span><br><span class="line">hadoop fs -mkdir -p  &lt;path&gt;  </span><br></pre></td></tr></table></figure><p><strong>3. 删除操作</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 删除文件</span></span><br><span class="line">hadoop fs -rm  &lt;path&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 递归删除目录和文件</span></span><br><span class="line">hadoop fs -rm -R  &lt;path&gt; </span><br></pre></td></tr></table></figure><p><strong>4. 从本地加载文件到 HDFS</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -put  [localsrc] [dst] </span><br><span class="line">hadoop fs - copyFromLocal [localsrc] [dst] </span><br></pre></td></tr></table></figure><p><strong>5. 从 HDFS 导出文件到本地</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -get  [dst] [localsrc] </span><br><span class="line">hadoop fs -copyToLocal [dst] [localsrc] </span><br></pre></td></tr></table></figure><p><strong>6. 查看文件内容</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 二选一执行即可</span></span><br><span class="line">hadoop fs -text  &lt;path&gt; </span><br><span class="line">hadoop fs -cat  &lt;path&gt;  </span><br></pre></td></tr></table></figure><p><strong>7. 显示文件的最后一千字节</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -tail  &lt;path&gt; </span><br><span class="line"><span class="meta">#</span><span class="bash"> 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节</span></span><br><span class="line">hadoop fs -tail -f  &lt;path&gt; </span><br></pre></td></tr></table></figure><p><strong>8. 拷贝文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp [src] [dst]</span><br></pre></td></tr></table></figure><p><strong>9. 移动文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv [src] [dst] </span><br></pre></td></tr></table></figure><p><strong>10. 统计当前目录下各文件大小</strong></p><ul><li>默认单位字节</li><li>-s : 显示所有文件大小总和，</li><li>-h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -du  &lt;path&gt;  </span><br></pre></td></tr></table></figure><p><strong>11. 合并下载多个文件</strong></p><ul><li>-nl  在每个文件的末尾添加换行符（LF）</li><li>-skip-empty-file 跳过空文件</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -getmerge</span><br><span class="line"><span class="meta">#</span><span class="bash"> 示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xml</span></span><br><span class="line">hadoop fs -getmerge -nl  /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml</span><br></pre></td></tr></table></figure><p><strong>12. 统计文件系统的可用空间信息</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -df -h /</span><br></pre></td></tr></table></figure><p><strong>13. 更改文件复制因子</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt;</span><br></pre></td></tr></table></figure><ul><li>更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子</li><li>-w : 请求命令是否等待复制完成</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 示例</span></span><br><span class="line">hadoop fs -setrep -w 3 /user/hadoop/dir1</span><br></pre></td></tr></table></figure><p><strong>14. 权限控制</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 权限控制和Linux上使用方式一致</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chgrp [-R] GROUP URI [URI ...]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改文件或目录的访问权限  用户必须是文件的所有者或超级用户。</span></span><br><span class="line">hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改文件的拥有者  用户必须是超级用户。</span></span><br><span class="line">hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]</span><br></pre></td></tr></table></figure><p><strong>15. 文件检测</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -test - [defsz]  URI</span><br></pre></td></tr></table></figure><p>可选选项：</p><ul><li>-d：如果路径是目录，返回 0。</li><li>-e：如果路径存在，则返回 0。</li><li>-f：如果路径是文件，则返回 0。</li><li>-s：如果路径不为空，则返回 0。</li><li>-r：如果路径存在且授予读权限，则返回 0。</li><li>-w：如果路径存在且授予写入权限，则返回 0。</li><li>-z：如果文件长度为零，则返回 0。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 示例</span></span><br><span class="line">hadoop fs -test -e filename</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群环境搭建</title>
      <link href="/2021/10/15/Hadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/10/15/Hadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="一、集群规划"><a href="#一、集群规划" class="headerlink" title="一、集群规划"></a>一、集群规划</h2><p>这里搭建一个 3 节点的 Hadoop 集群，其中三台主机均部署 <code>DataNode</code> 和 <code>NodeManager</code> 服务，但只有 hadoop001 上部署 <code>NameNode</code> 和 <code>ResourceManager</code> 服务。</p><p><img src="https://pic.downk.cc/item/5f7d360d1cd1bbb86b5169a4.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d360d1cd1bbb86b5169a4.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="二、前置条件"><a href="#二、前置条件" class="headerlink" title="二、前置条件"></a>二、前置条件</h2><p>Hadoop 的运行依赖 JDK，需要预先安装。其安装步骤单独整理至：</p><ul><li><a href="https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md">Linux 下 JDK 的安装</a></li></ul><h2 id="三、配置免密登录"><a href="#三、配置免密登录" class="headerlink" title="三、配置免密登录"></a>三、配置免密登录</h2><h3 id="3-1-生成密匙"><a href="#3-1-生成密匙" class="headerlink" title="3.1 生成密匙"></a>3.1 生成密匙</h3><p>在每台主机上使用 <code>ssh-keygen</code> 命令生成公钥私钥对：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><h3 id="3-2-免密登录"><a href="#3-2-免密登录" class="headerlink" title="3.2 免密登录"></a>3.2 免密登录</h3><p>将 <code>hadoop001</code> 的公钥写到本机和远程机器的 <code> ~/ .ssh/authorized_key</code> 文件中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop003</span><br></pre></td></tr></table></figure><h3 id="3-3-验证免密登录"><a href="#3-3-验证免密登录" class="headerlink" title="3.3 验证免密登录"></a>3.3 验证免密登录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh hadoop002</span><br><span class="line">ssh hadoop003</span><br></pre></td></tr></table></figure><h2 id="四、集群搭建"><a href="#四、集群搭建" class="headerlink" title="四、集群搭建"></a>四、集群搭建</h2><h3 id="3-1-下载并解压"><a href="#3-1-下载并解压" class="headerlink" title="3.1 下载并解压"></a>3.1 下载并解压</h3><p>下载 Hadoop。这里我下载的是 CDH 版本 Hadoop，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz </span><br></pre></td></tr></table></figure><h3 id="3-2-配置环境变量"><a href="#3-2-配置环境变量" class="headerlink" title="3.2 配置环境变量"></a>3.2 配置环境变量</h3><p>编辑 <code>profile</code> 文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/profile</span><br></pre></td></tr></table></figure><p>增加如下配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br><span class="line">export  PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure><p>执行 <code>source</code> 命令，使得配置立即生效：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="3-3-修改配置"><a href="#3-3-修改配置" class="headerlink" title="3.3 修改配置"></a>3.3 修改配置</h3><p>进入 <code>$&#123;HADOOP_HOME&#125;/etc/hadoop</code> 目录下，修改配置文件。各个配置文件内容如下：</p><h4 id="1-hadoop-env-sh"><a href="#1-hadoop-env-sh" class="headerlink" title="1. hadoop-env.sh"></a>1. hadoop-env.sh</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 指定JDK的安装位置</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_201/</span><br></pre></td></tr></table></figure><h4 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2.  core-site.xml"></a>2.  core-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--指定 hadoop 集群存储临时文件的目录--&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="3-hdfs-site-xml"><a href="#3-hdfs-site-xml" class="headerlink" title="3. hdfs-site.xml"></a>3. hdfs-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;!--namenode 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔--&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/namenode/data&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;!--datanode 节点数据（即数据块）的存放位置--&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/datanode/data&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="4-yarn-site-xml"><a href="#4-yarn-site-xml" class="headerlink" title="4. yarn-site.xml"></a>4. yarn-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--resourcemanager 的主机名--&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hadoop001&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="5-mapred-site-xml"><a href="#5-mapred-site-xml" class="headerlink" title="5.  mapred-site.xml"></a>5.  mapred-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--指定 mapreduce 作业运行在 yarn 上--&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="5-slaves"><a href="#5-slaves" class="headerlink" title="5. slaves"></a>5. slaves</h4><p>配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 <code>DataNode</code> 服务和 <code>NodeManager</code> 服务都会被启动。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop001</span><br><span class="line">hadoop002</span><br><span class="line">hadoop003</span><br></pre></td></tr></table></figure><h3 id="3-4-分发程序"><a href="#3-4-分发程序" class="headerlink" title="3.4 分发程序"></a>3.4 分发程序</h3><p>将 Hadoop 安装包分发到其他两台服务器，分发后建议在这两台服务器上也配置一下 Hadoop 的环境变量。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 将安装包分发到hadoop002</span><br><span class="line">scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/  hadoop002:/usr/app/</span><br><span class="line"># 将安装包分发到hadoop003</span><br><span class="line">scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/  hadoop003:/usr/app/</span><br></pre></td></tr></table></figure><h3 id="3-5-初始化"><a href="#3-5-初始化" class="headerlink" title="3.5  初始化"></a>3.5  初始化</h3><p>在 <code>Hadoop001</code> 上执行 namenode 初始化命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><h3 id="3-6-启动集群"><a href="#3-6-启动集群" class="headerlink" title="3.6 启动集群"></a>3.6 启动集群</h3><p>进入到 <code>Hadoop001</code> 的 <code>$&#123;HADOOP_HOME&#125;/sbin</code> 目录下，启动 Hadoop。此时 <code>hadoop002</code> 和 <code>hadoop003</code> 上的相关服务也会被启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 启动dfs服务</span><br><span class="line">start-dfs.sh</span><br><span class="line"># 启动yarn服务</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><h3 id="3-7-查看集群"><a href="#3-7-查看集群" class="headerlink" title="3.7 查看集群"></a>3.7 查看集群</h3><p>在每台服务器上使用 <code>jps</code> 命令查看服务进程，或直接进入 Web-UI 界面进行查看，端口为 <code>50070</code>。可以看到此时有三个可用的 <code>Datanode</code>：</p><p><img src="https://pic.downk.cc/item/5f7d360d1cd1bbb86b5169b8.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d360d1cd1bbb86b5169b8.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><p>点击 <code>Live Nodes</code> 进入，可以看到每个 <code>DataNode</code> 的详细情况：</p><p><img src="https://pic.downk.cc/item/5f7d360d1cd1bbb86b5169a8.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d360d1cd1bbb86b5169a8.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><p>接着可以查看 Yarn 的情况，端口号为 <code>8088</code> ：</p><p><img src="https://pic.downk.cc/item/5f7d360d1cd1bbb86b5169af.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d360d1cd1bbb86b5169af.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="五、提交服务到集群"><a href="#五、提交服务到集群" class="headerlink" title="五、提交服务到集群"></a>五、提交服务到集群</h2><p>提交作业到集群的方式和单机环境完全一致，这里以提交 Hadoop 内置的计算 Pi 的示例程序为例，在任何一个节点上执行都可以，命令如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /usr/app/hadoop-2.6.0-cdh5.15.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar  pi  3  3</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop单机版环境搭建</title>
      <link href="/2021/10/15/Hadoop%E5%8D%95%E6%9C%BA%E7%89%88%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2021/10/15/Hadoop%E5%8D%95%E6%9C%BA%E7%89%88%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="一、前置条件"><a href="#一、前置条件" class="headerlink" title="一、前置条件"></a>一、前置条件</h2><p>Hadoop 的运行依赖 JDK，需要预先安装。</p><h2 id="二、配置免密登录"><a href="#二、配置免密登录" class="headerlink" title="二、配置免密登录"></a>二、配置免密登录</h2><p>Hadoop 组件之间需要基于 SSH 进行通讯。</p><h4 id="2-1-配置映射"><a href="#2-1-配置映射" class="headerlink" title="2.1 配置映射"></a>2.1 配置映射</h4><p>配置 ip 地址和主机名映射：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"># 文件末尾增加</span><br><span class="line">192.168.43.202  hadoop001</span><br></pre></td></tr></table></figure><h3 id="2-2-生成公私钥"><a href="#2-2-生成公私钥" class="headerlink" title="2.2  生成公私钥"></a>2.2  生成公私钥</h3><p>执行下面命令行生成公匙和私匙：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><h3 id="3-3-授权"><a href="#3-3-授权" class="headerlink" title="3.3 授权"></a>3.3 授权</h3><p>进入 <code>~/.ssh</code> 目录下，查看生成的公匙和私匙，并将公匙写入到授权文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@@hadoop001 sbin]#  cd ~/.ssh</span><br><span class="line">[root@@hadoop001 .ssh]# ll</span><br><span class="line">-rw-------. 1 root root 1675 3 月  15 09:48 id_rsa</span><br><span class="line">-rw-r--r--. 1 root root  388 3 月  15 09:48 id_rsa.pub</span><br><span class="line"># 写入公匙到授权文件</span><br><span class="line">[root@hadoop001 .ssh]# cat id_rsa.pub &gt;&gt; authorized_keys</span><br><span class="line">[root@hadoop001 .ssh]# chmod 600 authorized_keys</span><br></pre></td></tr></table></figure><h2 id="三、Hadoop-HDFS-环境搭建"><a href="#三、Hadoop-HDFS-环境搭建" class="headerlink" title="三、Hadoop(HDFS)环境搭建"></a>三、Hadoop(HDFS)环境搭建</h2><h3 id="3-1-下载并解压"><a href="#3-1-下载并解压" class="headerlink" title="3.1 下载并解压"></a>3.1 下载并解压</h3><p>下载 Hadoop 安装包，这里我下载的是 CDH 版本的，下载地址为：<a href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 解压</span><br><span class="line">tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz </span><br></pre></td></tr></table></figure><h3 id="3-2-配置环境变量"><a href="#3-2-配置环境变量" class="headerlink" title="3.2 配置环境变量"></a>3.2 配置环境变量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vi /etc/profile</span><br></pre></td></tr></table></figure><p>配置环境变量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2</span><br><span class="line">export  PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure><p>执行 <code>source</code> 命令，使得配置的环境变量立即生效：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="3-3-修改Hadoop配置"><a href="#3-3-修改Hadoop配置" class="headerlink" title="3.3 修改Hadoop配置"></a>3.3 修改Hadoop配置</h3><p>进入 <code>$&#123;HADOOP_HOME&#125;/etc/hadoop/ </code> 目录下，修改以下配置：</p><h4 id="1-hadoop-env-sh"><a href="#1-hadoop-env-sh" class="headerlink" title="1. hadoop-env.sh"></a>1. hadoop-env.sh</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># JDK安装路径</span><br><span class="line">export  JAVA_HOME=/usr/java/jdk1.8.0_201/</span><br></pre></td></tr></table></figure><h4 id="2-core-site-xml"><a href="#2-core-site-xml" class="headerlink" title="2. core-site.xml"></a>2. core-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--指定 hadoop 存储临时文件的目录--&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/home/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="3-hdfs-site-xml"><a href="#3-hdfs-site-xml" class="headerlink" title="3. hdfs-site.xml"></a>3. hdfs-site.xml</h4><p>指定副本系数和临时文件存储位置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--由于我们这里搭建是单机版本，所以指定 dfs 的副本系数为 1--&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="4-slaves"><a href="#4-slaves" class="headerlink" title="4. slaves"></a>4. slaves</h4><p>配置所有从属节点的主机名或 IP 地址，由于是单机版本，所以指定本机即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop001</span><br></pre></td></tr></table></figure><h3 id="3-4-关闭防火墙"><a href="#3-4-关闭防火墙" class="headerlink" title="3.4 关闭防火墙"></a>3.4 关闭防火墙</h3><p>不关闭防火墙可能导致无法访问 Hadoop 的 Web UI 界面：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 查看防火墙状态</span><br><span class="line">sudo firewall-cmd --state</span><br><span class="line"># 关闭防火墙:</span><br><span class="line">sudo systemctl stop firewalld.service</span><br></pre></td></tr></table></figure><h3 id="3-5-初始化"><a href="#3-5-初始化" class="headerlink" title="3.5 初始化"></a>3.5 初始化</h3><p>第一次启动 Hadoop 时需要进行初始化，进入 <code>$&#123;HADOOP_HOME&#125;/bin/</code> 目录下，执行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 bin]# ./hdfs namenode -format</span><br></pre></td></tr></table></figure><h3 id="3-6-启动HDFS"><a href="#3-6-启动HDFS" class="headerlink" title="3.6 启动HDFS"></a>3.6 启动HDFS</h3><p>进入 <code>$&#123;HADOOP_HOME&#125;/sbin/</code> 目录下，启动 HDFS：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 sbin]# ./start-dfs.sh</span><br></pre></td></tr></table></figure><h3 id="3-7-验证是否启动成功"><a href="#3-7-验证是否启动成功" class="headerlink" title="3.7 验证是否启动成功"></a>3.7 验证是否启动成功</h3><p>方式一：执行 <code>jps</code> 查看 <code>NameNode</code> 和 <code>DataNode</code> 服务是否已经启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps</span><br><span class="line">9137 DataNode</span><br><span class="line">9026 NameNode</span><br><span class="line">9390 SecondaryNameNode</span><br></pre></td></tr></table></figure><p>方式二：查看 Web UI 界面，端口为 <code>50070</code>：</p><p><img src="https://pic.downk.cc/item/5f7d2ff01cd1bbb86b4ff178.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d2ff01cd1bbb86b4ff178.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="四、Hadoop-YARN-环境搭建"><a href="#四、Hadoop-YARN-环境搭建" class="headerlink" title="四、Hadoop(YARN)环境搭建"></a>四、Hadoop(YARN)环境搭建</h2><h3 id="4-1-修改配置"><a href="#4-1-修改配置" class="headerlink" title="4.1 修改配置"></a>4.1 修改配置</h3><p>进入 <code>$&#123;HADOOP_HOME&#125;/etc/hadoop/ </code> 目录下，修改以下配置：</p><h4 id="1-mapred-site-xml"><a href="#1-mapred-site-xml" class="headerlink" title="1. mapred-site.xml"></a>1. mapred-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 如果没有mapred-site.xml，则拷贝一份样例文件后再修改</span><br><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="2-yarn-site-xml"><a href="#2-yarn-site-xml" class="headerlink" title="2. yarn-site.xml"></a>2. yarn-site.xml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="4-2-启动服务"><a href="#4-2-启动服务" class="headerlink" title="4.2 启动服务"></a>4.2 启动服务</h3><p>进入 <code>$&#123;HADOOP_HOME&#125;/sbin/</code> 目录下，启动 YARN：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-yarn.sh</span><br></pre></td></tr></table></figure><h4 id="4-3-验证是否启动成功"><a href="#4-3-验证是否启动成功" class="headerlink" title="4.3 验证是否启动成功"></a>4.3 验证是否启动成功</h4><p>方式一：执行 <code>jps</code> 命令查看 <code>NodeManager</code> 和 <code>ResourceManager</code> 服务是否已经启动：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps</span><br><span class="line">9137 DataNode</span><br><span class="line">9026 NameNode</span><br><span class="line">12294 NodeManager</span><br><span class="line">12185 ResourceManager</span><br><span class="line">9390 SecondaryNameNode</span><br></pre></td></tr></table></figure><p>方式二：查看 Web UI 界面，端口号为 <code>8088</code>：</p><p><img src="https://pic.downk.cc/item/5f7d2ff01cd1bbb86b4ff17b.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d2ff01cd1bbb86b4ff17b.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集群资源管理器—YARN</title>
      <link href="/2021/10/15/%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8%E2%80%94YARN/"/>
      <url>/2021/10/15/%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8%E2%80%94YARN/</url>
      
        <content type="html"><![CDATA[<h2 id="一、hadoop-yarn-简介"><a href="#一、hadoop-yarn-简介" class="headerlink" title="一、hadoop yarn 简介"></a>一、hadoop yarn 简介</h2><p><strong>Apache YARN</strong> (Yet Another Resource Negotiator)  是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。</p><p><img src="https://pic.downk.cc/item/5f7d29591cd1bbb86b4e2bba.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d29591cd1bbb86b4e2bba.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h2 id="二、YARN架构"><a href="#二、YARN架构" class="headerlink" title="二、YARN架构"></a>二、YARN架构</h2><p><img src="https://pic.downk.cc/item/5f7d29591cd1bbb86b4e2bc2.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d29591cd1bbb86b4e2bc2.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h3><p><code>ResourceManager</code> 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。<code>ResourceManager</code> 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。</p><h3 id="2-NodeManager"><a href="#2-NodeManager" class="headerlink" title="2. NodeManager"></a>2. NodeManager</h3><p><code>NodeManager</code> 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下：</p><ul><li>启动时向 <code>ResourceManager</code> 注册并定时发送心跳消息，等待 <code>ResourceManager</code> 的指令；</li><li>维护 <code>Container</code> 的生命周期，监控 <code>Container</code> 的资源使用情况；</li><li>管理任务运行时的相关依赖，根据 <code>ApplicationMaster</code> 的需要，在启动 <code>Container</code> 之前将需要的程序及其依赖拷贝到本地。</li></ul><h3 id="3-ApplicationMaster"><a href="#3-ApplicationMaster" class="headerlink" title="3. ApplicationMaster"></a>3. ApplicationMaster</h3><p>在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 <code>ApplicationMaster</code>。<code>ApplicationMaster</code> 负责协调来自 <code>ResourceManager</code> 的资源，并通过 <code>NodeManager</code> 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下：</p><ul><li>根据应用的运行状态来决定动态计算资源需求；</li><li>向 <code>ResourceManager</code> 申请资源，监控申请的资源的使用情况；</li><li>跟踪任务状态和进度，报告资源的使用情况和应用的进度信息；</li><li>负责任务的容错。</li></ul><h3 id="4-Container"><a href="#4-Container" class="headerlink" title="4. Container"></a>4. Container</h3><p><code>Container</code> 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 <code>Container</code> 表示的。YARN 会为每个任务分配一个 <code>Container</code>，该任务只能使用该 <code>Container</code> 中描述的资源。<code>ApplicationMaster</code> 可在 <code>Container</code> 内运行任何类型的任务。例如，<code>MapReduce ApplicationMaster</code> 请求一个容器来启动 map 或 reduce 任务，而 <code>Giraph ApplicationMaster</code> 请求一个容器来运行 Giraph 任务。</p><h2 id="三、YARN工作原理简述"><a href="#三、YARN工作原理简述" class="headerlink" title="三、YARN工作原理简述"></a>三、YARN工作原理简述</h2><p><img src="https://pic.downk.cc/item/5f7d29591cd1bbb86b4e2bc7.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d29591cd1bbb86b4e2bc7.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><ol><li><code>Client</code> 提交作业到 YARN 上；</li><li><code>Resource Manager</code> 选择一个 <code>Node Manager</code>，启动一个 <code>Container</code> 并运行 <code>Application Master</code> 实例；</li><li><code>Application Master</code> 根据实际需要向 <code>Resource Manager</code> 请求更多的 <code>Container</code> 资源（如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务）；</li><li><code>Application Master</code> 通过获取到的 <code>Container</code> 资源执行分布式计算。</li></ol><h2 id="四、YARN工作原理详述"><a href="#四、YARN工作原理详述" class="headerlink" title="四、YARN工作原理详述"></a>四、YARN工作原理详述</h2><p><img src="https://pic.downk.cc/item/5f7d29591cd1bbb86b4e2bbd.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7d29591cd1bbb86b4e2bbd.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h4 id="1-作业提交"><a href="#1-作业提交" class="headerlink" title="1. 作业提交"></a>1. 作业提交</h4><p>client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 (第 1 步) 。新的作业 ID(应用 ID) 由资源管理器分配 (第 2 步)。作业的 client 核实作业的输出, 计算输入的 split, 将作业的资源 (包括  Jar 包，配置文件, split 信息) 拷贝给 HDFS(第 3 步)。 最后, 通过调用资源管理器的  submitApplication() 来提交作业 (第 4 步)。</p><h4 id="2-作业初始化"><a href="#2-作业初始化" class="headerlink" title="2. 作业初始化"></a>2. 作业初始化</h4><p>当资源管理器收到 submitApplciation() 的请求时, 就将该请求发给调度器 (scheduler), 调度器分配 container, 然后资源管理器在该 container 内启动应用管理器进程, 由节点管理器监控 (第 5 步)。</p><p>MapReduce 作业的应用管理器是一个主类为 MRAppMaster 的 Java 应用，其通过创造一些 bookkeeping  对象来监控作业的进度,  得到任务的进度和完成报告 (第 6 步)。然后其通过分布式文件系统得到由客户端计算好的输入 split(第 7  步)，然后为每个输入 split 创建一个 map 任务, 根据 mapreduce.job.reduces 创建 reduce 任务对象。</p><h4 id="3-任务分配"><a href="#3-任务分配" class="headerlink" title="3. 任务分配"></a>3. 任务分配</h4><p>如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务。</p><p>如果不是小作业,  那么应用管理器向资源管理器请求 container 来运行所有的 map 和 reduce 任务 (第 8  步)。这些请求是通过心跳来传输的,  包括每个 map 任务的数据位置，比如存放输入 split 的主机名和机架  (rack)，调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点, 或者分配给和存放输入 split 的节点相同机架的节点。</p><h4 id="4-任务运行"><a href="#4-任务运行" class="headerlink" title="4. 任务运行"></a>4. 任务运行</h4><p>当一个任务由资源管理器的调度器分配给一个 container 后，应用管理器通过联系节点管理器来启动 container(第 9  步)。任务由一个主类为 YarnChild 的 Java 应用执行， 在运行任务之前首先本地化任务需要的资源，比如作业配置，JAR 文件,   以及分布式缓存的所有文件 (第 10 步。 最后, 运行 map 或 reduce 任务 (第 11 步)。</p><p>YarnChild 运行在一个专用的 JVM 中, 但是 YARN 不支持 JVM 重用。</p><h4 id="5-进度和状态更新"><a href="#5-进度和状态更新" class="headerlink" title="5. 进度和状态更新"></a>5. 进度和状态更新</h4><p>YARN 中的任务将其进度和状态 (包括 counter) 返回给应用管理器, 客户端每秒 (通 mapreduce.client.progressmonitor.pollinterval 设置) 向应用管理器请求进度更新, 展示给用户。</p><h4 id="6-作业完成"><a href="#6-作业完成" class="headerlink" title="6. 作业完成"></a>6. 作业完成</h4><p>除了向应用管理器请求作业进度外,  客户端每 5 分钟都会通过调用 waitForCompletion()  来检查作业是否完成，时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后,  应用管理器和 container 会清理工作状态， OutputCommiter  的作业清理方法也会被调用。作业的信息会被作业历史服务器存储以备之后用户核查。</p><h2 id="五、提交作业到YARN上运行"><a href="#五、提交作业到YARN上运行" class="headerlink" title="五、提交作业到YARN上运行"></a>五、提交作业到YARN上运行</h2><p>这里以提交 Hadoop Examples 中计算 Pi 的 MApReduce 程序为例，相关 Jar 包在 Hadoop 安装目录的 <code>share/hadoop/mapreduce</code> 目录下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 提交格式: hadoop jar jar包路径 主类名称 主类参数</span><br><span class="line"># hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://www.cnblogs.com/codeOfLife/p/5492740.html">初步掌握 Yarn 的架构及原理</a></li><li><a href="http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html">Apache Hadoop 2.9.2 &gt; Apache Hadoop YARN</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分布式计算框架—MapReduce</title>
      <link href="/2021/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E2%80%94MapReduce/"/>
      <url>/2021/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E2%80%94MapReduce/</url>
      
        <content type="html"><![CDATA[<h2 id="一、MapReduce概述"><a href="#一、MapReduce概述" class="headerlink" title="一、MapReduce概述"></a>一、MapReduce概述</h2><p>Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。</p><p>MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 <code>map</code> 以并行的方式处理，框架对 <code>map</code> 的输出进行排序，然后输入到 <code>reduce</code> 中。MapReduce 框架专门用于 <code>键值对处理，它将作业的输入视为一组</code> 对，并生成一组 `` 对作为输出。输出和输出的 <code>key</code> 和 <code>value</code> 都必须实现<a href="http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html">Writable</a> 接口。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output)</span><br></pre></td></tr></table></figure><h2 id="二、MapReduce编程模型简述"><a href="#二、MapReduce编程模型简述" class="headerlink" title="二、MapReduce编程模型简述"></a>二、MapReduce编程模型简述</h2><p>这里以词频统计为例进行说明，MapReduce 处理的流程如下：</p><p><img src="https://pic.downk.cc/item/5f7bec4a160a154a67deeaf8.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7bec4a160a154a67deeaf8.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><ol><li><strong>input</strong> : 读取文本文件；</li><li><strong>splitting</strong> : 将文件按照行进行拆分，此时得到的 <code>K1</code> 行数，<code>V1</code> 表示对应行的文本内容；</li><li><strong>mapping</strong> : 并行将每一行按照空格进行拆分，拆分得到的 <code>List(K2,V2)</code>，其中 <code>K2</code> 代表每一个单词，由于是做词频统计，所以 <code>V2</code> 的值为 1，代表出现 1 次；</li><li><strong>shuffling</strong>：由于 <code>Mapping</code> 操作可能是在不同的机器上并行处理的，所以需要通过 <code>shuffling</code> 将相同 <code>key</code> 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 <code>K2</code> 为每一个单词，<code>List(V2)</code> 为可迭代集合，<code>V2</code> 就是 Mapping 中的 V2；</li><li><strong>Reducing</strong> : 这里的案例是统计单词出现的总次数，所以 <code>Reducing</code> 对 <code>List(V2)</code> 进行归约求和操作，最终输出。</li></ol><p>MapReduce 编程模型中 <code>splitting</code> 和 <code>shuffing</code> 操作都是由框架实现的，需要我们自己编程实现的只有 <code>mapping</code> 和 <code>reducing</code>，这也就是 MapReduce 这个称呼的来源。</p><h2 id="三、combiner-amp-partitioner"><a href="#三、combiner-amp-partitioner" class="headerlink" title="三、combiner &amp; partitioner"></a>三、combiner &amp; partitioner</h2><p><img src="https://pic.downk.cc/item/5f7bec95160a154a67defdb9.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7bec95160a154a67defdb9.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="3-1-InputFormat-amp-RecordReaders"><a href="#3-1-InputFormat-amp-RecordReaders" class="headerlink" title="3.1 InputFormat &amp; RecordReaders"></a>3.1 InputFormat &amp; RecordReaders</h3><p><code>InputFormat</code> 将输出文件拆分为多个 <code>InputSplit</code>，并由 <code>RecordReaders</code> 将 <code>InputSplit</code> 转换为标准的&lt;key，value&gt;键值对，作为 map 的输出。这一步的意义在于只有先进行逻辑拆分并转为标准的键值对格式后，才能为多个 <code>map</code> 提供输入，以便进行并行处理。</p><h3 id="3-2-Combiner"><a href="#3-2-Combiner" class="headerlink" title="3.2 Combiner"></a>3.2 Combiner</h3><p><code>combiner</code> 是 <code>map</code> 运算后的可选操作，它实际上是一个本地化的 <code>reduce</code> 操作，它主要是在 <code>map</code> 计算出中间文件后做一个简单的合并重复 <code>key</code> 值的操作。这里以词频统计为例：</p><p><code>map</code> 在遇到一个 hadoop 的单词时就会记录为 1，但是这篇文章里 hadoop 可能会出现 n 多次，那么 <code>map</code> 输出文件冗余就会很多，因此在 <code>reduce</code> 计算前对相同的 key 做一个合并操作，那么需要传输的数据量就会减少，传输效率就可以得到提升。</p><p>但并非所有场景都适合使用 <code>combiner</code>，使用它的原则是 <code>combiner</code> 的输出不会影响到 <code>reduce</code> 计算的最终输入，例如：求总数，最大值，最小值时都可以使用 <code>combiner</code>，但是做平均值计算则不能使用 <code>combiner</code>。</p><p>不使用 combiner 的情况：</p><p><img src="https://pic.downk.cc/item/5f7bec4a160a154a67deeaf5.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7bec4a160a154a67deeaf5.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><p>使用 combiner 的情况：</p><p><img src="https://pic.downk.cc/item/5f7bec49160a154a67deeaf3.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7bec49160a154a67deeaf3.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><p>可以看到使用 combiner 的时候，需要传输到 reducer 中的数据由 12keys，降低到 10keys。降低的幅度取决于你 keys 的重复率，下文词频统计案例会演示用 combiner 降低数百倍的传输量。</p><h3 id="3-3-Partitioner"><a href="#3-3-Partitioner" class="headerlink" title="3.3 Partitioner"></a>3.3 Partitioner</h3><p><code>partitioner</code> 可以理解成分类器，将 <code>map</code> 的输出按照 key 值的不同分别分给对应的 <code>reducer</code>，支持自定义实现，下文案例会给出演示。</p><h2 id="四、MapReduce词频统计案例"><a href="#四、MapReduce词频统计案例" class="headerlink" title="四、MapReduce词频统计案例"></a>四、MapReduce词频统计案例</h2><h3 id="4-1-项目简介"><a href="#4-1-项目简介" class="headerlink" title="4.1 项目简介"></a>4.1 项目简介</h3><p>这里给出一个经典的词频统计的案例：统计如下样本数据中每个单词出现的次数。</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Spark</span><span class="string">HBase</span></span><br><span class="line"><span class="attr">Hive</span><span class="string">FlinkStormHadoopHBaseSpark</span></span><br><span class="line"><span class="attr">Flink</span></span><br><span class="line"><span class="attr">HBase</span><span class="string">Storm</span></span><br><span class="line"><span class="attr">HBase</span><span class="string">HadoopHiveFlink</span></span><br><span class="line"><span class="attr">HBase</span><span class="string">FlinkHiveStorm</span></span><br><span class="line"><span class="attr">Hive</span><span class="string">FlinkHadoop</span></span><br><span class="line"><span class="attr">HBase</span><span class="string">Hive</span></span><br><span class="line"><span class="attr">Hadoop</span><span class="string">SparkHBaseStorm</span></span><br><span class="line"><span class="attr">HBase</span><span class="string">HadoopHiveFlink</span></span><br><span class="line"><span class="attr">HBase</span><span class="string">FlinkHiveStorm</span></span><br><span class="line"><span class="attr">Hive</span><span class="string">FlinkHadoop</span></span><br><span class="line"><span class="attr">HBase</span><span class="string">Hive</span></span><br></pre></td></tr></table></figure><p>为方便大家开发，我在项目源码中放置了一个工具类 <code>WordCountDataUtils</code>，用于模拟产生词频统计的样本，生成的文件支持输出到本地或者直接写到 HDFS 上。</p><blockquote><p>项目完整源码下载地址：<a href="https://github.com/ihadu/BigData-Notes/tree/master/code/Hadoop/hadoop-word-count">hadoop-word-count</a></p></blockquote><h3 id="4-2-项目依赖"><a href="#4-2-项目依赖" class="headerlink" title="4.2 项目依赖"></a>4.2 项目依赖</h3><p>想要进行 MapReduce 编程，需要导入 <code>hadoop-client</code> 依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="4-3-WordCountMapper"><a href="#4-3-WordCountMapper" class="headerlink" title="4.3 WordCountMapper"></a>4.3 WordCountMapper</h3><p>将每行数据按照指定分隔符进行拆分。这里需要注意在 MapReduce 中必须使用 Hadoop 定义的类型，因为 Hadoop 预定义的类型都是可序列化，可比较的，所有类型均实现了 <code>WritableComparable</code> 接口。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, </span></span><br><span class="line"><span class="function">                                                                      InterruptedException </span>&#123;</span><br><span class="line">        String[] words = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(word), <span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>WordCountMapper</code> 对应下图的 Mapping 操作：</p><img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-code-mapping.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-code-mapping.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/><p><code>WordCountMapper</code> 继承自 <code>Mapper</code> 类，这是一个泛型类，定义如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Mapper</span>&lt;<span class="title">KEYIN</span>, <span class="title">VALUEIN</span>, <span class="title">KEYOUT</span>, <span class="title">VALUEOUT</span>&gt; </span>&#123;</span><br><span class="line">   ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>KEYIN</strong> : <code>mapping</code> 输入 key 的类型，即每行的偏移量 (每行第一个字符在整个文本中的位置)，<code>Long</code> 类型，对应 Hadoop 中的 <code>LongWritable</code> 类型；</li><li><strong>VALUEIN</strong> : <code>mapping</code> 输入 value 的类型，即每行数据；<code>String</code> 类型，对应 Hadoop 中 <code>Text</code> 类型；</li><li><strong>KEYOUT</strong> ：<code>mapping</code> 输出的 key 的类型，即每个单词；<code>String</code> 类型，对应 Hadoop 中 <code>Text</code> 类型；</li><li><strong>VALUEOUT</strong>：<code>mapping</code> 输出 value 的类型，即每个单词出现的次数；这里用 <code>int</code> 类型，对应 <code>IntWritable</code> 类型。</li></ul><h3 id="4-4-WordCountReducer"><a href="#4-4-WordCountReducer" class="headerlink" title="4.4 WordCountReducer"></a>4.4 WordCountReducer</h3><p>在 Reduce 中进行单词出现次数的统计：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, </span></span><br><span class="line"><span class="function">                                                                                  InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            count += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如下图，<code>shuffling</code> 的输出是 reduce 的输入。这里的 key 是每个单词，values 是一个可迭代的数据类型，类似 <code>(1,1,1,...)</code>。</p><img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-code-reducer.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-code-reducer.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/><h3 id="4-4-WordCountApp"><a href="#4-4-WordCountApp" class="headerlink" title="4.4 WordCountApp"></a>4.4 WordCountApp</h3><p>组装 MapReduce 作业，并提交到服务器运行，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 组装作业 并提交到集群运行</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HDFS_URL = <span class="string">&quot;hdfs://192.168.0.107:8020&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> String HADOOP_USER_NAME = <span class="string">&quot;root&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  文件输入路径和输出路径由外部传参指定</span></span><br><span class="line">        <span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;Input and output paths are necessary!&quot;</span>);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常</span></span><br><span class="line">        System.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, HADOOP_USER_NAME);</span><br><span class="line"></span><br><span class="line">        Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">// 指明 HDFS 的地址</span></span><br><span class="line">        configuration.set(<span class="string">&quot;fs.defaultFS&quot;</span>, HDFS_URL);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 创建一个 Job</span></span><br><span class="line">        Job job = Job.getInstance(configuration);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置运行的主类</span></span><br><span class="line">        job.setJarByClass(WordCountApp.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper 和 Reducer</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Mapper 输出 key 和 value 的类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 Reducer 输出 key 和 value 的类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常</span></span><br><span class="line">        FileSystem fileSystem = FileSystem.get(<span class="keyword">new</span> URI(HDFS_URL), configuration, HADOOP_USER_NAME);</span><br><span class="line">        Path outputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span> (fileSystem.exists(outputPath)) &#123;</span><br><span class="line">            fileSystem.delete(outputPath, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置作业输入文件和输出文件的路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度</span></span><br><span class="line">        <span class="keyword">boolean</span> result = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭之前创建的 fileSystem</span></span><br><span class="line">        fileSystem.close();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据作业结果,终止当前运行的 Java 虚拟机,退出程序</span></span><br><span class="line">        System.exit(result ? <span class="number">0</span> : -<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要注意的是：如果不设置 <code>Mapper</code> 操作的输出类型，则程序默认它和 <code>Reducer</code> 操作输出的类型相同。</p><h3 id="4-5-提交到服务器运行"><a href="#4-5-提交到服务器运行" class="headerlink" title="4.5 提交到服务器运行"></a>4.5 提交到服务器运行</h3><p>在实际开发中，可以在本机配置 hadoop 开发环境，直接在 IDE 中启动进行测试。这里主要介绍一下打包提交到服务器运行。由于本项目没有使用除 Hadoop 外的第三方依赖，直接打包即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> mvn clean package</span></span><br></pre></td></tr></table></figure><p>使用以下命令提交作业：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /usr/appjar/hadoop-word-count-1.0.jar \</span><br><span class="line">com.heibaiying.WordCountApp \</span><br><span class="line">/wordcount/input.txt /wordcount/output/WordCountApp</span><br></pre></td></tr></table></figure><p>作业完成后查看 HDFS 上生成目录：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看目录</span></span><br><span class="line">hadoop fs -ls /wordcount/output/WordCountApp</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看统计结果</span></span><br><span class="line">hadoop fs -cat /wordcount/output/WordCountApp/part-r-00000</span><br></pre></td></tr></table></figure><img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-wordcountapp.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-wordcountapp.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/><h2 id="五、词频统计案例进阶之Combiner"><a href="#五、词频统计案例进阶之Combiner" class="headerlink" title="五、词频统计案例进阶之Combiner"></a>五、词频统计案例进阶之Combiner</h2><h3 id="5-1-代码实现"><a href="#5-1-代码实现" class="headerlink" title="5.1 代码实现"></a>5.1 代码实现</h3><p>想要使用 <code>combiner</code> 功能只要在组装作业时，添加下面一行代码即可：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置 Combiner</span></span><br><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure><h3 id="5-2-执行结果"><a href="#5-2-执行结果" class="headerlink" title="5.2 执行结果"></a>5.2 执行结果</h3><p>加入 <code>combiner</code> 后统计结果是不会有变化的，但是可以从打印的日志看出 <code>combiner</code> 的效果：</p><p>没有加入 <code>combiner</code> 的打印日志：</p><img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-no-combiner.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-no-combiner.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/><p>加入 <code>combiner</code> 后的打印日志如下：</p><img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-combiner.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-combiner.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/><p>这里我们只有一个输入文件并且小于 128M，所以只有一个 Map 进行处理。可以看到经过 combiner 后，records 由 <code>3519</code> 降低为 <code>6</code>(样本中单词种类就只有 6 种)，在这个用例中 combiner 就能极大地降低需要传输的数据量。</p><h2 id="六、词频统计案例进阶之Partitioner"><a href="#六、词频统计案例进阶之Partitioner" class="headerlink" title="六、词频统计案例进阶之Partitioner"></a>六、词频统计案例进阶之Partitioner</h2><h3 id="6-1-默认的Partitioner"><a href="#6-1-默认的Partitioner" class="headerlink" title="6.1  默认的Partitioner"></a>6.1  默认的Partitioner</h3><p>这里假设有个需求：将不同单词的统计结果输出到不同文件。这种需求实际上比较常见，比如统计产品的销量时，需要将结果按照产品种类进行拆分。要实现这个功能，就需要用到自定义 <code>Partitioner</code>。</p><p>这里先介绍下 MapReduce 默认的分类规则：在构建 job 时候，如果不指定，默认的使用的是 <code>HashPartitioner</code>：对 key 值进行哈希散列并对 <code>numReduceTasks</code> 取余。其实现如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashPartitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(K key, V value,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="keyword">int</span> numReduceTasks)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="6-2-自定义Partitioner"><a href="#6-2-自定义Partitioner" class="headerlink" title="6.2 自定义Partitioner"></a>6.2 自定义Partitioner</h3><p>这里我们继承 <code>Partitioner</code> 自定义分类规则，这里按照单词进行分类：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, IntWritable intWritable, <span class="keyword">int</span> numPartitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> WordCountDataUtils.WORD_LIST.indexOf(text.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在构建 <code>job</code> 时候指定使用我们自己的分类规则，并设置 <code>reduce</code> 的个数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置自定义分区规则</span></span><br><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br><span class="line"><span class="comment">// 设置 reduce 个数</span></span><br><span class="line">job.setNumReduceTasks(WordCountDataUtils.WORD_LIST.size());</span><br></pre></td></tr></table></figure><h3 id="6-3-执行结果"><a href="#6-3-执行结果" class="headerlink" title="6.3  执行结果"></a>6.3  执行结果</h3><p>执行结果如下，分别生成 6 个文件，每个文件中为对应单词的统计结果：</p><img  src="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-wordcountcombinerpartition.png" class="lazyload placeholder" data-srcset="https://gitee.com/ihadyou/BigData-Notes/raw/master/pictures/hadoop-wordcountcombinerpartition.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"/><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/28682581">分布式计算框架 MapReduce</a></li><li><a href="http://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">Apache Hadoop 2.9.2 &gt; MapReduce Tutorial</a></li><li><a href="https://www.tutorialscampus.com/tutorials/map-reduce/combiners.htm">MapReduce - Combiners</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop分布式文件系统—HDFS</title>
      <link href="/2021/10/15/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E2%80%94HDFS/"/>
      <url>/2021/10/15/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E2%80%94HDFS/</url>
      
        <content type="html"><![CDATA[<h2 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h2><p><strong>HDFS</strong> （<strong>Hadoop Distributed File System</strong>）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。</p><h2 id="二、HDFS-设计原理"><a href="#二、HDFS-设计原理" class="headerlink" title="二、HDFS 设计原理"></a>二、HDFS 设计原理</h2><p><img src="https://pic.downk.cc/item/5f7be0ef160a154a67dc8a81.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7be0ef160a154a67dc8a81.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="2-1-HDFS-架构"><a href="#2-1-HDFS-架构" class="headerlink" title="2.1 HDFS 架构"></a>2.1 HDFS 架构</h3><p>HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：</p><ul><li><strong>NameNode</strong> : 负责执行有关 <code>文件系统命名空间</code> 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。</li><li><strong>DataNode</strong>：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。</li></ul><h3 id="2-2-文件系统命名空间"><a href="#2-2-文件系统命名空间" class="headerlink" title="2.2 文件系统命名空间"></a>2.2 文件系统命名空间</h3><p>HDFS 的 <code>文件系统命名空间</code> 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。<code>NameNode</code> 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。</p><h3 id="2-3-数据复制"><a href="#2-3-数据复制" class="headerlink" title="2.3 数据复制"></a>2.3 数据复制</h3><p>由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列<strong>块</strong>，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。</p><p><img src="https://pic.downk.cc/item/5f7be0ef160a154a67dc8a7e.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7be0ef160a154a67dc8a7e.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="2-4-数据复制的实现原理"><a href="#2-4-数据复制的实现原理" class="headerlink" title="2.4 数据复制的实现原理"></a>2.4 数据复制的实现原理</h3><p>大型的 HDFS  实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：</p><p>在写入程序位于 <code>datanode</code> 上时，就优先将写入文件的一个副本放置在该 <code>datanode</code> 上，否则放在随机 <code>datanode</code> 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。</p><p><img src="https://pic.downk.cc/item/5f7be0ef160a154a67dc8a7c.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f7be0ef160a154a67dc8a7c.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><p>如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 <code>（复制系数 - 1）/机架数量 + 2</code>，需要注意的是不允许同一个 <code>dataNode</code> 上具有同一个块的多个副本。</p><h3 id="2-5-副本的选择"><a href="#2-5-副本的选择" class="headerlink" title="2.5  副本的选择"></a>2.5  副本的选择</h3><p>为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。</p><h3 id="2-6-架构的稳定性"><a href="#2-6-架构的稳定性" class="headerlink" title="2.6 架构的稳定性"></a>2.6 架构的稳定性</h3><h4 id="1-心跳机制和重新复制"><a href="#1-心跳机制和重新复制" class="headerlink" title="1. 心跳机制和重新复制"></a>1. 心跳机制和重新复制</h4><p>每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode  标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。  由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。</p><h4 id="2-数据的完整性"><a href="#2-数据的完整性" class="headerlink" title="2. 数据的完整性"></a>2. 数据的完整性</h4><p>由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下：</p><p>当客户端创建 HDFS 文件时，它会计算文件的每个块的 <code>校验和</code>，并将 <code>校验和</code> 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 <code>校验和</code> 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。</p><h4 id="3-元数据的磁盘故障"><a href="#3-元数据的磁盘故障" class="headerlink" title="3.元数据的磁盘故障"></a>3.元数据的磁盘故障</h4><p><code>FsImage</code> 和 <code>EditLog</code> 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 <code>FsImage</code> 和 <code>EditLog</code> 多副本同步，这样 <code>FsImage</code> 或 <code>EditLog</code> 的任何改变都会引起每个副本 <code>FsImage</code> 和 <code>EditLog</code> 的同步更新。</p><h4 id="4-支持快照"><a href="#4-支持快照" class="headerlink" title="4.支持快照"></a>4.支持快照</h4><p>快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。</p><h2 id="三、HDFS-的特点"><a href="#三、HDFS-的特点" class="headerlink" title="三、HDFS 的特点"></a>三、HDFS 的特点</h2><h3 id="3-1-高容错"><a href="#3-1-高容错" class="headerlink" title="3.1 高容错"></a>3.1 高容错</h3><p>由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。</p><h3 id="3-2-高吞吐量"><a href="#3-2-高吞吐量" class="headerlink" title="3.2 高吞吐量"></a>3.2 高吞吐量</h3><p>HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。</p><h3 id="3-3-大文件支持"><a href="#3-3-大文件支持" class="headerlink" title="3.3  大文件支持"></a>3.3  大文件支持</h3><p>HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。</p><h3 id="3-3-简单一致性模型"><a href="#3-3-简单一致性模型" class="headerlink" title="3.3 简单一致性模型"></a>3.3 简单一致性模型</h3><p>HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。</p><h3 id="3-4-跨平台移植性"><a href="#3-4-跨平台移植性" class="headerlink" title="3.4 跨平台移植性"></a>3.4 跨平台移植性</h3><p>HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。</p>]]></content>
      
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive自定义UDF函数</title>
      <link href="/2021/10/13/hive%E8%87%AA%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0/"/>
      <url>/2021/10/13/hive%E8%87%AA%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Hive自定义函数介绍"><a href="#1-Hive自定义函数介绍" class="headerlink" title="1. Hive自定义函数介绍"></a>1. Hive自定义函数介绍</h3><p>当Hive提供的内置函数无法满足你的业务处理需要时，此时可以考虑使用用户自定义函数（UDF: user-defined function）。<br>Hive 中常用的UDF有如下三种:</p><ul><li>UDF<br>一条记录使用函数后输出还是一条记录，比如:upper/substr;</li><li>UDAF(User-Defined Aggregation Funcation)<br>多条记录使用函数后输出还是一条记录，比如: count/max/min/sum/avg;</li><li>UDTF(User-Defined Table-Generating Functions)<br>一条记录使用函数后输出多条记录，比如: lateral view explore();<h3 id="2-Hive自定义函数开发"><a href="#2-Hive自定义函数开发" class="headerlink" title="2. Hive自定义函数开发"></a>2. Hive自定义函数开发</h3>需求:开发自定义函数，使得在指定字段前加上“Hello:”字样。Hive 中 UDF函数开发步骤:<br>(1）继承UDF 类。<br>(2）重写evaluate方法，该方法支持重载，每行记录执行一次evaluate方法。</li></ul><h4 id="注意："><a href="#注意：" class="headerlink" title="##### 注意："></a>##### 注意：</h4><p>1 UDF必须要有返回值,可以是null,但是不能为 void.<br>2 推荐使用 Text/LongWritable等Hadoop的类型,而不是Java类型(当然使用 Java类型也是可以的)。</p><p>功能实现:</p><h5 id="1-pom-xml中添加UDF函数开发的依赖包。"><a href="#1-pom-xml中添加UDF函数开发的依赖包。" class="headerlink" title="( 1)pom.xml中添加UDF函数开发的依赖包。"></a>( 1)pom.xml中添加UDF函数开发的依赖包。</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hadoop.version</span>&gt;</span>2.6.0-cdh5.7.0<span class="tag">&lt;/<span class="name">hadoop.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">hive.version</span>&gt;</span>1.1.0-cdh5.7.0<span class="tag">&lt;/<span class="name">hive.version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--CDH 版本建议大家添加一个repository--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>cloudera<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">url</span>&gt;</span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--Hadoop依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupld</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactld</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$ &#123;hadoop.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--Hive依赖--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupld</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupld</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$ &#123; hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">groupld</span>&gt;</span>org.apacne.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">artifactld</span>&gt;</span>hive-jdbc<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">version</span>&gt;</span>$ &#123; hive.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h5 id="2）开发UDF函数。"><a href="#2）开发UDF函数。" class="headerlink" title="(2）开发UDF函数。"></a>(2）开发UDF函数。</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.kgc.bigdata.hadoop.hive;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*功能:输入xxx，输出:Hello: xxx</span></span><br><span class="line"><span class="comment">*</span></span><br><span class="line"><span class="comment">*开发UDF 函数的步骤</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment">* 1) extends UDF</span></span><br><span class="line"><span class="comment">*2）重写evaluate方法，注意该方法是支持重载的</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HelloUDF</span> <span class="keyword">extends</span> <span class="title">UDF</span></span>&#123;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">*对于UDF 函数的evaluate的参数和返回值，个人建议使用Writable* <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">* <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(Text name)</span></span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Text(<span class="string">&quot;Hello: &quot;</span> + name);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(Text name,IntWritable age)</span></span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> Text(<span class="string">&quot;Hello: &quot;</span> +name + <span class="string">&quot; , age :&quot;</span> + age);</span><br><span class="line">    &#125;</span><br><span class="line">/功能测试</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span></span>&#123;</span><br><span class="line">HelloUDF udf = <span class="keyword">new</span> HelloUDF(</span><br><span class="line">System.out.println(udf.evaluate(<span class="keyword">new</span> Text(<span class="string">&quot;zhangsan&quot;</span>)));</span><br><span class="line">System.out.println(udf.evaluate(<span class="keyword">new</span> Text(<span class="string">&quot;zhangsan&quot;</span>), <span class="keyword">new</span> IntWritable(<span class="number">20</span>)));</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(3）编译jar包上传到服务器。<br>(4)将自定义UDF 函数添加到Hive 中去。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add JAR/home/hadoop/lib/hive-1.0.jar;</span><br><span class="line">create temporary function sayHello as &#x27;com.kgc.bigdata.hadoop.hive.HelloUDF&#x27;;</span><br></pre></td></tr></table></figure><p>(5)使用自定义函数。<br>//通过show functions可以看到我们自定义的sayHello函数show functions;<br>//将员工表的ename作为自定义UDF函数的参数值，即可查看到最终的输出结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> empno, ename, sayHello(ename) <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka分区</title>
      <link href="/2021/10/13/kafka%E5%88%86%E5%8C%BA/"/>
      <url>/2021/10/13/kafka%E5%88%86%E5%8C%BA/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2 id="设置topic下的分区数"><a href="#设置topic下的分区数" class="headerlink" title="设置topic下的分区数"></a>设置topic下的分区数</h2><ol><li>在 config/server.properties 配置文件中, 可以设置一个全局的分区数量, 这个分区数量的含义是: <strong>每个主题下的分区数量</strong>, 默认为 1</li></ol><img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230058746-945280798.png" class="lazyload placeholder" data-srcset="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230058746-945280798.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"><ol start="2"><li>也可以在创建主题的时候, 使用 –partitions 参数指定分区数量</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic my_topic --partitions 2 --replication-factor 1 </span><br></pre></td></tr></table></figure><p>3.查看已创建主题的分区数量:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my_topic </span><br></pre></td></tr></table></figure><img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230212246-1072040750.png" class="lazyload placeholder" data-srcset="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230212246-1072040750.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"><h2 id="生产者与分区"><a href="#生产者与分区" class="headerlink" title="生产者与分区"></a>生产者与分区</h2><p><strong>org.apache.kafka.clients.producer.internals.DefaultPartitioner</strong></p><p>默认的分区策略是：</p><ul><li>如果在发消息的时候指定了分区，则消息投递到指定的分区</li><li>如果没有指定分区，但是消息的key不为空，则基于key的哈希值来选择一个分区</li><li>如果既没有指定分区，且消息的key也是空，则用轮询的方式选择一个分区</li></ul><h2 id="消费者与分区"><a href="#消费者与分区" class="headerlink" title="消费者与分区"></a>消费者与分区</h2><p>首先需要了解的是:</p><ol><li>消费者是以组的名义订阅主题消息, 消费者组里边包含多个消费者实例.</li><li>主题下边包含多个分区</li></ol><p>消费者实例与主题下分区的分配关系</p><img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230325655-792813000.png" class="lazyload placeholder" data-srcset="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230325655-792813000.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"><p>kafka 集群上有两个节点, 4 个分区</p><p>A组有 2 个消费者实例 (两个消费线程)</p><p>B组有 4 个消费者实例</p><p>由图可以看出, A组的消费者C1, C2 平均要消费两个分区的数据, 而 B 组的消费者平均消费 一 个分区的数据 ( 最理想的状态 ), 得到的结论是 : <strong>一条消息只能被一个消费组中的一个消费者实例消费到</strong>, (换句话说, 不可能出现组中的两个消费者负责同一个分区, 同组内消费者不会重复消费 )</p><p>等等, 考虑的场景还不够, 下边再提些问题 :</p><p>如果分区数大于或等于组中的消费者实例数, 那就没有问题, 但是如果消费者实例的数量 &gt; 主题下分区数量, 那么按照默认的策略 ( 之所以强调默认策略是因为可以自定义策略 ), 有一些消费者是多余的, 一直接不到消息而处于空闲状态.</p><p>但是假设有消费者实例就是不安分, 就造成了多个消费者负责同一个分区, 这样会造成什么 ? (重复消费就太可怕了)</p><p>我们知道，Kafka它在设计的时候就是要保证分区下消息的顺序，也就是说消息在一个分区中的顺序是怎样的，那么消费者在消费的时候看到的就是什么样的顺序，那么要做到这一点就首先要保证消息是由消费者主动拉取的（pull），其次还要保证一个分区只能由一个消费者负责。倘若，两个消费者负责同一个分区，那么就意味着两个消费者同时读取分区的消息，由于消费者自己可以控制读取消息的offset (偏移量)，就有可能C1才读到2，而C2读到1，C1还没提交 offset，这时C2读到2了，相当于多线程读取同一个消息，会造成消息处理的重复，且不能保证消息的顺序，这就跟主动推送（push）无异。</p><h2 id="消费者分区分配策略-两种"><a href="#消费者分区分配策略-两种" class="headerlink" title="消费者分区分配策略 (两种)"></a>消费者分区分配策略 (两种)</h2><p>range策略是基于每个主题的，对于每个主题，我们以数字顺序排列可用分区，以字典顺序排列消费者。然后，将分区数量除以消费者总数，以确定分配给每个消费者的分区数量。如果没有平均划分（PS：除不尽），那么最初的几个消费者将有一个额外的分区。</p><p>简而言之:</p><ol><li>range分配策略针对的是主题（也就是说，这里所说的分区指的某个主题的分区，消费者值的是订阅这个主题的消费者组中的消费者实例）</li><li>首先，将分区按数字顺序排行序，消费者按消费者名称的字典顺序排好序.</li><li>然后，用分区总数除以消费者总数。如果能够除尽，则皆大欢喜，平均分配；若除不尽，则位于排序前面的消费者将多负责一个分区.</li></ol><p>例如，假设有两个消费者C0和C1，两个主题t0和t1，并且每个主题有3个分区，分区的情况是这样的：t0p0，t0p1，t0p2，t1p0，t1p1，t1p2</p><p>那么，基于以上信息，最终消费者分配分区的情况是这样的：</p><p>C0: [t0p0, t0p1, t1p0, t1p1]</p><p>C1: [t0p2, t1p2]</p><p>因为，对于主题t0，分配的结果是C0负责P0和P1，C1负责P2；对于主题t2，也是如此，综合起来就是这个结果</p><p>上面的过程用图形表示的话大概是这样的 :</p><img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230609350-1656648320.png" class="lazyload placeholder" data-srcset="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230609350-1656648320.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"><p>roundrobin (轮询)</p><p>roundronbin分配策略的具体实现是org.apache.kafka.clients.consumer.RoundRobinAssignor</p><p>轮询分配策略是基于所有可用的消费者和所有可用的分区的</p><p>与前面的range策略最大的不同就是它不再局限于某个主题</p><p>如果所有的消费者实例的订阅都是相同的，那么这样最好了，可用统一分配，均衡分配</p><p>例如，假设有两个消费者C0和C1，两个主题t0和t1，每个主题有3个分区，分别是t0p0，t0p1，t0p2，t1p0，t1p1，t1p2</p><p>那么，最终分配的结果是这样的：</p><p>C0: [t0p0, t0p2, t1p1]</p><p>C1: [t0p1, t1p0, t1p2]</p><p>用图形表示大概是这样的:</p><img src="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230645759-1165797262.png" class="lazyload placeholder" data-srcset="https://img2018.cnblogs.com/blog/1629331/201911/1629331-20191129230645759-1165797262.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer">]]></content>
      
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux环境变量加载顺序</title>
      <link href="/2021/10/13/linux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F/"/>
      <url>/2021/10/13/linux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="01、环境变量文件描述"><a href="#01、环境变量文件描述" class="headerlink" title="01、环境变量文件描述"></a>01、环境变量文件描述</h1><p><strong>/etc/profile</strong>: 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行,并从/etc/profile.d目录的配置文件中搜集shell的设置.<br><strong>/etc/bashrc</strong>: 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取.</p><p>//用户级别的环境变量，用户可以覆盖全局变量<br><strong>~/.bash_profile</strong>: 每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.<br><strong>~/.bashrc</strong>: 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取.<br><strong>~/.bash_logout</strong>: 当每次退出系统(退出bash shell)时,执行该文件.</p><p>/etc/profile中设定的变量(全局)的可以作用于任何用户,<br>而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是”父子”关系.</p><p>~/.bash_profile 是交互式、login 方式进入 bash 运行的<br>~/.bashrc 是交互式 non-login 方式进入 bash 运行的<br>通常二者设置大致相同，所以通常前者会调用后者</p><h2 id="一、系统环境变量："><a href="#一、系统环境变量：" class="headerlink" title="一、系统环境变量："></a>一、系统环境变量：</h2><p><strong>/etc/profile</strong> ：这个文件预设了几个重要的变量，例如PATH, USER, LOGNAME, MAIL, INPUTRC, HOSTNAME, HISTSIZE, umask等等。</p><p>为系统的每个用户设置环境信息。当用户第一次登陆时，该文件执行，并从/etc/profile.d目录中的配置文件搜索shell的设置（可以用于设定针对全系统所有用户的环境变量，环境变量周期是永久的）</p><p><strong>/etc/bashrc</strong> ：这个文件主要预设umask以及PS1。这个PS1就是我们在敲命令时，前面那串字符了，例如 [root@localhost ~]#,当bash shell被打开时,该文件被读取</p><p>这个文件是针对所有用户的bash初始化文件，在此设定中的环境信息将应用与所有用户的shell中，此文件会在用户每次打开shell时执行一次。（即每次新开一个终端，都会执行/etc/bashrc）**</p><h2 id="二、用户环境变量："><a href="#二、用户环境变量：" class="headerlink" title="二、用户环境变量："></a>二、用户环境变量：</h2><p><strong>.bash_profile</strong> ：定义了用户的个人化路径与环境变量的文件名称。每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次。（在这个文件中有执行.bashrc的脚本）</p><p><strong>.bashrc</strong> ：该文件包含专用于你的shell的bash信息,当登录时以及每次打开新的shell时,该该文件被读取。例如你可以将用户自定义的alias或者自定义变量写到这个文件中。</p><p><strong>.bash_history</strong> ：记录命令历史用的。</p><p><strong>.bash_logout</strong> ：当退出shell时，会执行该文件。可以把一些清理的工作放到这个文件中。</p><p>linux加载配置项时通过下面方式<br>首先 加载/etc/profile配置</p><p>然后 加载/ect/profile.d/下面的所有脚本</p><p>然后 加载当前用户 .bash_profile</p><p>然后 加载.bashrc</p><p>最后 加载 [/etc/bashrc]</p><p>/etc/profile → /etc/profile.d/*.sh → ~/.bash_profile → ~/.bashrc → [/etc/bashrc]</p>]]></content>
      
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识kudu</title>
      <link href="/2021/10/13/%E5%88%9D%E8%AF%86kudu/"/>
      <url>/2021/10/13/%E5%88%9D%E8%AF%86kudu/</url>
      
        <content type="html"><![CDATA[<h1 id="1、-kudu简介"><a href="#1、-kudu简介" class="headerlink" title="1、 kudu简介"></a>1、 kudu简介</h1><h2 id="1-1、kudu是什么"><a href="#1-1、kudu是什么" class="headerlink" title="1.1、kudu是什么"></a>1.1、kudu是什么</h2><p>简单来说:dudu是一个与hbase类似的列式存储分布式数据库。<br>官方给kudu的定位是:在更新更及时的基础上实现更快的数据分析</p><h2 id="1-2、为什么需要kudu"><a href="#1-2、为什么需要kudu" class="headerlink" title="1.2、为什么需要kudu"></a>1.2、为什么需要kudu</h2><h3 id="1-2-1、hdfs与hbase数据存储的缺点"><a href="#1-2-1、hdfs与hbase数据存储的缺点" class="headerlink" title="1.2.1、hdfs与hbase数据存储的缺点"></a>1.2.1、hdfs与hbase数据存储的缺点</h3><p>目前数据存储有了HDFS与hbase，为什么还要额外的弄一个kudu呢。<br>HDFS:使用列式存储格式Apache Parquet，Apache ORC，适合离线分析，不支持单条纪录级别的update操作，随机读写性能差。<br>HBASE:可以进行高效随机读写，却并不适用于基于SQL的数据分析方向，大批量数据获取时的性能较差。<br>正因为HDFS与HBASE有上面这些缺点，KUDU较好的解决了HDFS与HBASE的这些缺点，它不及HDFS批处理快，也不及HBase随机读写能力强，但是反过来它比HBase批处理快（适用于OLAP的分析场景），而且比HDFS随机读写能力强（适用于实时写入或者更新的场景），这就是它能解决的问题。</p><h1 id="2、架构介绍"><a href="#2、架构介绍" class="headerlink" title="2、架构介绍"></a>2、架构介绍</h1><h2 id="2-1、基本架构"><a href="#2-1、基本架构" class="headerlink" title="2.1、基本架构"></a>2.1、基本架构</h2><img src="https://img-blog.csdnimg.cn/20190422091319780.png" class="lazyload placeholder" data-srcset="https://img-blog.csdnimg.cn/20190422091319780.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"><h2 id="2-1-1、概念"><a href="#2-1-1、概念" class="headerlink" title="2.1.1、概念"></a>2.1.1、概念</h2><p> Table（表）：一张table是数据存储在kudu的位置。Table具有schema和全局有序的primary key(主键)。Table被分为很多段，也就是tablets.<br> Tablet (段)：一个tablet是一张table连续的segment，与其他数据存储引擎或关系型数据的partition相似。Tablet存在副本机制，其中一个副本为leader tablet。任何副本都可以对读取进行服务，并且写入时需要在所有副本对应的tablet server之间达成一致性。<br> Tablet server：存储tablet和为tablet向client提供服务。对于给定的tablet，一个tablet server充当leader，其他tablet server充当该tablet的follower副本。只有leader服务写请求，leader与follower为每个服务提供读请求。<br> Master：主要用来管理元数据(元数据存储在只有一个tablet的catalog table中)，即tablet与表的基本信息，监听tserver的状态<br> Catalog Table: 元数据表，用来存储table(schema、locations、states)与tablet（现有的tablet列表，每个tablet及其副本所处tserver，tablet当前状态以及开始和结束键）的信息。</p><h1 id="3、存储机制"><a href="#3、存储机制" class="headerlink" title="3、存储机制"></a>3、存储机制</h1><h2 id="3-1-存储结构全景图"><a href="#3-1-存储结构全景图" class="headerlink" title="3.1 存储结构全景图"></a>3.1 存储结构全景图</h2><img src="https://img-blog.csdnimg.cn/20190422091359222.png" class="lazyload placeholder" data-srcset="https://img-blog.csdnimg.cn/20190422091359222.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"><h2 id="3-2-存储结构解析"><a href="#3-2-存储结构解析" class="headerlink" title="3.2 存储结构解析"></a>3.2 存储结构解析</h2><p> 一个Table包含多个Tablet，其中Tablet的数量是根据hash或者range进行设置<br> 一个Tablet中包含MetaData信息和多个RowSet信息<br> 一个Rowset中包含一个MemRowSet与0个或多个DiskRowset，其中MemRowSet存储insert的数据，一旦MemRowSet写满会flush到磁盘生成一个或多个DiskRowSet，此时MemRowSet清空。MemRowSet默认写满1G或者120s flush一次<br>(注意:memRowSet是行式存储，DiskRowSet是列式存储，MemRowSet基于primary key有序)。每隔tablet中会定期对一些diskrowset做compaction操作，目的是对多个diskRowSet进行重新排序，以此来使其更有序并减少diskRowSet的数量，同时在compaction的过程中慧慧resolve掉deltaStores当中的delete记录<br> 一个DiskRowSet包含baseData与DeltaStores两部分，其中baseData存储的数据看起来不可改变，DeltaStores中存储的是改变的数据<br> DeltaStores包含一个DeltaMemStores和多个DeltaFile,其中DeltaMemStores放在内存，用来存储update与delete数据，一旦DeltaMemStores写满，会flush成DeltaFile。<br>当DeltaFile过多会影响查询性能，所以KUDU每隔一段时间会执行compaction操作，将其合并到baseData中，主要是resolve掉update数据。</p><h1 id="4、kudu的工作机制"><a href="#4、kudu的工作机制" class="headerlink" title="4、kudu的工作机制"></a>4、kudu的工作机制</h1><h2 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h2><p>1、kudu主要角色分为master与tserver<br>2、master主要负责:管理元数据信息，监听server，当server宕机后负责tablet的重分配<br>3、tserver主要负责tablet的存储与和数据的增删改查</p><h2 id="4-2-内部实现原理图"><a href="#4-2-内部实现原理图" class="headerlink" title="4.2 内部实现原理图"></a>4.2 内部实现原理图</h2><h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://img-blog.csdnimg.cn/20190422091414844.png" class="lazyload placeholder" data-srcset="https://img-blog.csdnimg.cn/20190422091414844.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"></h2><h2 id="4-3-读流程"><a href="#4-3-读流程" class="headerlink" title="4.3 读流程"></a>4.3 读流程</h2><h3 id="4-3-1-概述"><a href="#4-3-1-概述" class="headerlink" title="4.3.1 概述"></a>4.3.1 概述</h3><p>客户端将要读取的数据信息发送给master，master对其进行一定的校验，比如表是否存在，字段是否存在。Master返回元数据信息给client，然后client与tserver建立连接，通过metaData找到数据所在的RowSet，首先加载内存里面的数据(MemRowSet与DeltMemStore),然后加载磁盘里面的数据，最后返回最终数据给client.</p><h3 id="4-3-2-详细步骤图"><a href="#4-3-2-详细步骤图" class="headerlink" title="4.3.2 详细步骤图"></a>4.3.2 详细步骤图</h3> <img src="https://img-blog.csdnimg.cn/20190422091428943.png" class="lazyload placeholder" data-srcset="https://img-blog.csdnimg.cn/20190422091428943.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"><h3 id="4-3-3-详细步骤解析"><a href="#4-3-3-详细步骤解析" class="headerlink" title="4.3.3 详细步骤解析"></a>4.3.3 详细步骤解析</h3><p>1、客户端master请求查询表指定数据<br>2、master对请求进行校验，校验表是否存在，schema中是否存在指定查询的字段，主键是否存在<br>3、master通过查询catalog Table返回表，将tablet对应的tserver信息、tserver状态等元数据信息返回给client<br>4、client与tserver建立连接，通过metaData找到primary key对应的RowSet。<br>5、首先加载RowSet内存中MemRowSet与DeltMemStore中的数据<br>6、然后加载磁盘中的数据，也就是DiskRowSet中的BaseData与DeltFile中的数据<br>7、返回数据给Client<br>8、继续4-7步骤，直到拿到所有数据返回给client</p><h2 id="4-4、插入流程"><a href="#4-4、插入流程" class="headerlink" title="4.4、插入流程"></a>4.4、插入流程</h2><h3 id="4-4-1-概述"><a href="#4-4-1-概述" class="headerlink" title="4.4.1 概述"></a>4.4.1 概述</h3><p>Client首先连接master，获取元数据信息。然后连接tserver，查找MemRowSet与DeltMemStore中是否存在相同primary key，如果存在，则报错;如果不存在，则将待插入的数据写入WAL日志，然后将数据写入MemRowSet。</p><h3 id="4-4-2-详细步骤图"><a href="#4-4-2-详细步骤图" class="headerlink" title="4.4.2 详细步骤图"></a>4.4.2 详细步骤图</h3><img src="https://img-blog.csdnimg.cn/20190422091442719.png" class="lazyload placeholder" data-srcset="https://img-blog.csdnimg.cn/20190422091442719.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"> <h3 id="4-4-3-详细步骤解析"><a href="#4-4-3-详细步骤解析" class="headerlink" title="4.4.3 详细步骤解析"></a>4.4.3 详细步骤解析</h3><p>1、client向master请求预写表的元数据信息<br>2、master会进行一定的校验，表是否存在，字段是否存在等<br>3、如果master校验通过，则返回表的分区、tablet与其对应的tserver给client；如果校验失败则报错给client。<br>4、client根据master返回的元数据信息，将请求发送给tablet对应的tserver.<br>5、tserver首先会查询内存中MemRowSet与DeltMemStore中是否存在与待插入数据主键相同的数据，如果存在则报错<br>6、tserver会讲写请求预写到WAL日志，用来server宕机后的恢复操作<br>7、将数据写入内存中的MemRowSet中，一旦MemRowSet的大小达到1G或120s后，MemRowSet会flush成一个或DiskRowSet,用来将数据持久化<br>8、返回client数据处理完毕</p><h2 id="4-5、数据更新流程"><a href="#4-5、数据更新流程" class="headerlink" title="4.5、数据更新流程"></a>4.5、数据更新流程</h2><h3 id="4-5-1-概述"><a href="#4-5-1-概述" class="headerlink" title="4.5.1 概述"></a>4.5.1 概述</h3><p>Client首先向master请求元数据，然后根据元数据提供的tablet信息，连接tserver，根据数据所处位置的不同，有不同的操作:在内存MemRowSet中的数据，会将更新信息写入数据所在行的mutation链表中；在磁盘中的数据，会将更新信息写入DeltMemStore中。</p><h3 id="4-5-2、详细步骤图"><a href="#4-5-2、详细步骤图" class="headerlink" title="4.5.2、详细步骤图"></a>4.5.2、详细步骤图</h3> <img src="https://img-blog.csdnimg.cn/20190422091450166.png" class="lazyload placeholder" data-srcset="https://img-blog.csdnimg.cn/20190422091450166.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg" referrerpolicy="no-referrer"><h3 id="4-5-3-详细步骤解析"><a href="#4-5-3-详细步骤解析" class="headerlink" title="4.5.3 详细步骤解析"></a>4.5.3 详细步骤解析</h3><p>1、client向master请求预更新表的元数据，首先master会校验表是否存在，字段是否存在，如果校验通过则会返回给client表的分区、tablet、tablet所在tserver信息<br>2、client向tserver发起更新请求<br>3、将更新操作预写如WAL日志，用来在server宕机后的数据恢复<br>4、根据tserver中待更新的数据所处位置的不同，有不同的处理方式:<br>如果数据在内存中，则从MemRowSet中找到数据所处的行，然后在改行的mutation链表中写入更新信息，在MemRowSet flush的时候，将更新合并到baseData中<br>如果数据在DiskRowSet中，则将更新信息写入DeltMemStore中，DeltMemStore达到一定大小后会flush成DeltFile。<br>5、更新完毕后返回消息给client。</p>]]></content>
      
      
      
        <tags>
            
            <tag> kudu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息队列易错指南</title>
      <link href="/2021/10/13/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%98%93%E9%94%99%E6%8C%87%E5%8D%97/"/>
      <url>/2021/10/13/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%98%93%E9%94%99%E6%8C%87%E5%8D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="1-消息队列的坑之非幂等"><a href="#1-消息队列的坑之非幂等" class="headerlink" title="1.消息队列的坑之非幂等"></a>1.消息队列的坑之非幂等</h2><h3 id="（1）幂等性概念"><a href="#（1）幂等性概念" class="headerlink" title="（1）幂等性概念"></a>（1）幂等性概念</h3><p>所谓幂等性就是无论多少次操作和第一次的操作结果一样。如果消息被多次消费，很有可能造成数据的不一致。而如果消息不可避免地被消费多次，如果我们开发人员能通过技术手段保证数据的前后一致性，那也是可以接受的 。</p><p><code>RabbitMQ</code>、<code>RocketMQ</code>、<code>Kafka</code> 消息队列中间件都有可能出现消息重复消费问题。这种问题并不是 MQ 自己保证的，而是需要开发人员来保证。</p><p>这几款消息队列中间都是是全球最牛的分布式消息队列，那肯定考虑到了消息的幂等性。我们以 Kafka 为例，看看 Kafka 是怎么保证消息队列的幂等性。</p><p>Kafka 有一个 <code>偏移量</code> 的概念，代表着消息的序号，每条消息写到消息队列都会有一个偏移量，消费者消费了数据之后，每过一段固定的时间，就会把消费过的消息的偏移量提交一下，表示已经消费过了，下次消费就从偏移量后面开始消费。</p><h3 id="（2）避坑指南"><a href="#（2）避坑指南" class="headerlink" title="（2）避坑指南"></a>（2）避坑指南</h3><p>微信支付结果通知场景</p><ul><li>微信官方文档上提到微信支付通知结果可能会推送多次，需要开发者自行保证幂等性。第一次我们可以直接修改订单状态（如支付中 -&gt; 支付成功），第二次就根据订单状态来判断，如果不是支付中，则不进行订单处理逻辑。</li></ul><p>插入数据库场景</p><ul><li>每次插入数据时，先检查下数据库中是否有这条数据的主键 id，如果有，则进行更新操作。</li></ul><p>写 Redis 场景</p><ul><li>Redis 的 <code>Set</code> 操作天然幂等性，所以不用考虑 Redis 写数据的问题。</li></ul><p>其他场景方案</p><ul><li>生产者发送每条数据时，增加一个全局唯一 id，类似订单 id。每次消费时，先去 Redis 查下是否有这个 id，如果没有，则进行正常处理消息，且将 id 存到 Redis。如果查到有这个 id，说明之前消费过，则不要进行重复处理这条消息。</li><li>不同业务场景，可能会有不同的幂等性方案，大家选择合适的即可，上面的几种方案只是提供常见的解决思路。</li></ul><h2 id="2-消息队列的坑之消息丢失"><a href="#2-消息队列的坑之消息丢失" class="headerlink" title="2.消息队列的坑之消息丢失"></a>2.消息队列的坑之消息丢失</h2><blockquote><p> 消息丢失会带来什么问题？如果是订单下单、支付结果通知、扣费相关的消息丢失，则可能造成财务损失，如果量很大，就会给甲方带来巨大损失</p></blockquote><h3 id="（1）生产者存放消息的过程中丢失消息"><a href="#（1）生产者存放消息的过程中丢失消息" class="headerlink" title="（1）生产者存放消息的过程中丢失消息"></a>（1）生产者存放消息的过程中丢失消息</h3><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul><li>事务机制（不推荐，异步方式）</li></ul><p>对于 RabbitMQ 来说，生产者发送数据之前开启 RabbitMQ 的<strong>事务机制</strong><code>channel.txselect</code> ，如果消息没有进队列，则生产者受到异常报错，并进行回滚 <code>channel.txRollback</code>，然后重试发送消息；如果收到了消息，则可以提交事务 <code>channel.txCommit</code>。但这是一个同步的操作，会影响性能。</p><ul><li>confirm 机制（推荐，异步方式）</li></ul><p>我们可以采用另外一种模式： <code>confirm</code> 模式来解决同步机制的性能问题。每次生产者发送的消息都会分配一个唯一的 id，如果写入到了 RabbitMQ 队列中，则 RabbitMQ 会回传一个 <code>ack</code> 消息，说明这个消息接收成功。如果 RabbitMQ 没能处理这个消息，则回调 <code>nack</code> 接口。说明需要重试发送消息。</p><p>也可以自定义超时时间 + 消息 id 来实现超时等待后重试机制。但可能出现的问题是调用 ack 接口时失败了，所以会出现消息被发送两次的问题，这个时候就需要保证消费者消费消息的幂等性。</p><h4 id="事务模式-和-confirm-模式的区别："><a href="#事务模式-和-confirm-模式的区别：" class="headerlink" title="事务模式 和 confirm 模式的区别："></a><code>事务模式</code> 和 <code>confirm</code> 模式的区别：</h4><ul><li>事务机制是同步的，提交事务后悔被<strong>阻塞</strong>直到提交事务完成后。</li><li>confirm 模式异步接收通知，但可能<strong>接收不到通知</strong>。需要考虑接收不到通知的场景。</li></ul><h3 id="（2）消息队列丢失消息"><a href="#（2）消息队列丢失消息" class="headerlink" title="（2）消息队列丢失消息"></a>（2）消息队列丢失消息</h3><p>消息队列的消息可以放到内存中，或将内存中的消息转到硬盘（比如数据库）中，一般都是内存和硬盘中都存有消息。如果只是放在内存中，那么当机器重启了，消息就全部丢失了。如果是硬盘中，则可能存在一种极端情况，就是将内存中的数据转换到硬盘的期间中，消息队列出问题了，未能将消息持久化到硬盘。</p><p><strong>解决方案</strong></p><ul><li>创建 <code>Queue</code> 的时候将其设置为持久化。这个地方没搞懂，欢迎探讨解答。</li><li>发送消息的时候将消息的 <code>deliveryMode</code> 设置为 2 。</li><li>开启生产者 <code>confirm</code> 模式，可以重试发送消息。</li></ul><h3 id="（3）消费者丢失消息"><a href="#（3）消费者丢失消息" class="headerlink" title="（3）消费者丢失消息"></a>（3）消费者丢失消息</h3><p>消费者刚拿到数据，还没开始处理消息，结果进程因为异常退出了，消费者没有机会再次拿到消息。</p><p><strong>解决方案</strong></p><ul><li>关闭 RabbitMQ 的自动 <code>ack</code>，每次生产者将消息写入消息队列后，就自动回传一个 <code>ack</code> 给生产者。</li><li>消费者处理完消息再主动 <code>ack</code>，告诉消息队列我处理完了。</li></ul><p><strong>问题：</strong> 那这种主动 <code>ack</code> 有什么漏洞了？如果 主动 <code>ack</code> 的时候挂了，怎么办？</p><p>则可能会被再次消费，这个时候就需要幂等处理了。</p><p><strong>问题：</strong> 如果这条消息一直被重复消费怎么办？</p><p>则需要有加上重试次数的监测，如果超过一定次数则将消息丢失，记录到异常表或发送异常通知给值班人员。</p><h3 id="（4）RabbitMQ-消息丢失总结"><a href="#（4）RabbitMQ-消息丢失总结" class="headerlink" title="（4）RabbitMQ 消息丢失总结"></a>（4）RabbitMQ 消息丢失总结</h3><p><img src="https://pic.downk.cc/item/5f6d873a160a154a67d32cb8.png" class="lazyload placeholder" data-srcset="https://pic.downk.cc/item/5f6d873a160a154a67d32cb8.png" srcset="https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@3/img/placeholder/c617bfd2497fcea598e621413e315c368f8d8e.svg"></p><h3 id="（5）Kafka-消息丢失"><a href="#（5）Kafka-消息丢失" class="headerlink" title="（5）Kafka 消息丢失"></a>（5）Kafka 消息丢失</h3><p><strong>场景：</strong><code>Kafka</code> 的某个 broker（节点）宕机了，重新选举 leader （写入的节点）。如果 leader 挂了，follower 还有些数据未同步完，则 follower 成为 leader 后，消息队列会丢失一部分数据。</p><p><strong>解决方案</strong></p><ul><li>给 topic 设置 <code>replication.factor</code> 参数，值必须大于 1，要求每个 partition 必须有至少 2 个副本。</li><li>给 kafka 服务端设置 <code>min.insyc.replicas</code> 必须大于 1，表示一个 leader 至少一个 follower 还跟自己保持联系。</li></ul><h2 id="3-消息队列的坑之消息乱序"><a href="#3-消息队列的坑之消息乱序" class="headerlink" title="3. 消息队列的坑之消息乱序"></a>3. 消息队列的坑之消息乱序</h2><blockquote><p> 用户先下单成功，然后取消订单，如果顺序颠倒，则最后数据库里面会有一条下单成功的订单。</p></blockquote><p><strong>RabbitMQ 场景：</strong></p><ul><li>生产者向消息队列按照顺序发送了 2 条消息，消息1：增加数据 A，消息2：删除数据 A。</li><li>期望结果：数据 A 被删除。</li><li>但是如果有两个消费者，消费顺序是：消息2、消息 1。则最后结果是增加了数据 A。</li></ul><p><strong>RabbitMQ 解决方案：</strong></p><ul><li>将 Queue 进行拆分，创建多个内存 Queue，消息 1 和 消息 2 进入同一个 Queue。</li><li>创建多个消费者，每一个消费者对应一个 Queue。</li></ul><p><strong>Kafka 场景：</strong></p><ul><li>创建了 topic，有 3 个 partition。</li><li>创建一条订单记录，订单 id 作为 key，订单相关的消息都丢到同一个 partition 中，同一个生产者创建的消息，顺序是正确的。</li><li>为了快速消费消息，会创建多个消费者去处理消息，而为了提高效率，每个消费者可能会创建多个线程来并行的去拿消息及处理消息，处理消息的顺序可能就乱序了。</li></ul><p><strong>Kafka 解决方案：</strong></p><ul><li>解决方案和 RabbitMQ 类似，利用多个 内存 Queue，每个线程消费 1个 Queue。</li><li>具有相同 key 的消息 进同一个 Queue。</li></ul><h2 id="4-消息队列的坑之消息积压"><a href="#4-消息队列的坑之消息积压" class="headerlink" title="4. 消息队列的坑之消息积压"></a>4. 消息队列的坑之消息积压</h2><p>消息积压：消息队列里面有很多消息来不及消费。</p><p><strong>场景 1：</strong> 消费端出了问题，比如消费者都挂了，没有消费者来消费了，导致消息在队列里面不断积压。</p><p><strong>场景 2：</strong> 消费端出了问题，比如消费者消费的速度太慢了，导致消息不断积压。</p><blockquote><p> 比如线上正在做订单活动，下单全部走消息队列，如果消息不断积压，订单都没有下单成功 ，那么会造成很大的损失</p></blockquote><p>解决方案：<strong>解铃还须系铃人</strong></p><ul><li>修复代码层面消费者的问题，确保后续消费速度恢复或尽可能加快消费的速度。</li><li>停掉现有的消费者。</li><li>临时建立好原先 5 倍的 Queue 数量。</li><li>临时建立好原先 5 倍数量的 消费者。</li><li>将堆积的消息全部转入临时的 Queue，消费者来消费这些 Queue。</li></ul><h2 id="5-消息队列的坑之消息过期失效"><a href="#5-消息队列的坑之消息过期失效" class="headerlink" title="5. 消息队列的坑之消息过期失效"></a>5. 消息队列的坑之消息过期失效</h2><blockquote><p> RabbitMQ 可以设置过期时间，如果消息超过一定的时间还没有被消费，则会被 RabbitMQ 给清理掉。消息就丢失了</p></blockquote><p>解决方案：</p><ul><li>准备好批量重导的程序</li><li>手动将消息闲时批量重导</li></ul><h2 id="6-消息队列的坑之队列写满"><a href="#6-消息队列的坑之队列写满" class="headerlink" title="6. 消息队列的坑之队列写满"></a>6. 消息队列的坑之队列写满</h2><blockquote><p> 当消息队列因消息积压导致的队列快写满，所以不能接收更多的消息了。生产者生产的消息将被丢弃。</p></blockquote><p>解决方案：</p><ul><li>判断哪些是无用的消息，RabbitMQ 可以进行 <code>Purge Message</code> 操作。</li><li>如果是有用的消息，则需要将消息快速消费，将消息里面的内容转存到数据库。</li><li>准备好程序将转存在数据库中的消息再次重导到消息队列。</li><li>闲时重导消息到消息队列。</li></ul><p>原文链接：<a href="https://www.cnblogs.com/jackson0714/p/fenbushi.html">https://www.cnblogs.com/jackson0714/p/fenbushi.html</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
