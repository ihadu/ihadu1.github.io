{"meta":{"title":"ihadu","subtitle":"","description":"","author":"oicio","url":"https://www.ihadyou.cn","root":"/"},"pages":[{"title":"Page","date":"2013-12-26T14:52:56.000Z","updated":"2021-09-23T01:18:46.483Z","comments":true,"path":"page/index.html","permalink":"https://www.ihadyou.cn/page/index.html","excerpt":"","text":"This is a page test."},{"title":"categories","date":"2021-09-26T03:58:00.000Z","updated":"2021-09-26T03:58:00.456Z","comments":true,"path":"categories/index.html","permalink":"https://www.ihadyou.cn/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-09-26T03:58:38.000Z","updated":"2021-09-26T03:58:38.470Z","comments":true,"path":"tags/index.html","permalink":"https://www.ihadyou.cn/tags/index.html","excerpt":"","text":""},{"title":"friends","date":"2021-09-26T04:00:30.000Z","updated":"2021-09-26T04:00:30.426Z","comments":true,"path":"friends/index.html","permalink":"https://www.ihadyou.cn/friends/index.html","excerpt":"","text":""},{"title":"about","date":"2021-09-26T04:00:52.000Z","updated":"2021-10-08T03:41:05.968Z","comments":true,"path":"about/index.html","permalink":"https://www.ihadyou.cn/about/index.html","excerpt":"","text":"怎么了你累了说好的幸福呢"}],"posts":[{"title":"Hbase的SQL中间层—Phoenix","slug":"Hbase的SQL中间层—Phoenix","date":"2021-10-25T06:35:13.000Z","updated":"2021-10-25T06:38:48.705Z","comments":true,"path":"2021/10/25/Hbase的SQL中间层—Phoenix/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E7%9A%84SQL%E4%B8%AD%E9%97%B4%E5%B1%82%E2%80%94Phoenix/","excerpt":"","text":"Hbase的SQL中间层——Phoenix一、Phoenix简介Phoenix 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 Phoenix 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。Phoenix 的理念是 we put sql SQL back in NOSQL，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 Spring Data JPA 或 Mybatis 等常用的持久层框架来操作 HBase。 其次 Phoenix 的性能表现也非常优异，Phoenix 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 Phoenix 成为了 HBase 最优秀的 SQL 中间层。 二、Phoenix安装 我们可以按照官方安装说明进行安装，官方说明如下： download and expand our installation tar copy the phoenix server jar that is compatible with your HBase installation into the lib directory of every region server restart the region servers add the phoenix client jar to the classpath of your HBase client download and setup SQuirrel as your SQL client so you can issue adhoc SQL against your HBase cluster 2.1 下载并解压官方针对 Apache 版本和 CDH 版本的 HBase 均提供了安装包，按需下载即可。官方下载地址: http://phoenix.apache.org/download.html 1234# 下载wget http://mirror.bit.edu.cn/apache/phoenix/apache-phoenix-4.14.0-cdh5.14.2/bin/apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz# 解压tar tar apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz 2.2 拷贝Jar包按照官方文档的说明，需要将 phoenix server jar 添加到所有 Region Servers 的安装目录的 lib 目录下。 这里由于我搭建的是 HBase 伪集群，所以只需要拷贝到当前机器的 HBase 的 lib 目录下。如果是真实集群，则使用 scp 命令分发到所有 Region Servers 机器上。 1cp /usr/app/apache-phoenix-4.14.0-cdh5.14.2-bin/phoenix-4.14.0-cdh5.14.2-server.jar /usr/app/hbase-1.2.0-cdh5.15.2/lib 2.3 重启 Region Servers1234# 停止Hbasestop-hbase.sh# 启动Hbasestart-hbase.sh 2.4 启动Phoenix在 Phoenix 解压目录下的 bin 目录下执行如下命令，需要指定 Zookeeper 的地址： 如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则默认采用内置的 Zookeeper 服务，端口为 2181； 如果是 HBase 是集群模式并采用外置的 Zookeeper 集群，则按照自己的实际情况进行指定。 1# ./sqlline.py hadoop001:2181 2.5 启动结果启动后则进入了 Phoenix 交互式 SQL 命令行，可以使用 !table 或 !tables 查看当前所有表的信息 三、Phoenix 简单使用3.1 创建表12345CREATE TABLE IF NOT EXISTS us_population ( state CHAR(2) NOT NULL, city VARCHAR NOT NULL, population BIGINT CONSTRAINT my_pk PRIMARY KEY (state, city)); 新建的表会按照特定的规则转换为 HBase 上的表，关于表的信息，可以通过 Hbase Web UI 进行查看： ### 3.2 插入数据 Phoenix 中插入数据采用的是 UPSERT 而不是 INSERT,因为 Phoenix 并没有更新操作，插入相同主键的数据就视为更新，所以 UPSERT 就相当于 UPDATE+INSERT 12345678910UPSERT INTO us_population VALUES(&#x27;NY&#x27;,&#x27;New York&#x27;,8143197);UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;Los Angeles&#x27;,3844829);UPSERT INTO us_population VALUES(&#x27;IL&#x27;,&#x27;Chicago&#x27;,2842518);UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;Houston&#x27;,2016582);UPSERT INTO us_population VALUES(&#x27;PA&#x27;,&#x27;Philadelphia&#x27;,1463281);UPSERT INTO us_population VALUES(&#x27;AZ&#x27;,&#x27;Phoenix&#x27;,1461575);UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;San Antonio&#x27;,1256509);UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;San Diego&#x27;,1255540);UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;Dallas&#x27;,1213825);UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;San Jose&#x27;,912332); 3.3 修改数据12-- 插入主键相同的数据就视为更新UPSERT INTO us_population VALUES(&#x27;NY&#x27;,&#x27;New York&#x27;,999999); ### 3.4 删除数据 1DELETE FROM us_population WHERE city=&#x27;Dallas&#x27;; ### 3.5 查询数据 1234SELECT state as &quot;州&quot;,count(city) as &quot;市&quot;,sum(population) as &quot;热度&quot;FROM us_populationGROUP BY stateORDER BY sum(population) DESC; 3.6 退出命令1!quit 3.7 扩展从上面的操作中可以看出，Phoenix 支持大多数标准的 SQL 语法。关于 Phoenix 支持的语法、数据类型、函数、序列等详细信息，因为涉及内容很多，可以参考其官方文档，官方文档上有详细的说明： 语法 (Grammar) ：https://phoenix.apache.org/language/index.html 函数 (Functions) ：http://phoenix.apache.org/language/functions.html 数据类型 (Datatypes) ：http://phoenix.apache.org/language/datatypes.html 序列 (Sequences) :http://phoenix.apache.org/sequences.html 联结查询 (Joins) ：http://phoenix.apache.org/joins.html 四、Phoenix Java API因为 Phoenix 遵循 JDBC 规范，并提供了对应的数据库驱动 PhoenixDriver，这使得采用 Java 语言对其进行操作的时候，就如同对其他关系型数据库一样，下面给出基本的使用示例。 4.1 引入Phoenix core JAR包如果是 maven 项目，直接在 maven 中央仓库找到对应的版本，导入依赖即可： 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.14.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; 如果是普通项目，则可以从 Phoenix 解压目录下找到对应的 JAR 包，然后手动引入： ### 4.2 简单的Java API实例 1234567891011121314151617181920212223242526272829303132import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;public class PhoenixJavaApi &#123; public static void main(String[] args) throws Exception &#123; // 加载数据库驱动 Class.forName(&quot;org.apache.phoenix.jdbc.PhoenixDriver&quot;); /* * 指定数据库地址,格式为 jdbc:phoenix:Zookeeper 地址 * 如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则 HBase 默认使用内置的 Zookeeper，默认端口为 2181 */ Connection connection = DriverManager.getConnection(&quot;jdbc:phoenix:192.168.200.226:2181&quot;); PreparedStatement statement = connection.prepareStatement(&quot;SELECT * FROM us_population&quot;); ResultSet resultSet = statement.executeQuery(); while (resultSet.next()) &#123; System.out.println(resultSet.getString(&quot;city&quot;) + &quot; &quot; + resultSet.getInt(&quot;population&quot;)); &#125; statement.close(); connection.close(); &#125;&#125; 结果如下： 实际的开发中我们通常都是采用第三方框架来操作数据库，如 mybatis，Hibernate，Spring Data 等。关于 Phoenix 与这些框架的整合步骤参见下一篇文章：Spring/Spring Boot + Mybatis + Phoenix 参考资料 http://phoenix.apache.org/","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase容灾与备份","slug":"Hbase容灾与备份","date":"2021-10-25T06:33:53.000Z","updated":"2021-10-25T06:39:09.597Z","comments":true,"path":"2021/10/25/Hbase容灾与备份/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E5%AE%B9%E7%81%BE%E4%B8%8E%E5%A4%87%E4%BB%BD/","excerpt":"","text":"Hbase容灾与备份一、前言本文主要介绍 Hbase 常用的三种简单的容灾备份方案，即CopyTable、Export/Import、Snapshot。分别介绍如下： 二、CopyTable2.1 简介CopyTable可以将现有表的数据复制到新表中，具有以下特点： 支持时间区间 、row 区间 、改变表名称 、改变列族名称 、以及是否 Copy 已被删除的数据等功能； 执行命令前，需先创建与原表结构相同的新表； CopyTable 的操作是基于 HBase Client API 进行的，即采用 scan 进行查询, 采用 put 进行写入。 2.2 命令格式1Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt; 2.3 常用命令 同集群下 CopyTable 1hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=tableCopy tableOrig 不同集群下 CopyTable 12345678# 两表名称相同的情况hbase org.apache.hadoop.hbase.mapreduce.CopyTable \\--peer.adr=dstClusterZK:2181:/hbase tableOrig# 也可以指新的表名hbase org.apache.hadoop.hbase.mapreduce.CopyTable \\--peer.adr=dstClusterZK:2181:/hbase \\--new.name=tableCopy tableOrig 下面是一个官方给的比较完整的例子，指定开始和结束时间，集群地址，以及只复制指定的列族： 12345hbase org.apache.hadoop.hbase.mapreduce.CopyTable \\--starttime=1265875194289 \\--endtime=1265878794289 \\--peer.adr=server1,server2,server3:2181:/hbase \\--families=myOldCf:myNewCf,cf2,cf3 TestTable 2.4 更多参数可以通过 --help 查看更多支持的参数 1# hbase org.apache.hadoop.hbase.mapreduce.CopyTable --help 三、Export/Import3.1 简介 Export 支持导出数据到 HDFS, Import 支持从 HDFS 导入数据。Export 还支持指定导出数据的开始时间和结束时间，因此可以用于增量备份。 Export 导出与 CopyTable 一样，依赖 HBase 的 scan 操作 3.2 命令格式12345# Exporthbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]# Inporthbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt; 导出的 outputdir 目录可以不用预先创建，程序会自动创建。导出完成后，导出文件的所有权将由执行导出命令的用户所拥有。 默认情况下，仅导出给定 Cell 的最新版本，而不管历史版本。要导出多个版本，需要将 &lt;versions&gt; 参数替换为所需的版本数。 3.3 常用命令 导出命令 1hbase org.apache.hadoop.hbase.mapreduce.Export tableName hdfs 路径/tableName.db 导入命令 1hbase org.apache.hadoop.hbase.mapreduce.Import tableName hdfs 路径/tableName.db 四、Snapshot4.1 简介HBase 的快照 (Snapshot) 功能允许您获取表的副本 (包括内容和元数据)，并且性能开销很小。因为快照存储的仅仅是表的元数据和 HFiles 的信息。快照的 clone 操作会从该快照创建新表，快照的 restore 操作会将表的内容还原到快照节点。clone 和 restore 操作不需要复制任何数据，因为底层 HFiles(包含 HBase 表数据的文件) 不会被修改，修改的只是表的元数据信息。 4.2 配置HBase 快照功能默认没有开启，如果要开启快照，需要在 hbase-site.xml 文件中添加如下配置项： 1234&lt;property&gt; &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 4.3 常用命令快照的所有命令都需要在 Hbase Shell 交互式命令行中执行。 1. Take a Snapshot12# 拍摄快照hbase&gt; snapshot &#x27;表名&#x27;, &#x27;快照名&#x27; 默认情况下拍摄快照之前会在内存中执行数据刷新。以保证内存中的数据包含在快照中。但是如果你不希望包含内存中的数据，则可以使用 SKIP_FLUSH 选项禁止刷新。 12# 禁止内存刷新hbase&gt; snapshot &#x27;表名&#x27;, &#x27;快照名&#x27;, &#123;SKIP_FLUSH =&gt; true&#125; 2. Listing Snapshots12# 获取快照列表hbase&gt; list_snapshots 3. Deleting Snapshots12# 删除快照hbase&gt; delete_snapshot &#x27;快照名&#x27; 4. Clone a table from snapshot12# 从现有的快照创建一张新表hbase&gt; clone_snapshot &#x27;快照名&#x27;, &#x27;新表名&#x27; 5. Restore a snapshot将表恢复到快照节点，恢复操作需要先禁用表 12hbase&gt; disable &#x27;表名&#x27;hbase&gt; restore_snapshot &#x27;快照名&#x27; 这里需要注意的是：是如果 HBase 配置了基于 Replication 的主从复制，由于 Replication 在日志级别工作，而快照在文件系统级别工作，因此在还原之后，会出现副本与主服务器处于不同的状态的情况。这时候可以先停止同步，所有服务器还原到一致的数据点后再重新建立同步。 参考资料 Online Apache HBase Backups with CopyTable Apache HBase ™ Reference Guide","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase协处理器","slug":"Hbase协处理器","date":"2021-10-25T06:31:06.000Z","updated":"2021-10-25T06:39:15.391Z","comments":true,"path":"2021/10/25/Hbase协处理器/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8/","excerpt":"","text":"Hbase 协处理器一、简述在使用 HBase 时，如果你的数据量达到了数十亿行或数百万列，此时能否在查询中返回大量数据将受制于网络的带宽，即便网络状况允许，但是客户端的计算处理也未必能够满足要求。在这种情况下，协处理器（Coprocessors）应运而生。它允许你将业务计算代码放入在 RegionServer 的协处理器中，将处理好的数据再返回给客户端，这可以极大地降低需要传输的数据量，从而获得性能上的提升。同时协处理器也允许用户扩展实现 HBase 目前所不具备的功能，如权限校验、二级索引、完整性约束等。 二、协处理器类型2.1 Observer协处理器1. 功能Observer 协处理器类似于关系型数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。通常可以用来实现下面功能： 权限校验：在执行 Get 或 Put 操作之前，您可以使用 preGet 或 prePut 方法检查权限； 完整性约束： HBase 不支持关系型数据库中的外键功能，可以通过触发器在插入或者删除数据的时候，对关联的数据进行检查； 二级索引： 可以使用协处理器来维护二级索引。 2. 类型当前 Observer 协处理器有以下四种类型： RegionObserver :允许您观察 Region 上的事件，例如 Get 和 Put 操作。 RegionServerObserver :允许您观察与 RegionServer 操作相关的事件，例如启动，停止或执行合并，提交或回滚。 MasterObserver :允许您观察与 HBase Master 相关的事件，例如表创建，删除或 schema 修改。 WalObserver :允许您观察与预写日志（WAL）相关的事件。 3. 接口以上四种类型的 Observer 协处理器均继承自 Coprocessor 接口，这四个接口中分别定义了所有可用的钩子方法，以便在对应方法前后执行特定的操作。通常情况下，我们并不会直接实现上面接口，而是继承其 Base 实现类，Base 实现类只是简单空实现了接口中的方法，这样我们在实现自定义的协处理器时，就不必实现所有方法，只需要重写必要方法即可。 这里以 RegionObservers 为例，其接口类中定义了所有可用的钩子方法，下面截取了部分方法的定义，多数方法都是成对出现的，有 pre 就有 post： 4. 执行流程 客户端发出 put 请求 该请求被分派给合适的 RegionServer 和 region coprocessorHost 拦截该请求，然后在该表的每个 RegionObserver 上调用 prePut() 如果没有被 prePut() 拦截，该请求继续送到 region，然后进行处理 region 产生的结果再次被 CoprocessorHost 拦截，调用 postPut() 假如没有 postPut() 拦截该响应，最终结果被返回给客户端 如果大家了解 Spring，可以将这种执行方式类比于其 AOP 的执行原理即可，官方文档当中也是这样类比的： If you are familiar with Aspect Oriented Programming (AOP), you can think of a coprocessor as applying advice by intercepting a request and then running some custom code,before passing the request on to its final destination (or even changing the destination). 如果您熟悉面向切面编程（AOP），您可以将协处理器视为通过拦截请求然后运行一些自定义代码来使用 Advice，然后将请求传递到其最终目标（或者更改目标）。 2.2 Endpoint协处理器Endpoint 协处理器类似于关系型数据库中的存储过程。客户端可以调用 Endpoint 协处理器在服务端对数据进行处理，然后再返回。 以聚集操作为例，如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，然后在客户端上遍历扫描结果，这必然会加重了客户端处理数据的压力。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出来，仅仅将该 max 值返回给客户端。之后客户端只需要将每个 Region 的最大值进行比较而找到其中最大的值即可。 三、协处理的加载方式要使用我们自己开发的协处理器，必须通过静态（使用 HBase 配置）或动态（使用 HBase Shell 或 Java API）加载它。 静态加载的协处理器称之为 System Coprocessor（系统级协处理器）,作用范围是整个 HBase 上的所有表，需要重启 HBase 服务； 动态加载的协处理器称之为 Table Coprocessor（表处理器），作用于指定的表，不需要重启 HBase 服务。 其加载和卸载方式分别介绍如下。 四、静态加载与卸载4.1 静态加载静态加载分以下三步： 在 hbase-site.xml 定义需要加载的协处理器。 1234&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;org.myname.hbase.coprocessor.endpoint.SumEndPoint&lt;/value&gt;&lt;/property&gt; &lt;name&gt; 标签的值必须是下面其中之一： RegionObservers 和 Endpoints 协处理器：hbase.coprocessor.region.classes WALObservers 协处理器： hbase.coprocessor.wal.classes MasterObservers 协处理器：hbase.coprocessor.master.classes &lt;value&gt; 必须是协处理器实现类的全限定类名。如果为加载指定了多个类，则类名必须以逗号分隔。 将 jar(包含代码和所有依赖项) 放入 HBase 安装目录中的 lib 目录下； 重启 HBase。 4.2 静态卸载 从 hbase-site.xml 中删除配置的协处理器的&lt;property&gt;元素及其子元素； 从类路径或 HBase 的 lib 目录中删除协处理器的 JAR 文件（可选）； 重启 HBase。 五、动态加载与卸载使用动态加载协处理器，不需要重新启动 HBase。但动态加载的协处理器是基于每个表加载的，只能用于所指定的表。此外，在使用动态加载必须使表脱机（disable）以加载协处理器。动态加载通常有两种方式：Shell 和 Java API 。 以下示例基于两个前提： coprocessor.jar 包含协处理器实现及其所有依赖项。 JAR 包存放在 HDFS 上的路径为：hdfs：// &lt;namenode&gt;：&lt;port&gt; / user / &lt;hadoop-user&gt; /coprocessor.jar 5.1 HBase Shell动态加载 使用 HBase Shell 禁用表 1hbase &gt; disable &#x27;tableName&#x27; 使用如下命令加载协处理器 123hbase &gt; alter &#x27;tableName&#x27;, METHOD =&gt; &#x27;table_att&#x27;, &#x27;Coprocessor&#x27;=&gt;&#x27;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar| org.myname.hbase.Coprocessor.RegionObserverExample|1073741823|arg1=1,arg2=2&#x27; Coprocessor 包含由管道（|）字符分隔的四个参数，按顺序解释如下： JAR 包路径：通常为 JAR 包在 HDFS 上的路径。关于路径以下两点需要注意： 允许使用通配符，例如：hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/*.jar 来添加指定的 JAR 包； 可以使指定目录，例如：hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/ ，这会添加目录中的所有 JAR 包，但不会搜索子目录中的 JAR 包。 类名：协处理器的完整类名。 优先级：协处理器的优先级，遵循数字的自然序，即值越小优先级越高。可以为空，在这种情况下，将分配默认优先级值。 可选参数 ：传递的协处理器的可选参数。 启用表 1hbase &gt; enable &#x27;tableName&#x27; 验证协处理器是否已加载 1hbase &gt; describe &#x27;tableName&#x27; 协处理器出现在 TABLE_ATTRIBUTES 属性中则代表加载成功。 5.2 HBase Shell动态卸载 禁用表 1hbase&gt; disable &#x27;tableName&#x27; 移除表协处理器 1hbase&gt; alter &#x27;tableName&#x27;, METHOD =&gt; &#x27;table_att_unset&#x27;, NAME =&gt; &#x27;coprocessor$1&#x27; 启用表 1hbase&gt; enable &#x27;tableName&#x27; 5.3 Java API 动态加载123456789101112131415161718TableName tableName = TableName.valueOf(&quot;users&quot;);String path = &quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;;Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Admin admin = connection.getAdmin();admin.disableTable(tableName);HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);HColumnDescriptor columnFamily1 = new HColumnDescriptor(&quot;personalDet&quot;);columnFamily1.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily1);HColumnDescriptor columnFamily2 = new HColumnDescriptor(&quot;salaryDet&quot;);columnFamily2.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily2);hTableDescriptor.setValue(&quot;COPROCESSOR$1&quot;, path + &quot;|&quot;+ RegionObserverExample.class.getCanonicalName() + &quot;|&quot;+ Coprocessor.PRIORITY_USER);admin.modifyTable(tableName, hTableDescriptor);admin.enableTable(tableName); 在 HBase 0.96 及其以后版本中，HTableDescriptor 的 addCoprocessor() 方法提供了一种更为简便的加载方法。 1234567891011121314151617TableName tableName = TableName.valueOf(&quot;users&quot;);Path path = new Path(&quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;);Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Admin admin = connection.getAdmin();admin.disableTable(tableName);HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);HColumnDescriptor columnFamily1 = new HColumnDescriptor(&quot;personalDet&quot;);columnFamily1.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily1);HColumnDescriptor columnFamily2 = new HColumnDescriptor(&quot;salaryDet&quot;);columnFamily2.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily2);hTableDescriptor.addCoprocessor(RegionObserverExample.class.getCanonicalName(), path,Coprocessor.PRIORITY_USER, null);admin.modifyTable(tableName, hTableDescriptor);admin.enableTable(tableName); 5.4 Java API 动态卸载卸载其实就是重新定义表但不设置协处理器。这会删除所有表上的协处理器。 123456789101112131415TableName tableName = TableName.valueOf(&quot;users&quot;);String path = &quot;hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar&quot;;Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Admin admin = connection.getAdmin();admin.disableTable(tableName);HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);HColumnDescriptor columnFamily1 = new HColumnDescriptor(&quot;personalDet&quot;);columnFamily1.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily1);HColumnDescriptor columnFamily2 = new HColumnDescriptor(&quot;salaryDet&quot;);columnFamily2.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily2);admin.modifyTable(tableName, hTableDescriptor);admin.enableTable(tableName); 六、协处理器案例这里给出一个简单的案例，实现一个类似于 Redis 中 append 命令的协处理器，当我们对已有列执行 put 操作时候，HBase 默认执行的是 update 操作，这里我们修改为执行 append 操作。 123456789# redis append 命令示例redis&gt; EXISTS mykey(integer) 0redis&gt; APPEND mykey &quot;Hello&quot;(integer) 5redis&gt; APPEND mykey &quot; World&quot;(integer) 11redis&gt; GET mykey &quot;Hello World&quot; 6.1 创建测试表12# 创建一张杂志表 有文章和图片两个列族hbase &gt; create &#x27;magazine&#x27;,&#x27;article&#x27;,&#x27;picture&#x27; 6.2 协处理器编程 完整代码可见本仓库：hbase-observer-coprocessor 新建 Maven 工程，导入下面依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-common&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt; 继承 BaseRegionObserver 实现我们自定义的 RegionObserver,对相同的 article:content 执行 put 命令时，将新插入的内容添加到原有内容的末尾，代码如下： 12345678910111213141516171819202122232425262728293031public class AppendRegionObserver extends BaseRegionObserver &#123; private byte[] columnFamily = Bytes.toBytes(&quot;article&quot;); private byte[] qualifier = Bytes.toBytes(&quot;content&quot;); @Override public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123; if (put.has(columnFamily, qualifier)) &#123; // 遍历查询结果，获取指定列的原值 Result rs = e.getEnvironment().getRegion().get(new Get(put.getRow())); String oldValue = &quot;&quot;; for (Cell cell : rs.rawCells()) if (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123; oldValue = Bytes.toString(CellUtil.cloneValue(cell)); &#125; // 获取指定列新插入的值 List&lt;Cell&gt; cells = put.get(columnFamily, qualifier); String newValue = &quot;&quot;; for (Cell cell : cells) &#123; if (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123; newValue = Bytes.toString(CellUtil.cloneValue(cell)); &#125; &#125; // Append 操作 put.addColumn(columnFamily, qualifier, Bytes.toBytes(oldValue + newValue)); &#125; &#125;&#125; 6.3 打包项目使用 maven 命令进行打包，打包后的文件名为 hbase-observer-coprocessor-1.0-SNAPSHOT.jar 1# mvn clean package 6.4 上传JAR包到HDFS1234# 上传项目到HDFS上的hbase目录hadoop fs -put /usr/app/hbase-observer-coprocessor-1.0-SNAPSHOT.jar /hbase# 查看上传是否成功hadoop fs -ls /hbase 6.5 加载协处理器 加载协处理器前需要先禁用表 1hbase &gt; disable &#x27;magazine&#x27; 加载协处理器 1hbase &gt; alter &#x27;magazine&#x27;, METHOD =&gt; &#x27;table_att&#x27;, &#x27;Coprocessor&#x27;=&gt;&#x27;hdfs://hadoop001:8020/hbase/hbase-observer-coprocessor-1.0-SNAPSHOT.jar|com.ihadyou.AppendRegionObserver|1001|&#x27; 启用表 1hbase &gt; enable &#x27;magazine&#x27; 查看协处理器是否加载成功 1hbase &gt; desc &#x27;magazine&#x27; 协处理器出现在 TABLE_ATTRIBUTES 属性中则代表加载成功，如下图： 6.6 测试加载结果插入一组测试数据： 1234hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;Hello&#x27;hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;World&#x27;hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27; 可以看到对于指定列的值已经执行了 append 操作： 插入一组对照数据： 1234hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:author&#x27;,&#x27;zhangsan&#x27;hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:author&#x27;hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:author&#x27;,&#x27;lisi&#x27;hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:author&#x27; 可以看到对于正常的列还是执行 update 操作: 6.7 卸载协处理器 卸载协处理器前需要先禁用表 1hbase &gt; disable &#x27;magazine&#x27; 卸载协处理器 1hbase &gt; alter &#x27;magazine&#x27;, METHOD =&gt; &#x27;table_att_unset&#x27;, NAME =&gt; &#x27;coprocessor$1&#x27; 启用表 1hbase &gt; enable &#x27;magazine&#x27; 查看协处理器是否卸载成功 1hbase &gt; desc &#x27;magazine&#x27; 6.8 测试卸载结果依次执行下面命令可以测试卸载是否成功 123hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27;hbase &gt; put &#x27;magazine&#x27;, &#x27;rowkey1&#x27;,&#x27;article:content&#x27;,&#x27;Hello&#x27;hbase &gt; get &#x27;magazine&#x27;,&#x27;rowkey1&#x27;,&#x27;article:content&#x27; 参考资料 Apache HBase Coprocessors Apache HBase Coprocessor Introduction HBase 高階知識","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase过滤器","slug":"Hbase过滤器","date":"2021-10-25T06:29:38.000Z","updated":"2021-10-25T06:38:44.535Z","comments":true,"path":"2021/10/25/Hbase过滤器/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E8%BF%87%E6%BB%A4%E5%99%A8/","excerpt":"","text":"Hbase 过滤器详解一、HBase过滤器简介Hbase 提供了种类丰富的过滤器（filter）来提高数据处理的效率，用户可以通过内置或自定义的过滤器来对数据进行过滤，所有的过滤器都在服务端生效，即谓词下推（predicate push down）。这样可以保证过滤掉的数据不会被传送到客户端，从而减轻网络传输和客户端处理的压力。 二、过滤器基础2.1 Filter接口和FilterBase抽象类Filter 接口中定义了过滤器的基本方法，FilterBase 抽象类实现了 Filter 接口。所有内置的过滤器则直接或者间接继承自 FilterBase 抽象类。用户只需要将定义好的过滤器通过 setFilter 方法传递给 Scan 或 put 的实例即可。 1setFilter(Filter filter) 123456// Scan 中定义的 setFilter@Override public Scan setFilter(Filter filter) &#123; super.setFilter(filter); return this; &#125; 123456 // Get 中定义的 setFilter@Override public Get setFilter(Filter filter) &#123; super.setFilter(filter); return this; &#125; FilterBase 的所有子类过滤器如下： 说明：上图基于当前时间点（2019.4）最新的 Hbase-2.1.4 ，下文所有说明均基于此版本。 2.2 过滤器分类HBase 内置过滤器可以分为三类：分别是比较过滤器，专用过滤器和包装过滤器。分别在下面的三个小节中做详细的介绍。 三、比较过滤器所有比较过滤器均继承自 CompareFilter。创建一个比较过滤器需要两个参数，分别是比较运算符和比较器实例。 1234public CompareFilter(final CompareOp compareOp,final ByteArrayComparable comparator) &#123; this.compareOp = compareOp; this.comparator = comparator; &#125; 3.1 比较运算符 LESS (&lt;) LESS_OR_EQUAL (&lt;=) EQUAL (=) NOT_EQUAL (!=) GREATER_OR_EQUAL (&gt;=) GREATER (&gt;) NO_OP (排除所有符合条件的值) 比较运算符均定义在枚举类 CompareOperator 中 12345678910@InterfaceAudience.Publicpublic enum CompareOperator &#123; LESS, LESS_OR_EQUAL, EQUAL, NOT_EQUAL, GREATER_OR_EQUAL, GREATER, NO_OP,&#125; 注意：在 1.x 版本的 HBase 中，比较运算符定义在 CompareFilter.CompareOp 枚举类中，但在 2.0 之后这个类就被标识为 @deprecated ，并会在 3.0 移除。所以 2.0 之后版本的 HBase 需要使用 CompareOperator 这个枚举类。 3.2 比较器所有比较器均继承自 ByteArrayComparable 抽象类，常用的有以下几种： BinaryComparator : 使用 Bytes.compareTo(byte []，byte []) 按字典序比较指定的字节数组。 BinaryPrefixComparator : 按字典序与指定的字节数组进行比较，但只比较到这个字节数组的长度。 RegexStringComparator : 使用给定的正则表达式与指定的字节数组进行比较。仅支持 EQUAL 和 NOT_EQUAL 操作。 SubStringComparator : 测试给定的子字符串是否出现在指定的字节数组中，比较不区分大小写。仅支持 EQUAL 和 NOT_EQUAL 操作。 NullComparator ：判断给定的值是否为空。 BitComparator ：按位进行比较。 BinaryPrefixComparator 和 BinaryComparator 的区别不是很好理解，这里举例说明一下： 在进行 EQUAL 的比较时，如果比较器传入的是 abcd 的字节数组，但是待比较数据是 abcdefgh： 如果使用的是 BinaryPrefixComparator 比较器，则比较以 abcd 字节数组的长度为准，即 efgh 不会参与比较，这时候认为 abcd 与 abcdefgh 是满足 EQUAL 条件的； 如果使用的是 BinaryComparator 比较器，则认为其是不相等的。 3.3 比较过滤器种类比较过滤器共有五个（Hbase 1.x 版本和 2.x 版本相同），见下图： RowFilter ：基于行键来过滤数据； FamilyFilterr ：基于列族来过滤数据； QualifierFilterr ：基于列限定符（列名）来过滤数据； ValueFilterr ：基于单元格 (cell) 的值来过滤数据； DependentColumnFilter ：指定一个参考列来过滤其他列的过滤器，过滤的原则是基于参考列的时间戳来进行筛选 。 前四种过滤器的使用方法相同，均只要传递比较运算符和运算器实例即可构建，然后通过 setFilter 方法传递给 scan： 123Filter filter = new RowFilter(CompareOperator.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;xxx&quot;))); scan.setFilter(filter); DependentColumnFilter 的使用稍微复杂一点，这里单独做下说明。 3.4 DependentColumnFilter可以把 DependentColumnFilter 理解为一个 valueFilter 和一个时间戳过滤器的组合。DependentColumnFilter 有三个带参构造器，这里选择一个参数最全的进行说明： 123DependentColumnFilter(final byte [] family, final byte[] qualifier, final boolean dropDependentColumn, final CompareOperator op, final ByteArrayComparable valueComparator) family ：列族 qualifier ：列限定符（列名） dropDependentColumn ：决定参考列是否被包含在返回结果内，为 true 时表示参考列被返回，为 false 时表示被丢弃 op ：比较运算符 valueComparator ：比较器 这里举例进行说明： 123456DependentColumnFilter dependentColumnFilter = new DependentColumnFilter( Bytes.toBytes(&quot;student&quot;), Bytes.toBytes(&quot;name&quot;), false, CompareOperator.EQUAL, new BinaryPrefixComparator(Bytes.toBytes(&quot;xiaolan&quot;))); 首先会去查找 student:name 中值以 xiaolan 开头的所有数据获得 参考数据集，这一步等同于 valueFilter 过滤器； 其次再用参考数据集中所有数据的时间戳去检索其他列，获得时间戳相同的其他列的数据作为 结果数据集，这一步等同于时间戳过滤器； 最后如果 dropDependentColumn 为 true，则返回 参考数据集+结果数据集，若为 false，则抛弃参考数据集，只返回 结果数据集。 四、专用过滤器专用过滤器通常直接继承自 FilterBase，适用于范围更小的筛选规则。 4.1 单列列值过滤器 (SingleColumnValueFilter)基于某列（参考列）的值决定某行数据是否被过滤。其实例有以下方法： setFilterIfMissing(boolean filterIfMissing) ：默认值为 false，即如果该行数据不包含参考列，其依然被包含在最后的结果中；设置为 true 时，则不包含； setLatestVersionOnly(boolean latestVersionOnly) ：默认为 true，即只检索参考列的最新版本数据；设置为 false，则检索所有版本数据。 1234567SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter( &quot;student&quot;.getBytes(), &quot;name&quot;.getBytes(), CompareOperator.EQUAL, new SubstringComparator(&quot;xiaolan&quot;));singleColumnValueFilter.setFilterIfMissing(true);scan.setFilter(singleColumnValueFilter); 4.2 单列列值排除器 (SingleColumnValueExcludeFilter)SingleColumnValueExcludeFilter 继承自上面的 SingleColumnValueFilter，过滤行为与其相反。 4.3 行键前缀过滤器 (PrefixFilter)基于 RowKey 值决定某行数据是否被过滤。 12PrefixFilter prefixFilter = new PrefixFilter(Bytes.toBytes(&quot;xxx&quot;));scan.setFilter(prefixFilter); 4.4 列名前缀过滤器 (ColumnPrefixFilter)基于列限定符（列名）决定某行数据是否被过滤。 12ColumnPrefixFilter columnPrefixFilter = new ColumnPrefixFilter(Bytes.toBytes(&quot;xxx&quot;)); scan.setFilter(columnPrefixFilter); 4.5 分页过滤器 (PageFilter)可以使用这个过滤器实现对结果按行进行分页，创建 PageFilter 实例的时候需要传入每页的行数。 1234public PageFilter(final long pageSize) &#123; Preconditions.checkArgument(pageSize &gt;= 0, &quot;must be positive %s&quot;, pageSize); this.pageSize = pageSize; &#125; 下面的代码体现了客户端实现分页查询的主要逻辑，这里对其进行一下解释说明： 客户端进行分页查询，需要传递 startRow(起始 RowKey)，知道起始 startRow 后，就可以返回对应的 pageSize 行数据。这里唯一的问题就是，对于第一次查询，显然 startRow 就是表格的第一行数据，但是之后第二次、第三次查询我们并不知道 startRow，只能知道上一次查询的最后一条数据的 RowKey（简单称之为 lastRow）。 我们不能将 lastRow 作为新一次查询的 startRow 传入，因为 scan 的查询区间是[startRow，endRow) ，即前开后闭区间，这样 startRow 在新的查询也会被返回，这条数据就重复了。 同时在不使用第三方数据库存储 RowKey 的情况下，我们是无法通过知道 lastRow 的下一个 RowKey 的，因为 RowKey 的设计可能是连续的也有可能是不连续的。 由于 Hbase 的 RowKey 是按照字典序进行排序的。这种情况下，就可以在 lastRow 后面加上 0 ，作为 startRow 传入，因为按照字典序的规则，某个值加上 0 后的新值，在字典序上一定是这个值的下一个值，对于 HBase 来说下一个 RowKey 在字典序上一定也是等于或者大于这个新值的。 所以最后传入 lastRow+0，如果等于这个值的 RowKey 存在就从这个值开始 scan,否则从字典序的下一个 RowKey 开始 scan。 25 个字母以及数字字符，字典排序如下: &#39;0&#39; &lt; &#39;1&#39; &lt; &#39;2&#39; &lt; ... &lt; &#39;9&#39; &lt; &#39;a&#39; &lt; &#39;b&#39; &lt; ... &lt; &#39;z&#39; 分页查询主要实现逻辑： 12345678910111213141516171819202122232425262728byte[] POSTFIX = new byte[] &#123; 0x00 &#125;;Filter filter = new PageFilter(15);int totalRows = 0;byte[] lastRow = null;while (true) &#123; Scan scan = new Scan(); scan.setFilter(filter); if (lastRow != null) &#123; // 如果不是首行 则 lastRow + 0 byte[] startRow = Bytes.add(lastRow, POSTFIX); System.out.println(&quot;start row: &quot; + Bytes.toStringBinary(startRow)); scan.withStartRow(startRow); &#125; ResultScanner scanner = table.getScanner(scan); int localRows = 0; Result result; while ((result = scanner.next()) != null) &#123; System.out.println(localRows++ + &quot;: &quot; + result); totalRows++; lastRow = result.getRow(); &#125; scanner.close(); //最后一页，查询结束 if (localRows == 0) break;&#125;System.out.println(&quot;total rows: &quot; + totalRows); 需要注意的是在多台 Regin Services 上执行分页过滤的时候，由于并行执行的过滤器不能共享它们的状态和边界，所以有可能每个过滤器都会在完成扫描前获取了 PageCount 行的结果，这种情况下会返回比分页条数更多的数据，分页过滤器就有失效的可能。 4.6 时间戳过滤器 (TimestampsFilter)1234List&lt;Long&gt; list = new ArrayList&lt;&gt;();list.add(1554975573000L);TimestampsFilter timestampsFilter = new TimestampsFilter(list);scan.setFilter(timestampsFilter); 4.7 首次行键过滤器 (FirstKeyOnlyFilter)FirstKeyOnlyFilter 只扫描每行的第一列，扫描完第一列后就结束对当前行的扫描，并跳转到下一行。相比于全表扫描，其性能更好，通常用于行数统计的场景，因为如果某一行存在，则行中必然至少有一列。 12FirstKeyOnlyFilter firstKeyOnlyFilter = new FirstKeyOnlyFilter();scan.set(firstKeyOnlyFilter); 五、包装过滤器包装过滤器就是通过包装其他过滤器以实现某些拓展的功能。 5.1 SkipFilter过滤器SkipFilter 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，则拓展过滤整行数据。下面是一个使用示例： 12345// 定义 ValueFilter 过滤器Filter filter1 = new ValueFilter(CompareOperator.NOT_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;xxx&quot;)));// 使用 SkipFilter 进行包装Filter filter2 = new SkipFilter(filter1); 5.2 WhileMatchFilter过滤器WhileMatchFilter 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，WhileMatchFilter 则结束本次扫描，返回已经扫描到的结果。下面是其使用示例： 1234567891011121314151617181920212223242526Filter filter1 = new RowFilter(CompareOperator.NOT_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;rowKey4&quot;)));Scan scan = new Scan();scan.setFilter(filter1);ResultScanner scanner1 = table.getScanner(scan);for (Result result : scanner1) &#123; for (Cell cell : result.listCells()) &#123; System.out.println(cell); &#125;&#125;scanner1.close();System.out.println(&quot;--------------------&quot;);// 使用 WhileMatchFilter 进行包装Filter filter2 = new WhileMatchFilter(filter1);scan.setFilter(filter2);ResultScanner scanner2 = table.getScanner(scan);for (Result result : scanner1) &#123; for (Cell cell : result.listCells()) &#123; System.out.println(cell); &#125;&#125;scanner2.close(); 1234567891011121314rowKey0/student:name/1555035006994/Put/vlen=8/seqid=0rowKey1/student:name/1555035007019/Put/vlen=8/seqid=0rowKey2/student:name/1555035007025/Put/vlen=8/seqid=0rowKey3/student:name/1555035007037/Put/vlen=8/seqid=0rowKey5/student:name/1555035007051/Put/vlen=8/seqid=0rowKey6/student:name/1555035007057/Put/vlen=8/seqid=0rowKey7/student:name/1555035007062/Put/vlen=8/seqid=0rowKey8/student:name/1555035007068/Put/vlen=8/seqid=0rowKey9/student:name/1555035007073/Put/vlen=8/seqid=0--------------------rowKey0/student:name/1555035006994/Put/vlen=8/seqid=0rowKey1/student:name/1555035007019/Put/vlen=8/seqid=0rowKey2/student:name/1555035007025/Put/vlen=8/seqid=0rowKey3/student:name/1555035007037/Put/vlen=8/seqid=0 可以看到被包装后，只返回了 rowKey4 之前的数据。 六、FilterList以上都是讲解单个过滤器的作用，当需要多个过滤器共同作用于一次查询的时候，就需要使用 FilterList。FilterList 支持通过构造器或者 addFilter 方法传入多个过滤器。 12345678// 构造器传入public FilterList(final Operator operator, final List&lt;Filter&gt; filters)public FilterList(final List&lt;Filter&gt; filters)public FilterList(final Filter... filters)// 方法传入 public void addFilter(List&lt;Filter&gt; filters) public void addFilter(Filter filter) 多个过滤器组合的结果由 operator 参数定义 ，其可选参数定义在 Operator 枚举类中。只有 MUST_PASS_ALL 和 MUST_PASS_ONE 两个可选的值： MUST_PASS_ALL ：相当于 AND，必须所有的过滤器都通过才认为通过； MUST_PASS_ONE ：相当于 OR，只有要一个过滤器通过则认为通过。 1234567@InterfaceAudience.Public public enum Operator &#123; /** !AND */ MUST_PASS_ALL, /** !OR */ MUST_PASS_ONE &#125; 使用示例如下： 123456789101112131415161718List&lt;Filter&gt; filters = new ArrayList&lt;Filter&gt;();Filter filter1 = new RowFilter(CompareOperator.GREATER_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;XXX&quot;)));filters.add(filter1);Filter filter2 = new RowFilter(CompareOperator.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;YYY&quot;)));filters.add(filter2);Filter filter3 = new QualifierFilter(CompareOperator.EQUAL, new RegexStringComparator(&quot;ZZZ&quot;));filters.add(filter3);FilterList filterList = new FilterList(filters);Scan scan = new Scan();scan.setFilter(filterList); 参考资料HBase: The Definitive Guide _&gt; Chapter 4. Client API: Advanced Features","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase JAVA API使用","slug":"Hbase JAVA API使用","date":"2021-10-25T06:27:11.000Z","updated":"2021-10-25T06:39:26.511Z","comments":true,"path":"2021/10/25/Hbase JAVA API使用/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%20JAVA%20API%E4%BD%BF%E7%94%A8/","excerpt":"","text":"HBase Java API 的基本使用一、简述截至到目前 (2019.04)，HBase 有两个主要的版本，分别是 1.x 和 2.x ，两个版本的 Java API 有所不同，1.x 中某些方法在 2.x 中被标识为 @deprecated 过时。所以下面关于 API 的样例，我会分别给出 1.x 和 2.x 两个版本。完整的代码见本仓库： Java API 1.x Examples Java API 2.x Examples 同时你使用的客户端的版本必须与服务端版本保持一致，如果用 2.x 版本的客户端代码去连接 1.x 版本的服务端，会抛出 NoSuchColumnFamilyException 等异常。 二、Java API 1.x 基本使用2.1 新建Maven工程，导入项目依赖要使用 Java API 操作 HBase，需要引入 hbase-client。这里选取的 HBase Client 的版本为 1.2.0。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt; 2.2 API 基本使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252public class HBaseUtils &#123; private static Connection connection; static &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); // 如果是集群 则主机名用逗号分隔 configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop001&quot;); try &#123; connection = ConnectionFactory.createConnection(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 创建 HBase 表 * * @param tableName 表名 * @param columnFamilies 列族的数组 */ public static boolean createTable(String tableName, List&lt;String&gt; columnFamilies) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); if (admin.tableExists(tableName)) &#123; return false; &#125; HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName)); columnFamilies.forEach(columnFamily -&gt; &#123; HColumnDescriptor columnDescriptor = new HColumnDescriptor(columnFamily); columnDescriptor.setMaxVersions(1); tableDescriptor.addFamily(columnDescriptor); &#125;); admin.createTable(tableDescriptor); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除 hBase 表 * * @param tableName 表名 */ public static boolean deleteTable(String tableName) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); // 删除表前需要先禁用表 admin.disableTable(tableName); admin.deleteTable(tableName); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param qualifier 列标识 * @param value 数据 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, String qualifier, String value) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value)); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param pairList 列标识和值的集合 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue()))); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 根据 rowKey 获取指定行的数据 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static Result getRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); return table.get(get); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 获取指定行指定列 (cell) 的最新版本的数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamily 列族 * @param qualifier 列标识 */ public static String getCell(String tableName, String rowKey, String columnFamily, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); if (!get.isCheckExistenceOnly()) &#123; get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); Result result = table.get(get); byte[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); return Bytes.toString(resultValue); &#125; else &#123; return null; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索全表 * * @param tableName 表名 */ public static ResultScanner getScanner(String tableName) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param startRowKey 起始 RowKey * @param endRowKey 终止 RowKey * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, String startRowKey, String endRowKey, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setStartRow(Bytes.toBytes(startRowKey)); scan.setStopRow(Bytes.toBytes(endRowKey)); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 删除指定行记录 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static boolean deleteRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); table.delete(delete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除指定行的指定列 * * @param tableName 表名 * @param rowKey 唯一标识 * @param familyName 列族 * @param qualifier 列标识 */ public static boolean deleteColumn(String tableName, String rowKey, String familyName, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier)); table.delete(delete); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125;&#125; 2.3 单元测试以单元测试的方式对上面封装的 API 进行测试。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class HBaseUtilsTest &#123; private static final String TABLE_NAME = &quot;class&quot;; private static final String TEACHER = &quot;teacher&quot;; private static final String STUDENT = &quot;student&quot;; @Test public void createTable() &#123; // 新建表 List&lt;String&gt; columnFamilies = Arrays.asList(TEACHER, STUDENT); boolean table = HBaseUtils.createTable(TABLE_NAME, columnFamilies); System.out.println(&quot;表创建结果:&quot; + table); &#125; @Test public void insertData() &#123; List&lt;Pair&lt;String, String&gt;&gt; pairs1 = Arrays.asList(new Pair&lt;&gt;(&quot;name&quot;, &quot;Tom&quot;), new Pair&lt;&gt;(&quot;age&quot;, &quot;22&quot;), new Pair&lt;&gt;(&quot;gender&quot;, &quot;1&quot;)); HBaseUtils.putRow(TABLE_NAME, &quot;rowKey1&quot;, STUDENT, pairs1); List&lt;Pair&lt;String, String&gt;&gt; pairs2 = Arrays.asList(new Pair&lt;&gt;(&quot;name&quot;, &quot;Jack&quot;), new Pair&lt;&gt;(&quot;age&quot;, &quot;33&quot;), new Pair&lt;&gt;(&quot;gender&quot;, &quot;2&quot;)); HBaseUtils.putRow(TABLE_NAME, &quot;rowKey2&quot;, STUDENT, pairs2); List&lt;Pair&lt;String, String&gt;&gt; pairs3 = Arrays.asList(new Pair&lt;&gt;(&quot;name&quot;, &quot;Mike&quot;), new Pair&lt;&gt;(&quot;age&quot;, &quot;44&quot;), new Pair&lt;&gt;(&quot;gender&quot;, &quot;1&quot;)); HBaseUtils.putRow(TABLE_NAME, &quot;rowKey3&quot;, STUDENT, pairs3); &#125; @Test public void getRow() &#123; Result result = HBaseUtils.getRow(TABLE_NAME, &quot;rowKey1&quot;); if (result != null) &#123; System.out.println(Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(&quot;name&quot;)))); &#125; &#125; @Test public void getCell() &#123; String cell = HBaseUtils.getCell(TABLE_NAME, &quot;rowKey2&quot;, STUDENT, &quot;age&quot;); System.out.println(&quot;cell age :&quot; + cell); &#125; @Test public void getScanner() &#123; ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME); if (scanner != null) &#123; scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + &quot;-&gt;&quot; + Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(&quot;name&quot;))))); scanner.close(); &#125; &#125; @Test public void getScannerWithFilter() &#123; FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL); SingleColumnValueFilter nameFilter = new SingleColumnValueFilter(Bytes.toBytes(STUDENT), Bytes.toBytes(&quot;name&quot;), CompareOperator.EQUAL, Bytes.toBytes(&quot;Jack&quot;)); filterList.addFilter(nameFilter); ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME, filterList); if (scanner != null) &#123; scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + &quot;-&gt;&quot; + Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes(&quot;name&quot;))))); scanner.close(); &#125; &#125; @Test public void deleteColumn() &#123; boolean b = HBaseUtils.deleteColumn(TABLE_NAME, &quot;rowKey2&quot;, STUDENT, &quot;age&quot;); System.out.println(&quot;删除结果: &quot; + b); &#125; @Test public void deleteRow() &#123; boolean b = HBaseUtils.deleteRow(TABLE_NAME, &quot;rowKey2&quot;); System.out.println(&quot;删除结果: &quot; + b); &#125; @Test public void deleteTable() &#123; boolean b = HBaseUtils.deleteTable(TABLE_NAME); System.out.println(&quot;删除结果: &quot; + b); &#125;&#125; 三、Java API 2.x 基本使用3.1 新建Maven工程，导入项目依赖这里选取的 HBase Client 的版本为最新的 2.1.4。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt;&lt;/dependency&gt; 3.2 API 的基本使用2.x 版本相比于 1.x 废弃了一部分方法，关于废弃的方法在源码中都会指明新的替代方法，比如，在 2.x 中创建表时：HTableDescriptor 和 HColumnDescriptor 等类都标识为废弃，取而代之的是使用 TableDescriptorBuilder 和 ColumnFamilyDescriptorBuilder 来定义表和列族。 以下为 HBase 2.x 版本 Java API 的使用示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253public class HBaseUtils &#123; private static Connection connection; static &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); // 如果是集群 则主机名用逗号分隔 configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;hadoop001&quot;); try &#123; connection = ConnectionFactory.createConnection(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 创建 HBase 表 * * @param tableName 表名 * @param columnFamilies 列族的数组 */ public static boolean createTable(String tableName, List&lt;String&gt; columnFamilies) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); if (admin.tableExists(TableName.valueOf(tableName))) &#123; return false; &#125; TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)); columnFamilies.forEach(columnFamily -&gt; &#123; ColumnFamilyDescriptorBuilder cfDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(columnFamily)); cfDescriptorBuilder.setMaxVersions(1); ColumnFamilyDescriptor familyDescriptor = cfDescriptorBuilder.build(); tableDescriptor.setColumnFamily(familyDescriptor); &#125;); admin.createTable(tableDescriptor.build()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除 hBase 表 * * @param tableName 表名 */ public static boolean deleteTable(String tableName) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); // 删除表前需要先禁用表 admin.disableTable(TableName.valueOf(tableName)); admin.deleteTable(TableName.valueOf(tableName)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param qualifier 列标识 * @param value 数据 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, String qualifier, String value) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value)); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param pairList 列标识和值的集合 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue()))); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 根据 rowKey 获取指定行的数据 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static Result getRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); return table.get(get); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 获取指定行指定列 (cell) 的最新版本的数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamily 列族 * @param qualifier 列标识 */ public static String getCell(String tableName, String rowKey, String columnFamily, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); if (!get.isCheckExistenceOnly()) &#123; get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); Result result = table.get(get); byte[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); return Bytes.toString(resultValue); &#125; else &#123; return null; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索全表 * * @param tableName 表名 */ public static ResultScanner getScanner(String tableName) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param startRowKey 起始 RowKey * @param endRowKey 终止 RowKey * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, String startRowKey, String endRowKey, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.withStartRow(Bytes.toBytes(startRowKey)); scan.withStopRow(Bytes.toBytes(endRowKey)); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 删除指定行记录 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static boolean deleteRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); table.delete(delete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除指定行指定列 * * @param tableName 表名 * @param rowKey 唯一标识 * @param familyName 列族 * @param qualifier 列标识 */ public static boolean deleteColumn(String tableName, String rowKey, String familyName, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier)); table.delete(delete); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125;&#125; 四、正确连接Hbase在上面的代码中，在类加载时就初始化了 Connection 连接，并且之后的方法都是复用这个 Connection，这时我们可能会考虑是否可以使用自定义连接池来获取更好的性能表现？实际上这是没有必要的。 首先官方对于 Connection 的使用说明如下： 1234567891011121314Connection Pooling For applications which require high-end multithreaded access (e.g., web-servers or application servers that may serve many application threads in a single JVM), you can pre-create a Connection, as shown in the following example:对于高并发多线程访问的应用程序（例如，在单个 JVM 中存在的为多个线程服务的 Web 服务器或应用程序服务器）， 您只需要预先创建一个 Connection。例子如下：// Create a connection to the cluster.Configuration conf = HBaseConfiguration.create();try (Connection connection = ConnectionFactory.createConnection(conf); Table table = connection.getTable(TableName.valueOf(tablename))) &#123; // use table as needed, the table returned is lightweight&#125; 之所以能这样使用，这是因为 Connection 并不是一个简单的 socket 连接，接口文档 中对 Connection 的表述是： 1234567A cluster connection encapsulating lower level individual connections to actual servers and a connection to zookeeper. Connections are instantiated through the ConnectionFactory class. The lifecycle of the connection is managed by the caller, who has to close() the connection to release the resources. Connection 是一个集群连接，封装了与多台服务器（Matser/Region Server）的底层连接以及与 zookeeper 的连接。 连接通过 ConnectionFactory 类实例化。连接的生命周期由调用者管理，调用者必须使用 close() 关闭连接以释放资源。 之所以封装这些连接，是因为 HBase 客户端需要连接三个不同的服务角色： Zookeeper ：主要用于获取 meta 表的位置信息，Master 的信息； HBase Master ：主要用于执行 HBaseAdmin 接口的一些操作，例如建表等； HBase RegionServer ：用于读、写数据。 Connection 对象和实际的 Socket 连接之间的对应关系如下图： 上面两张图片引用自博客：连接 HBase 的正确姿势 在 HBase 客户端代码中，真正对应 Socket 连接的是 RpcConnection 对象。HBase 使用 PoolMap 这种数据结构来存储客户端到 HBase 服务器之间的连接。PoolMap 的内部有一个 ConcurrentHashMap 实例，其 key 是 ConnectionId(封装了服务器地址和用户 ticket)，value 是一个 RpcConnection 对象的资源池。当 HBase 需要连接一个服务器时，首先会根据 ConnectionId 找到对应的连接池，然后从连接池中取出一个连接对象。 123456789101112@InterfaceAudience.Privatepublic class PoolMap&lt;K, V&gt; implements Map&lt;K, V&gt; &#123; private PoolType poolType; private int poolMaxSize; private Map&lt;K, Pool&lt;V&gt;&gt; pools = new ConcurrentHashMap&lt;&gt;(); public PoolMap(PoolType poolType) &#123; this.poolType = poolType; &#125; ..... HBase 中提供了三种资源池的实现，分别是 Reusable，RoundRobin 和 ThreadLocal。具体实现可以通 hbase.client.ipc.pool.type 配置项指定，默认为 Reusable。连接池的大小也可以通过 hbase.client.ipc.pool.size 配置项指定，默认为 1，即每个 Server 1 个连接。也可以通过修改配置实现： 123config.set(&quot;hbase.client.ipc.pool.type&quot;,...);config.set(&quot;hbase.client.ipc.pool.size&quot;,...);connection = ConnectionFactory.createConnection(config); 由此可以看出 HBase 中 Connection 类已经实现了对连接的管理功能，所以我们不必在 Connection 上在做额外的管理。 另外，Connection 是线程安全的，但 Table 和 Admin 却不是线程安全的，因此正确的做法是一个进程共用一个 Connection 对象，而在不同的线程中使用单独的 Table 和 Admin 对象。Table 和 Admin 的获取操作 getTable() 和 getAdmin() 都是轻量级，所以不必担心性能的消耗，同时建议在使用完成后显示的调用 close() 方法来关闭它们。 参考资料 连接 HBase 的正确姿势 Apache HBase ™ Reference Guide","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Spring整合Mybatis+Phoenix(附springBoot)","slug":"SpringBoot整合Mybatis+Phoenix","date":"2021-10-25T06:25:30.000Z","updated":"2021-11-01T07:24:49.349Z","comments":true,"path":"2021/10/25/SpringBoot整合Mybatis+Phoenix/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/SpringBoot%E6%95%B4%E5%90%88Mybatis+Phoenix/","excerpt":"","text":"Spring整合Mybatis+Phoenix(附springBoot)一、前言使用 Spring+Mybatis 操作 Phoenix 和操作其他的关系型数据库（如 Mysql，Oracle）在配置上是基本相同的，下面会分别给出 Spring/Spring Boot 整合步骤，完整代码见本仓库： Spring + Mybatis + Phoenix SpringBoot + Mybatis + Phoenix 二、Spring + Mybatis + Phoenix2.1 项目结构 2.2 主要依赖除了 Spring 相关依赖外，还需要导入 phoenix-core 和对应的 Mybatis 依赖包 1234567891011121314151617&lt;!--mybatis 依赖包--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt;&lt;/dependency&gt;&lt;!--phoenix core--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.14.0-cdh5.14.2&lt;/version&gt;&lt;/dependency&gt; 2.3 数据库配置文件在数据库配置文件 jdbc.properties 中配置数据库驱动和 zookeeper 地址 1234# 数据库驱动phoenix.driverClassName=org.apache.phoenix.jdbc.PhoenixDriver# zookeeper地址phoenix.url=jdbc:phoenix:192.168.0.105:2181 2.4 配置数据源和会话工厂1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd&quot;&gt; &lt;!-- 开启注解包扫描--&gt; &lt;context:component-scan base-package=&quot;com.ihadyou.*&quot;/&gt; &lt;!--指定配置文件的位置--&gt; &lt;context:property-placeholder location=&quot;classpath:jdbc.properties&quot;/&gt; &lt;!--配置数据源--&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;org.springframework.jdbc.datasource.DriverManagerDataSource&quot;&gt; &lt;!--Phoenix 配置--&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;phoenix.driverClassName&#125;&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;phoenix.url&#125;&quot;/&gt; &lt;/bean&gt; &lt;!--配置 mybatis 会话工厂 --&gt; &lt;bean id=&quot;sqlSessionFactory&quot; class=&quot;org.mybatis.spring.SqlSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;!--指定 mapper 文件所在的位置--&gt; &lt;property name=&quot;mapperLocations&quot; value=&quot;classpath*:/mappers/**/*.xml&quot;/&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:mybatisConfig.xml&quot;/&gt; &lt;/bean&gt; &lt;!--扫描注册接口 --&gt; &lt;!--作用:从接口的基础包开始递归搜索，并将它们注册为 MapperFactoryBean(只有至少一种方法的接口才会被注册;, 具体类将被忽略)--&gt; &lt;bean class=&quot;org.mybatis.spring.mapper.MapperScannerConfigurer&quot;&gt; &lt;!--指定会话工厂 --&gt; &lt;property name=&quot;sqlSessionFactoryBeanName&quot; value=&quot;sqlSessionFactory&quot;/&gt; &lt;!-- 指定 mybatis 接口所在的包 --&gt; &lt;property name=&quot;basePackage&quot; value=&quot;com.ihadyou.dao&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt; 2.5 Mybtais参数配置新建 mybtais 配置文件，按照需求配置额外参数， 更多 settings 配置项可以参考官方文档 1234567891011121314&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;!-- mybatis 配置文件 --&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!-- 开启驼峰命名 --&gt; &lt;setting name=&quot;mapUnderscoreToCamelCase&quot; value=&quot;true&quot;/&gt; &lt;!-- 打印查询 sql --&gt; &lt;setting name=&quot;logImpl&quot; value=&quot;STDOUT_LOGGING&quot;/&gt; &lt;/settings&gt;&lt;/configuration&gt; 2.6 查询接口12345678910public interface PopulationDao &#123; List&lt;USPopulation&gt; queryAll(); void save(USPopulation USPopulation); USPopulation queryByStateAndCity(@Param(&quot;state&quot;) String state, @Param(&quot;city&quot;) String city); void deleteByStateAndCity(@Param(&quot;state&quot;) String state, @Param(&quot;city&quot;) String city);&#125; 123456789101112131415161718192021222324&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.ihadyou.dao.PopulationDao&quot;&gt; &lt;select id=&quot;queryAll&quot; resultType=&quot;com.ihadyou.bean.USPopulation&quot;&gt; SELECT * FROM us_population &lt;/select&gt; &lt;insert id=&quot;save&quot;&gt; UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; ) &lt;/insert&gt; &lt;select id=&quot;queryByStateAndCity&quot; resultType=&quot;com.ihadyou.bean.USPopulation&quot;&gt; SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125; &lt;/select&gt; &lt;delete id=&quot;deleteByStateAndCity&quot;&gt; DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125; &lt;/delete&gt;&lt;/mapper&gt; 2.7 单元测试123456789101112131415161718192021222324252627282930313233343536373839@RunWith(SpringRunner.class)@ContextConfiguration(&#123;&quot;classpath:springApplication.xml&quot;&#125;)public class PopulationDaoTest &#123; @Autowired private PopulationDao populationDao; @Test public void queryAll() &#123; List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll(); if (USPopulationList != null) &#123; for (USPopulation USPopulation : USPopulationList) &#123; System.out.println(USPopulation.getCity() + &quot; &quot; + USPopulation.getPopulation()); &#125; &#125; &#125; @Test public void save() &#123; populationDao.save(new USPopulation(&quot;TX&quot;, &quot;Dallas&quot;, 66666)); USPopulation usPopulation = populationDao.queryByStateAndCity(&quot;TX&quot;, &quot;Dallas&quot;); System.out.println(usPopulation); &#125; @Test public void update() &#123; populationDao.save(new USPopulation(&quot;TX&quot;, &quot;Dallas&quot;, 99999)); USPopulation usPopulation = populationDao.queryByStateAndCity(&quot;TX&quot;, &quot;Dallas&quot;); System.out.println(usPopulation); &#125; @Test public void delete() &#123; populationDao.deleteByStateAndCity(&quot;TX&quot;, &quot;Dallas&quot;); USPopulation usPopulation = populationDao.queryByStateAndCity(&quot;TX&quot;, &quot;Dallas&quot;); System.out.println(usPopulation); &#125;&#125; 三、SpringBoot + Mybatis + Phoenix3.1 项目结构 3.2 主要依赖1234567891011121314&lt;!--spring 1.5 x 以上版本对应 mybatis 1.3.x (1.3.1) 关于更多 spring-boot 与 mybatis 的版本对应可以参见 &lt;a href=&quot;http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/&quot;&gt;--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;!--phoenix core--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.14.0-cdh5.14.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; spring boot 与 mybatis 版本的对应关系： MyBatis-Spring-Boot-Starter 版本 MyBatis-Spring 版本 Spring Boot 版本 1.3.x (1.3.1) 1.3 or higher 1.5 or higher 1.2.x (1.2.1) 1.3 or higher 1.4 or higher 1.1.x (1.1.1) 1.3 or higher 1.3 or higher 1.0.x (1.0.2) 1.2 or higher 1.3 or higher 3.3 配置数据源在 application.yml 中配置数据源，spring boot 2.x 版本默认采用 Hikari 作为数据库连接池，Hikari 是目前 java 平台性能最好的连接池，性能好于 druid。 1234567891011121314151617181920212223242526272829303132spring: datasource: #zookeeper 地址 url: jdbc:phoenix:192.168.0.105:2181 driver-class-name: org.apache.phoenix.jdbc.PhoenixDriver # 如果不想配置对数据库连接池做特殊配置的话,以下关于连接池的配置就不是必须的 # spring-boot 2.X 默认采用高性能的 Hikari 作为连接池 更多配置可以参考 https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby type: com.zaxxer.hikari.HikariDataSource hikari: # 池中维护的最小空闲连接数 minimum-idle: 10 # 池中最大连接数，包括闲置和使用中的连接 maximum-pool-size: 20 # 此属性控制从池返回的连接的默认自动提交行为。默认为 true auto-commit: true # 允许最长空闲时间 idle-timeout: 30000 # 此属性表示连接池的用户定义名称，主要显示在日志记录和 JMX 管理控制台中，以标识池和池配置。 默认值：自动生成 pool-name: custom-hikari #此属性控制池中连接的最长生命周期，值 0 表示无限生命周期，默认 1800000 即 30 分钟 max-lifetime: 1800000 # 数据库连接超时时间,默认 30 秒，即 30000 connection-timeout: 30000 # 连接测试 sql 这个地方需要根据数据库方言差异而配置 例如 oracle 就应该写成 select 1 from dual connection-test-query: SELECT 1# mybatis 相关配置mybatis: configuration: # 是否打印 sql 语句 调试的时候可以开启 log-impl: org.apache.ibatis.logging.stdout.StdOutImpl 3.4 新建查询接口上面 Spring+Mybatis 我们使用了 XML 的方式来写 SQL，为了体现 Mybatis 支持多种方式，这里使用注解的方式来写 SQL。 12345678910111213141516@Mapperpublic interface PopulationDao &#123; @Select(&quot;SELECT * from us_population&quot;) List&lt;USPopulation&gt; queryAll(); @Insert(&quot;UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; )&quot;) void save(USPopulation USPopulation); @Select(&quot;SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;&quot;) USPopulation queryByStateAndCity(String state, String city); @Delete(&quot;DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;&quot;) void deleteByStateAndCity(String state, String city);&#125; 3.5 单元测试1234567891011121314151617181920212223242526272829303132333435363738394041@RunWith(SpringRunner.class)@SpringBootTestpublic class PopulationTest &#123; @Autowired private PopulationDao populationDao; @Test public void queryAll() &#123; List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll(); if (USPopulationList != null) &#123; for (USPopulation USPopulation : USPopulationList) &#123; System.out.println(USPopulation.getCity() + &quot; &quot; + USPopulation.getPopulation()); &#125; &#125; &#125; @Test public void save() &#123; populationDao.save(new USPopulation(&quot;TX&quot;, &quot;Dallas&quot;, 66666)); USPopulation usPopulation = populationDao.queryByStateAndCity(&quot;TX&quot;, &quot;Dallas&quot;); System.out.println(usPopulation); &#125; @Test public void update() &#123; populationDao.save(new USPopulation(&quot;TX&quot;, &quot;Dallas&quot;, 99999)); USPopulation usPopulation = populationDao.queryByStateAndCity(&quot;TX&quot;, &quot;Dallas&quot;); System.out.println(usPopulation); &#125; @Test public void delete() &#123; populationDao.deleteByStateAndCity(&quot;TX&quot;, &quot;Dallas&quot;); USPopulation usPopulation = populationDao.queryByStateAndCity(&quot;TX&quot;, &quot;Dallas&quot;); System.out.println(usPopulation); &#125;&#125; 附：建表语句上面单元测试涉及到的测试表的建表语句如下： 12345678910111213141516CREATE TABLE IF NOT EXISTS us_population ( state CHAR(2) NOT NULL, city VARCHAR NOT NULL, population BIGINT CONSTRAINT my_pk PRIMARY KEY (state, city)); -- 测试数据UPSERT INTO us_population VALUES(&#x27;NY&#x27;,&#x27;New York&#x27;,8143197);UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;Los Angeles&#x27;,3844829);UPSERT INTO us_population VALUES(&#x27;IL&#x27;,&#x27;Chicago&#x27;,2842518);UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;Houston&#x27;,2016582);UPSERT INTO us_population VALUES(&#x27;PA&#x27;,&#x27;Philadelphia&#x27;,1463281);UPSERT INTO us_population VALUES(&#x27;AZ&#x27;,&#x27;Phoenix&#x27;,1461575);UPSERT INTO us_population VALUES(&#x27;TX&#x27;,&#x27;San Antonio&#x27;,1256509);UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;San Diego&#x27;,1255540);UPSERT INTO us_population VALUES(&#x27;CA&#x27;,&#x27;San Jose&#x27;,912332);","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase常用shell命令","slug":"Hbase常用shell命令","date":"2021-10-25T06:24:47.000Z","updated":"2021-10-25T06:39:02.719Z","comments":true,"path":"2021/10/25/Hbase常用shell命令/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/","excerpt":"","text":"Hbase 常用 Shell 命令一、基本命令打开 Hbase Shell： 1# hbase shell 1.1 获取帮助1234# 获取帮助help# 获取命令的详细信息help &#x27;status&#x27; 1.2 查看服务器状态1status 1.3 查看版本信息1version 二、关于表的操作2.1 查看所有表1list 2.2 创建表命令格式： create ‘表名称’, ‘列族名称 1’,’列族名称 2’,’列名称 N’ 12# 创建一张名为Student的表,包含基本信息（baseInfo）、学校信息（schoolInfo）两个列族create &#x27;Student&#x27;,&#x27;baseInfo&#x27;,&#x27;schoolInfo&#x27; 2.3 查看表的基本信息命令格式：desc ‘表名’ 1describe &#x27;Student&#x27; 2.4 表的启用/禁用enable 和 disable 可以启用/禁用这个表,is_enabled 和 is_disabled 来检查表是否被禁用 12345678# 禁用表disable &#x27;Student&#x27;# 检查表是否被禁用is_disabled &#x27;Student&#x27;# 启用表enable &#x27;Student&#x27;# 检查表是否被启用is_enabled &#x27;Student&#x27; 2.5 检查表是否存在1exists &#x27;Student&#x27; 2.6 删除表1234# 删除表前需要先禁用表disable &#x27;Student&#x27;# 删除表drop &#x27;Student&#x27; 三、增删改3.1 添加列族命令格式： alter ‘表名’, ‘列族名’ 1alter &#x27;Student&#x27;, &#x27;teacherInfo&#x27; 3.2 删除列族命令格式：alter ‘表名’, {NAME =&gt; ‘列族名’, METHOD =&gt; ‘delete’} 1alter &#x27;Student&#x27;, &#123;NAME =&gt; &#x27;teacherInfo&#x27;, METHOD =&gt; &#x27;delete&#x27;&#125; 3.3 更改列族存储版本的限制默认情况下，列族只存储一个版本的数据，如果需要存储多个版本的数据，则需要修改列族的属性。修改后可通过 desc 命令查看。 1alter &#x27;Student&#x27;,&#123;NAME=&gt;&#x27;baseInfo&#x27;,VERSIONS=&gt;3&#125; 3.4 插入数据命令格式：put ‘表名’, ‘行键’,’列族:列’,’值’ 注意：如果新增数据的行键值、列族名、列名与原有数据完全相同，则相当于更新操作 12345678910111213141516171819put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:name&#x27;,&#x27;tom&#x27;put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1990-01-09&#x27;put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;baseInfo:age&#x27;,&#x27;29&#x27;put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;Havard&#x27;put &#x27;Student&#x27;, &#x27;rowkey1&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;Boston&#x27;put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:name&#x27;,&#x27;jack&#x27;put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1998-08-22&#x27;put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;baseInfo:age&#x27;,&#x27;21&#x27;put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;yale&#x27;put &#x27;Student&#x27;, &#x27;rowkey2&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;New Haven&#x27;put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27;,&#x27;maike&#x27;put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:birthday&#x27;,&#x27;1995-01-22&#x27;put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;baseInfo:age&#x27;,&#x27;24&#x27;put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;schoolInfo:name&#x27;,&#x27;yale&#x27;put &#x27;Student&#x27;, &#x27;rowkey3&#x27;,&#x27;schoolInfo:localtion&#x27;,&#x27;New Haven&#x27;put &#x27;Student&#x27;, &#x27;wrowkey4&#x27;,&#x27;baseInfo:name&#x27;,&#x27;maike-jack&#x27; 3.5 获取指定行、指定行中的列族、列的信息123456# 获取指定行中所有列的数据信息get &#x27;Student&#x27;,&#x27;rowkey3&#x27;# 获取指定行中指定列族下所有列的数据信息get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo&#x27;# 获取指定行中指定列的数据信息get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27; 3.6 删除指定行、指定行中的列1234# 删除指定行delete &#x27;Student&#x27;,&#x27;rowkey3&#x27;# 删除指定行中指定列的数据delete &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27; 四、查询hbase 中访问数据有两种基本的方式： 按指定 rowkey 获取数据：get 方法； 按指定条件获取数据：scan 方法。 scan 可以设置 begin 和 end 参数来访问一个范围内所有的数据。get 本质上就是 begin 和 end 相等的一种特殊的 scan。 4.1Get查询123456# 获取指定行中所有列的数据信息get &#x27;Student&#x27;,&#x27;rowkey3&#x27;# 获取指定行中指定列族下所有列的数据信息get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo&#x27;# 获取指定行中指定列的数据信息get &#x27;Student&#x27;,&#x27;rowkey3&#x27;,&#x27;baseInfo:name&#x27; 4.2 查询整表数据1scan &#x27;Student&#x27; 4.3 查询指定列簇的数据1scan &#x27;Student&#x27;, &#123;COLUMN=&gt;&#x27;baseInfo&#x27;&#125; 4.4 条件查询12# 查询指定列的数据scan &#x27;Student&#x27;, &#123;COLUMNS=&gt; &#x27;baseInfo:birthday&#x27;&#125; 除了列 （COLUMNS） 修饰词外，HBase 还支持 Limit（限制查询结果行数），STARTROW（ROWKEY 起始行，会先根据这个 key 定位到 region，再向后扫描）、STOPROW(结束行)、TIMERANGE（限定时间戳范围）、VERSIONS（版本数）、和 FILTER（按条件过滤行）等。 如下代表从 rowkey2 这个 rowkey 开始，查找下两个行的最新 3 个版本的 name 列的数据： 1scan &#x27;Student&#x27;, &#123;COLUMNS=&gt; &#x27;baseInfo:name&#x27;,STARTROW =&gt; &#x27;rowkey2&#x27;,STOPROW =&gt; &#x27;wrowkey4&#x27;,LIMIT=&gt;2, VERSIONS=&gt;3&#125; 4.5 条件过滤Filter 可以设定一系列条件来进行过滤。如我们要查询值等于 24 的所有数据： 1scan &#x27;Student&#x27;, FILTER=&gt;&quot;ValueFilter(=,&#x27;binary:24&#x27;)&quot; 值包含 yale 的所有数据： 1scan &#x27;Student&#x27;, FILTER=&gt;&quot;ValueFilter(=,&#x27;substring:yale&#x27;)&quot; 列名中的前缀为 birth 的： 1scan &#x27;Student&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;birth&#x27;)&quot; FILTER 中支持多个过滤条件通过括号、AND 和 OR 进行组合： 12# 列名中的前缀为birth且列值中包含1998的数据scan &#x27;Student&#x27;, FILTER=&gt;&quot;ColumnPrefixFilter(&#x27;birth&#x27;) AND ValueFilter ValueFilter(=,&#x27;substring:1998&#x27;)&quot; PrefixFilter 用于对 Rowkey 的前缀进行判断： 1scan &#x27;Student&#x27;, FILTER=&gt;&quot;PrefixFilter(&#x27;wr&#x27;)&quot;","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase集群搭建","slug":"Hbase集群搭建","date":"2021-10-25T06:22:07.000Z","updated":"2021-10-25T06:38:44.524Z","comments":true,"path":"2021/10/25/Hbase集群搭建/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","excerpt":"","text":"HBase集群环境配置一、集群规划这里搭建一个 3 节点的 HBase 集群，其中三台主机上均为 Regin Server。同时为了保证高可用，除了在 hadoop001 上部署主 Master 服务外，还在 hadoop002 上部署备用的 Master 服务。Master 服务由 Zookeeper 集群进行协调管理，如果主 Master 不可用，则备用 Master 会成为新的主 Master。 二、前置条件HBase 的运行需要依赖 Hadoop 和 JDK(HBase 2.0+ 对应 JDK 1.8+) 。同时为了保证高可用，这里我们不采用 HBase 内置的 Zookeeper 服务，而采用外置的 Zookeeper 集群。相关搭建步骤可以参阅： Linux 环境下 JDK 安装 Zookeeper 单机环境和集群环境搭建 Hadoop 集群环境搭建 三、集群搭建3.1 下载并解压下载并解压，这里我下载的是 CDH 版本 HBase，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 1# tar -zxvf hbase-1.2.0-cdh5.15.2.tar.gz 3.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export HBASE_HOME=usr/app/hbase-1.2.0-cdh5.15.2export PATH=$HBASE_HOME/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 3.3 集群配置进入 $&#123;HBASE_HOME&#125;/conf 目录下，修改配置： 1. hbase-env.sh1234# 配置JDK安装位置export JAVA_HOME=/usr/java/jdk1.8.0_201# 不使用内置的zookeeper服务export HBASE_MANAGES_ZK=false 2. hbase-site.xml1234567891011121314151617&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定 hbase 以分布式集群的方式运行 --&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定 hbase 在 HDFS 上的存储位置 --&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定 zookeeper 的地址--&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. regionservers123hadoop001hadoop002hadoop003 4. backup-masters1hadoop002 backup-masters 这个文件是不存在的，需要新建，主要用来指明备用的 master 节点，可以是多个，这里我们以 1 个为例。 3.4 HDFS客户端配置这里有一个可选的配置：如果您在 Hadoop 集群上进行了 HDFS 客户端配置的更改，比如将副本系数 dfs.replication 设置成 5，则必须使用以下方法之一来使 HBase 知道，否则 HBase 将依旧使用默认的副本系数 3 来创建文件： Add a pointer to your HADOOP_CONF_DIR to the HBASE_CLASSPATH environment variable in hbase-env.sh. Add a copy of hdfs-site.xml (or hadoop-site.xml) or, better, symlinks, under ${HBASE_HOME}/conf, or if only a small set of HDFS client configurations, add them to hbase-site.xml. 以上是官方文档的说明，这里解释一下： 第一种 ：将 Hadoop 配置文件的位置信息添加到 hbase-env.sh 的 HBASE_CLASSPATH 属性，示例如下： 1export HBASE_CLASSPATH=usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop 第二种 ：将 Hadoop 的 hdfs-site.xml 或 hadoop-site.xml 拷贝到 $&#123;HBASE_HOME&#125;/conf 目录下，或者通过符号链接的方式。如果采用这种方式的话，建议将两者都拷贝或建立符号链接，示例如下： 12345# 拷贝cp core-site.xml hdfs-site.xml /usr/app/hbase-1.2.0-cdh5.15.2/conf/# 使用符号链接ln -s /usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop/core-site.xmlln -s /usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop/hdfs-site.xml 注：hadoop-site.xml 这个配置文件现在叫做 core-site.xml 第三种 ：如果你只有少量更改，那么直接配置到 hbase-site.xml 中即可。 3.5 安装包分发将 HBase 的安装包分发到其他服务器，分发后建议在这两台服务器上也配置一下 HBase 的环境变量。 12scp -r /usr/app/hbase-1.2.0-cdh5.15.2/ hadoop002:usr/app/scp -r /usr/app/hbase-1.2.0-cdh5.15.2/ hadoop003:usr/app/ 四、启动集群4.1 启动ZooKeeper集群分别到三台服务器上启动 ZooKeeper 服务： 1zkServer.sh start 4.2 启动Hadoop集群1234# 启动dfs服务start-dfs.sh# 启动yarn服务start-yarn.sh 4.3 启动HBase集群进入 hadoop001 的 $&#123;HBASE_HOME&#125;/bin，使用以下命令启动 HBase 集群。执行此命令后，会在 hadoop001 上启动 Master 服务，在 hadoop002 上启动备用 Master 服务，在 regionservers 文件中配置的所有节点启动 region server 服务。 1start-hbase.sh 4.5 查看服务访问 HBase 的 Web-UI 界面，这里我安装的 HBase 版本为 1.2，访问端口为 60010，如果你安装的是 2.0 以上的版本，则访问端口号为 16010。可以看到 Master 在 hadoop001 上，三个 Regin Servers 分别在 hadoop001，hadoop002，和 hadoop003 上，并且还有一个 Backup Matser 服务在 hadoop002 上。 hadoop002 上的 HBase 出于备用状态：","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase环境搭建","slug":"Hbase环境搭建","date":"2021-10-25T06:21:57.000Z","updated":"2021-10-25T06:38:52.094Z","comments":true,"path":"2021/10/25/Hbase环境搭建/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","excerpt":"","text":"HBase基本环境搭建一、安装前置条件说明1.1 JDK版本说明HBase 需要依赖 JDK 环境，同时 HBase 2.0+ 以上版本不再支持 JDK 1.7 ，需要安装 JDK 1.8+ 。JDK 安装方式见本仓库： Linux 环境下 JDK 安装 1.2 Standalone模式和伪集群模式的区别 在 Standalone 模式下，所有守护进程都运行在一个 jvm 进程/实例中； 在伪分布模式下，HBase 仍然在单个主机上运行，但是每个守护进程 (HMaster，HRegionServer 和 ZooKeeper) 则分别作为一个单独的进程运行。 说明：两种模式任选其一进行部署即可，对于开发测试来说区别不大。 二、Standalone 模式2.1 下载并解压从官方网站 下载所需要版本的二进制安装包，并进行解压： 1# tar -zxvf hbase-2.1.4-bin.tar.gz 2.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export HBASE_HOME=/usr/app/hbase-2.1.4export PATH=$HBASE_HOME/bin:$PATH 使得配置的环境变量生效： 1# source /etc/profile 2.3 进行HBase相关配置修改安装目录下的 conf/hbase-env.sh,指定 JDK 的安装路径： 12# The java implementation to use. Java 1.8+ required.export JAVA_HOME=/usr/java/jdk1.8.0_201 修改安装目录下的 conf/hbase-site.xml，增加如下配置： 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///home/hbase/rootdir&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/zookeeper/dataDir&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hbase.rootdir: 配置 hbase 数据的存储路径； hbase.zookeeper.property.dataDir: 配置 zookeeper 数据的存储路径； hbase.unsafe.stream.capability.enforce: 使用本地文件系统存储，不使用 HDFS 的情况下需要禁用此配置，设置为 false。 2.4 启动HBase由于已经将 HBase 的 bin 目录配置到环境变量，直接使用以下命令启动： 1# start-hbase.sh 2.5 验证启动是否成功验证方式一 ：使用 jps 命令查看 HMaster 进程是否启动。 123[root@hadoop001 hbase-2.1.4]# jps16336 Jps15500 HMaster 验证方式二 ：访问 HBaseWeb UI 页面，默认端口为 16010 。 三、伪集群模式安装（Pseudo-Distributed）3.1 Hadoop单机伪集群安装这里我们采用 HDFS 作为 HBase 的存储方案，需要预先安装 Hadoop。Hadoop 的安装方式单独整理至： Hadoop 单机伪集群搭建 3.2 Hbase版本选择HBase 的版本必须要与 Hadoop 的版本兼容，不然会出现各种 Jar 包冲突。这里我 Hadoop 安装的版本为 hadoop-2.6.0-cdh5.15.2，为保持版本一致，选择的 HBase 版本为 hbase-1.2.0-cdh5.15.2 。所有软件版本如下： Hadoop 版本： hadoop-2.6.0-cdh5.15.2 HBase 版本： hbase-1.2.0-cdh5.15.2 JDK 版本：JDK 1.8 3.3 软件下载解压下载后进行解压，下载地址：http://archive.cloudera.com/cdh5/cdh/5/ 1# tar -zxvf hbase-1.2.0-cdh5.15.2.tar.gz 3.4 配置环境变量1# vim /etc/profile 添加环境变量： 12export HBASE_HOME=/usr/app/hbase-1.2.0-cdh5.15.2export PATH=$HBASE_HOME/bin:$PATH 使得配置的环境变量生效： 1# source /etc/profile 3.5 进行HBase相关配置1.修改安装目录下的 conf/hbase-env.sh,指定 JDK 的安装路径： 12# The java implementation to use. Java 1.7+ required.export JAVA_HOME=/usr/java/jdk1.8.0_201 2.修改安装目录下的 conf/hbase-site.xml，增加如下配置 (hadoop001 为主机名)： 1234567891011121314151617&lt;configuration&gt; &lt;!--指定 HBase 以分布式模式运行--&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定 HBase 数据存储路径为 HDFS 上的 hbase 目录,hbase 目录不需要预先创建，程序会自动创建--&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;!--指定 zookeeper 数据的存储位置--&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/zookeeper/dataDir&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.修改安装目录下的 conf/regionservers，指定 region servers 的地址，修改后其内容如下： 1hadoop001 3.6 启动1# bin/start-hbase.sh 3.7 验证启动是否成功验证方式一 ：使用 jps 命令查看进程。其中 HMaster，HRegionServer 是 HBase 的进程，HQuorumPeer 是 HBase 内置的 Zookeeper 的进程，其余的为 HDFS 和 YARN 的进程。 123456789101112[root@hadoop001 conf]# jps28688 NodeManager25824 GradleDaemon10177 Jps22083 HRegionServer20534 DataNode20807 SecondaryNameNode18744 Main20411 NameNode21851 HQuorumPeer28573 ResourceManager21933 HMaster 验证方式二 ：访问 HBase Web UI 界面，需要注意的是 1.2 版本的 HBase 的访问端口为 60010","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase架构及数据结构","slug":"Hbase架构及数据结构","date":"2021-10-25T06:20:34.000Z","updated":"2021-10-25T06:38:57.089Z","comments":true,"path":"2021/10/25/Hbase架构及数据结构/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"Hbase系统架构及数据结构一、基本概念一个典型的 Hbase Table 表如下： 1.1 Row Key (行键)Row Key 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式： 通过指定的 Row Key 进行访问； 通过 Row Key 的 range 进行访问，即访问指定范围内的行； 进行全表扫描。 Row Key 可以是任意字符串，存储时数据按照 Row Key 的字典序进行排序。这里需要注意以下两点： 因为字典序对 Int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。 行的一次读写操作时原子性的 (不论一次读写多少列)。 1.2 Column Family（列族）HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时进行定义。列族的所有列都以列族名作为前缀，例如 courses:history，courses:math 都属于 courses 这个列族。 1.3 Column Qualifier (列限定符)列限定符，你可以理解为是具体的列名，例如 courses:history，courses:math 都属于 courses 这个列族，它们的列限定符分别是 history 和 math。需要注意的是列限定符不是表 Schema 的一部分，你可以在插入数据的过程中动态创建列。 1.4 Column(列)HBase 中的列由列族和列限定符组成，它们由 :(冒号) 进行分隔，即一个完整的列名应该表述为 列族名 ：列限定符。 1.5 CellCell 是行，列族和列限定符的组合，并包含值和时间戳。你可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，但不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。 1.6 Timestamp(时间戳)HBase 中通过 row key 和 column 确定的为一个存储单元称为 Cell。每个 Cell 都保存着同一份数据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入时自动赋值，也可以由客户显式指定。每个 Cell 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。 二、存储结构2.1 RegionsHBase Table 中的所有行按照 Row Key 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 Region, 一个 Region 包含了在 start key 和 end key 之间的所有行。 每个表一开始只有一个 Region，随着数据不断增加，Region 会不断增大，当增大到一个阀值的时候，Region 就会等分为两个新的 Region。当 Table 中的行不断增多，就会有越来越多的 Region。 Region 是 HBase 中分布式存储和负载均衡的最小单元。这意味着不同的 Region 可以分布在不同的 Region Server 上。但一个 Region 是不会拆分到多个 Server 上的。 2.2 Region ServerRegion Server 运行在 HDFS 的 DataNode 上。它具有以下组件： **WAL(Write Ahead Log，预写日志)**：用于存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。 BlockCache：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 最近最少使用原则 清除多余的数据。 MemStore：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。 HFile ：将行数据按照 Key\\Values 的形式存储在文件系统上。 Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 Store 实例，每个 Store 会有 0 个或多个 StoreFile 与之对应，每个 StoreFile 则对应一个 HFile，HFile 就是实际存储在 HDFS 上的文件。 三、Hbase系统架构3.1 系统架构HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成： Zookeeper 保证任何时候，集群中只有一个 Master； 存贮所有 Region 的寻址入口； 实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master； 存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。 Master 为 Region Server 分配 Region ； 负责 Region Server 的负载均衡 ； 发现失效的 Region Server 并重新分配其上的 Region； GFS 上的垃圾文件回收； 处理 Schema 的更新请求。 Region Server Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求； Region Server 负责切分在运行过程中变得过大的 Region。 3.2 组件间的协作HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务： 每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server； 所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听； 如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。 四、数据的读写流程简述4.1 写入数据的流程 Client 向 Region Server 提交写请求； Region Server 找到目标 Region； Region 检查数据是否与 Schema 一致； 如果客户端没有指定版本，则获取当前系统时间作为数据版本； 将更新写入 WAL Log； 将更新写入 Memstore； 判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。 更为详细写入流程可以参考：HBase － 数据写入流程解析 4.2 读取数据的流程以下是客户端首次读写 HBase 上数据的流程： 客户端从 Zookeeper 获取 META 表所在的 Region Server； 客户端访问 META 表所在的 Region Server，从 META 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 META 表的位置； 客户端从行键所在的 Region Server 上获取数据。 如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 META 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。 注：META 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息则存储在 ZooKeeper 上。 更为详细读取数据流程参考： HBase 原理－数据读取流程解析 HBase 原理－迟到的‘数据读取流程部分细节 参考资料本篇文章内容主要参考自官方文档和以下两篇博客，图片也主要引用自以下两篇博客： HBase Architectural Components Hbase 系统架构及数据结构 官方文档： Apache HBase ™ Reference Guide","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hbase简介","slug":"Hbase简介","date":"2021-10-25T06:16:23.000Z","updated":"2021-10-25T06:38:44.546Z","comments":true,"path":"2021/10/25/Hbase简介/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hbase%E7%AE%80%E4%BB%8B/","excerpt":"","text":"HBase简介一、Hadoop的局限HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。 要想明白为什么产生 HBase，就需要先了解一下 Hadoop 存在的限制？Hadoop 可以通过 HDFS 来存储结构化、半结构甚至非结构化的数据，它是传统数据库的补充，是海量数据存储的最佳方法，它针对大文件的存储，批量访问和流式访问都做了优化，同时也通过多副本解决了容灾问题。 但是 Hadoop 的缺陷在于它只能执行批处理，并且只能以顺序方式访问数据，这意味着即使是最简单的工作，也必须搜索整个数据集，无法实现对数据的随机访问。实现数据的随机访问是传统的关系型数据库所擅长的，但它们却不能用于海量数据的存储。在这种情况下，必须有一种新的方案来解决海量数据存储和随机访问的问题，HBase 就是其中之一 (HBase，Cassandra，couchDB，Dynamo 和 MongoDB 都能存储海量数据并支持随机访问)。 注：数据结构分类： 结构化数据：即以关系型数据库表形式管理的数据； 半结构化数据：非关系模型的，有基本固定结构模式的数据，例如日志文件、XML 文档、JSON 文档、Email 等； 非结构化数据：没有固定模式的数据，如 WORD、PDF、PPT、EXL，各种格式的图片、视频等。 二、HBase简介HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。 HBase 是一种类似于 Google’s Big Table 的数据模型，它是 Hadoop 生态系统的一部分，它将数据存储在 HDFS 上，客户端可以通过 HBase 实现对 HDFS 上数据的随机访问。它具有以下特性： 不支持复杂的事务，只支持行级事务，即单行数据的读写都是原子性的； 由于是采用 HDFS 作为底层存储，所以和 HDFS 一样，支持结构化、半结构化和非结构化的存储； 支持通过增加机器进行横向扩展； 支持数据分片； 支持 RegionServers 之间的自动故障转移； 易于使用的 Java 客户端 API； 支持 BlockCache 和布隆过滤器； 过滤器支持谓词下推。 三、HBase TableHBase 是一个面向 列 的数据库管理系统，这里更为确切的而说，HBase 是一个面向 列族 的数据库管理系统。表 schema 仅定义列族，表具有多个列族，每个列族可以包含任意数量的列，列由多个单元格（cell ）组成，单元格可以存储多个版本的数据，多个版本数据以时间戳进行区分。 下图为 HBase 中一张表的： RowKey 为行的唯一标识，所有行按照 RowKey 的字典序进行排序； 该表具有两个列族，分别是 personal 和 office; 其中列族 personal 拥有 name、city、phone 三个列，列族 office 拥有 tel、addres 两个列。 图片引用自 : HBase 是列式存储数据库吗 https://www.iteblog.com/archives/2498.html Hbase 的表具有以下特点： 容量大：一个表可以有数十亿行，上百万列； 面向列：数据是按照列存储，每一列都单独存放，数据即索引，在查询时可以只访问指定列的数据，有效地降低了系统的 I/O 负担； 稀疏性：空 (null) 列并不占用存储空间，表可以设计的非常稀疏 ； 数据多版本：每个单元中的数据可以有多个版本，按照时间戳排序，新的数据在最上面； 存储类型：所有数据的底层存储格式都是字节数组 (byte[])。 四、PhoenixPhoenix 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 Phoenix 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。Phoenix 的理念是 we put sql SQL back in NOSQL，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 Spring Data JPA 或 Mybatis 等常用的持久层框架来操作 HBase。 其次 Phoenix 的性能表现也非常优异，Phoenix 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 Phoenix 成为了 HBase 最优秀的 SQL 中间层。 参考资料 HBase - Overview","categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"}]},{"title":"Hive条件与日期函数汇总","slug":"Hive条件与日期函数汇总","date":"2021-10-25T05:55:18.000Z","updated":"2021-10-25T06:41:02.030Z","comments":true,"path":"2021/10/25/Hive条件与日期函数汇总/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Hive%E6%9D%A1%E4%BB%B6%E4%B8%8E%E6%97%A5%E6%9C%9F%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/","excerpt":"","text":"条件函数assert_true(BOOLEAN condition) 解释 如果condition不为true，则抛出异常，否则返回null 使用案例 12select assert_true(1&lt;2) -- 返回nullselect assert_true(1&gt;2) -- 抛出异常 coalesce(T v1, T v2, …) 解释 返回第一个不为null的值，如果都为null，则返回null 使用案例 123select coalesce(null,1,2,null) -- 返回1select coalesce(1,null) -- 返回1select coalesce(null,null) -- 返回null if(BOOLEAN testCondition,valueTrue, valueFalseOrNull) 解释 如果testCondition条件为true，则返回第一个值，否则返回第二个值 使用案例 12select if(1 is null,0,1) -- 返回1select if(null is null,0,1) -- 返回0 isnotnull(a) 解释 如果参数a不为null，则返回true，否则返回false 使用案例 12select isnotnull(1) -- 返回trueselect isnotnull(null) -- 返回false isnull(a) 解释 与isnotnull相反，如果参数a为null，则返回true，否则返回false 使用案例 12select isnull(null) -- 返回trueselect isnull(1) -- 返回false nullif(a, b) 解释 如果参数a=b，返回null，否则返回a值(Hive2.2.0版本) 使用案例 12select nullif(1,2) -- 返回1select nullif(1,1) -- 返回null nvl(T value, T default_value) 解释 如果value的值为null，则返回default_value默认值，否则返回value的值。在null值判断时，可以使用if函数给定默认值，也可以使用此函数给定默认值，使用该函数sql特别简洁。 使用案例 12select nvl(1,0) -- 返回1select nvl(null,0) -- 返回0 日期函数add_months(DATE|STRING|TIMESTAMP start_date, INT num_months) 解释 &emsp;&emsp;start_date参数可以是string, date 或者timestamp类型，num_months参数时int类型。返回一个日期，该日期是在start_date基础之上加上num_months个月，即start_date之后null_months个月的一个日期。如果start_date的时间部分的数据会被忽略。注意：如果start_date所在月份的天数大于结果日期月的天数，则返回结果月的最后一天的日期。 使用案例 123select add_months(&quot;2020-05-20&quot;,2); -- 返回2020-07-20select add_months(&quot;2020-05-20&quot;,8); -- 返回2021-01-20select add_months(&quot;2020-05-31&quot;,1); -- 返回2020-06-30,5月有31天，6月只有30天，所以返回下一个月的最后一天 current_date 解释 返回查询时刻的当前日期 使用案例 1select current_date() -- 返回当前查询日期2020-05-20 current_timestamp() 解释 返回查询时刻的当前时间 使用案例 1select current_timestamp() -- 2020-05-20 14:40:47.273 datediff(STRING enddate, STRING startdate) 解释 返回开始日期startdate与结束日期enddate之前相差的天数 使用案例 12select datediff(&quot;2020-05-20&quot;,&quot;2020-05-21&quot;); -- 返回-1select datediff(&quot;2020-05-21&quot;,&quot;2020-05-20&quot;); -- 返回1 date_add(DATE startdate, INT days) 解释 在startdate基础上加上几天，然后返回加上几天之后的一个日期 使用案例 12select date_add(&quot;2020-05-20&quot;,1); -- 返回2020-05-21,1表示加1天select date_add(&quot;2020-05-20&quot;,-1); -- 返回2020-05-19，-1表示减一天 date_sub(DATE startdate, INT days) 解释 在startdate基础上减去几天，然后返回减去几天之后的一个日期,功能与date_add很类似 使用案例 12select date_sub(&quot;2020-05-20&quot;,1); -- 返回2020-05-19,1表示减1天select date_sub(&quot;2020-05-20&quot;,-1); -- 返回2020-05-21，-1表示加1天 date_format(DATE|TIMESTAMP|STRING ts, STRING fmt) 解释 将date/timestamp/string类型的值转换为一个具体格式化的字符串。支持java的SimpleDateFormat格式，第二个参数fmt必须是一个常量 使用案例 123456select date_format(&#x27;2020-05-20&#x27;, &#x27;yyyy&#x27;); -- 返回2020select date_format(&#x27;2020-05-20&#x27;, &#x27;MM&#x27;); -- 返回05select date_format(&#x27;2020-05-20&#x27;, &#x27;dd&#x27;); -- 返回20-- 返回2020年05月20日 00时00分00秒select date_format(&#x27;2020-05-20&#x27;, &#x27;yyyy年MM月dd日 HH时mm分ss秒&#x27;) ;select date_format(&#x27;2020-05-20&#x27;, &#x27;yy/MM/dd&#x27;) -- 返回 20/05/20 dayofmonth(STRING date) 解释 返回一个日期或时间的天,与day()函数功能相同 使用案例 1select dayofmonth(&#x27;2020-05-20&#x27;) -- 返回20 extract(field FROM source) 解释 &emsp;&emsp;提取 day, dayofweek, hour, minute, month, quarter, second, week 或者year的值，field可以选择day, dayofweek, hour, minute, month, quarter, second, week 或者year，source必须是一个date、timestamp或者可以转为 date 、timestamp的字符串。注意：Hive 2.2.0版本之后支持该函数 使用案例 123456789select extract(year from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回2020，年select extract(quarter from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回2，季度select extract(month from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回05，月份select extract(week from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回21，同weekofyear，一年中的第几周select extract(dayofweek from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回4,代表星期三select extract(day from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回20，天select extract(hour from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回15，小时select extract(minute from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回21，分钟select extract(second from &#x27;2020-05-20 15:21:34.467&#x27;); -- 返回34，秒 year(STRING date) 解释 返回时间的年份,可以用extract函数替代 使用案例 1select year(&#x27;2020-05-20 15:21:34&#x27;); -- 返回2020 quarter(DATE|TIMESTAMP|STRING a) 解释 返回给定时间或日期的季度，1至4个季度,可以用extract函数替代 使用案例 1select quarter(&#x27;2020-05-20 15:21:34&#x27;); -- 返回2，第2季度 month(STRING date) 解释 返回时间的月份,可以用extract函数替代 使用案例 1select month(&#x27;2020-05-20 15:21:34&#x27;) -- 返回5 day(STRING date) 解释 返回一个日期或者时间的天,可以用extract函数替代 使用案例 12select day(&quot;2020-05-20&quot;); -- 返回20select day(&quot;2020-05-20 15:05:27.5&quot;); -- 返回20 hour(STRING date) 解释 返回一个时间的小时,可以用extract函数替代 使用案例 1select hour(&#x27;2020-05-20 15:21:34&#x27;);-- 返回15 minute(STRING date) 解释 返回一个时间的分钟值,可以用extract函数替代 使用案例 1select minute(&#x27;2020-05-20 15:21:34&#x27;); -- 返回21 second(STRING date) 解释 返回一个时间的秒,可以用extract函数替代 使用案例 1select second(&#x27;2020-05-20 15:21:34&#x27;); --返回34 from_unixtime(BIGINT unixtime [, STRING format]) 解释 将Unix时间戳转换为字符串格式的时间(比如yyyy-MM-dd HH:mm:ss格式) 使用案例 123select from_unixtime(1589960708); -- 返回2020-05-20 15:45:08select from_unixtime(1589960708, &#x27;yyyy-MM-dd hh:mm:ss&#x27;); -- -- 返回2020-05-20 15:45:08select from_unixtime(1589960708, &#x27;yyyy-MM-dd&#x27;); -- 返回2020-05-20 from_utc_timestamp(T a, STRING timezone) 解释 转换为特定时区的时间 使用案例 12345select from_utc_timestamp(&#x27;2020-05-20 15:21:34&#x27;,&#x27;PST&#x27;); -- 返回2020-05-20 08:21:34.0select from_utc_timestamp(&#x27;2020-05-20 15:21:34&#x27;,&#x27;GMT&#x27;); -- 返回2020-05-20 15:21:34.0select from_utc_timestamp(&#x27;2020-05-20 15:21:34&#x27;,&#x27;UTC&#x27;); -- 返回2020-05-20 15:21:34.0select from_utc_timestamp(&#x27;2020-05-20 15:21:34&#x27;,&#x27;DST&#x27;); -- 返回2020-05-20 15:21:34.0select from_utc_timestamp(&#x27;2020-05-20 15:21:34&#x27;,&#x27;CST&#x27;); -- 返回2020-05-20 10:21:34.0 last_day(STRING date) 解释 返回给定时间或日期所在月的最后一天，参数可以是’yyyy-MM-dd HH:mm:ss’ 或者 ‘yyyy-MM-dd’类型，时间部分会被忽略 使用案例 12select last_day(&#x27;2020-05-20 15:21:34&#x27;); -- 返回2020-05-31select last_day(&#x27;2020-05-20&#x27;); -- 返回2020-05-31 to_date(STRING timestamp) 解释 返回一个字符串时间的日期部分，去掉时间部分，2.1.0之前版本返回的是string，2.1.0版本及之后返回的是date 使用案例 12select to_date(&#x27;2020-05-20 15:21:34&#x27;); -- 返回2020-05-20select to_date(&#x27;2020-05-20&#x27;); -- 返回2020-05-20 to_utc_timestamp(T a, STRING timezone) 解释 转换为世界标准时间UTC的时间戳,与from_utc_timestamp类似 使用案例 1select to_utc_timestamp(&#x27;2020-05-20 15:21:34&#x27;, &#x27;GMT&#x27;); -- 返回2020-05-20 15:21:34.0 trunc(STRING date, STRING format) 解释 截断日期到指定的日期精度，仅支持月（MONTH/MON/MM）或者年（YEAR/YYYY/YY） 使用案例 123select trunc(&#x27;2020-05-20&#x27;, &#x27;YY&#x27;); -- 返回2020-01-01，返回年的1月1日select trunc(&#x27;2020-05-20&#x27;, &#x27;MM&#x27;); -- 返回2020-05-01，返回月的第一天select trunc(&#x27;2020-05-20 15:21:34&#x27;, &#x27;MM&#x27;); -- 返回2020-05-01 unix_timestamp([STRING date [, STRING pattern]]) 解释 参数时可选的，当参数为空时，返回当前Unix是时间戳，精确到秒。可以指定一个具体的日期，转换为Unix时间戳格式 使用案例 1234-- 返回1589959294select unix_timestamp(&#x27;2020-05-20 15:21:34&#x27;,&#x27;yyyy-MM-dd hh:mm:ss&#x27;);-- 返回1589904000select unix_timestamp(&#x27;2020-05-20&#x27;,&#x27;yyyy-MM-dd&#x27;); weekofyear(STRING date) 解释 返回一个日期或时间在一年中的第几周，可以用extract替代 使用案例 12select weekofyear(&#x27;2020-05-20 15:21:34&#x27;); -- 返回21，第21周select weekofyear(&#x27;2020-05-20&#x27;); -- 返回21，第21周 next_day(STRING start_date, STRING day_of_week) 解释 参数start_date可以是一个时间或日期，day_of_week表示星期几，比如Mo表示星期一，Tu表示星期二，Wed表示星期三，Thur表示星期四，Fri表示星期五，Sat表示星期六，Sun表示星期日。如果指定的星期几在该日期所在的周且在该日期之后，则返回当周的星期几日期，如果指定的星期几不在该日期所在的周，则返回下一个星期几对应的日期 使用案例 12345678select next_day(&#x27;2020-05-20&#x27;,&#x27;Mon&#x27;);-- 返回当前日期的下一个周一日期:2020-05-25select next_day(&#x27;2020-05-20&#x27;,&#x27;Tu&#x27;);-- 返回当前日期的下一个周二日期:2020-05-26select next_day(&#x27;2020-05-20&#x27;,&#x27;Wed&#x27;);-- 返回当前日期的下一个周三日期:2020-05-27-- 2020-05-20为周三，指定的参数为周四，所以返回当周的周四就是2020-05-21select next_day(&#x27;2020-05-20&#x27;,&#x27;Th&#x27;);select next_day(&#x27;2020-05-20&#x27;,&#x27;Fri&#x27;);-- 返回周五日期2020-05-22select next_day(&#x27;2020-05-20&#x27;,&#x27;Sat&#x27;); -- 返回周六日期2020-05-23select next_day(&#x27;2020-05-20&#x27;,&#x27;Sun&#x27;); -- 返回周六日期2020-05-24 该函数比较重要：比如取当前日期所在的周一和周日，通过长用在按周进行汇总数据 12select date_add(next_day(&#x27;2020-05-20&#x27;,&#x27;MO&#x27;),-7); -- 返回当前日期的周一日期2020-05-18select date_add(next_day(&#x27;2020-05-20&#x27;,&#x27;MO&#x27;),-1); -- 返回当前日期的周日日期2020-05-24 months_between(DATE|TIMESTAMP|STRING date1, … date2) 解释 返回 date1 和 date2 的月份差。如果date1大于date2，返回正值，否则返回负值，如果是相减是整数月，则返回一个整数，否则会返回小数 使用案例 123456select months_between(&#x27;2020-05-20&#x27;,&#x27;2020-05-20&#x27;); -- 返回0select months_between(&#x27;2020-05-20&#x27;,&#x27;2020-06-20&#x27;); -- 返回-1-- 相差的整数月select months_between(&#x27;2020-06-30&#x27;,&#x27;2020-05-31&#x27;); -- 返回1-- 非整数月，一个月差一天select months_between(&#x27;2020-06-29&#x27;,&#x27;2020-05-31&#x27;); -- 返回0.93548387","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"ZooKeeper使用场景","slug":"ZooKeeper使用场景","date":"2021-10-25T05:51:10.000Z","updated":"2021-10-25T06:42:10.326Z","comments":true,"path":"2021/10/25/ZooKeeper使用场景/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/ZooKeeper%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/","excerpt":"","text":"ZooKeeper 是什么&emsp;&emsp;ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 Google 的 Chubby 一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。&emsp;&emsp;客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的 zookeeper 机器来处理。对于写请求，这些请求会同时发给其他 zookeeper 机器并且达成一致后，请求才会返回成功。因此，随着 zookeeper 的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。有序性是 zookeeper 中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为 zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个 zookeeper 最新的 zxid。 Zookeeper 工作原理&emsp;&emsp;Zookeeper 的核心是原子广播，这个机制保证了各个 Server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 Server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 Server 具有相同的系统状态。 ZooKeeper 提供了什么1、文件系统2、通知机制 Zookeeper 文件系统&emsp;&emsp;Zookeeper 提供一个多层级的节点命名空间（节点称为 znode）。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。Zookeeper 为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为 1M。 四种类型的 znode1、PERSISTENT-持久化目录节点客户端与 zookeeper 断开连接后，该节点依旧存在2、PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点客户端与 zookeeper 断开连接后，该节点依旧存在，只是 Zookeeper 给该节点名称进行顺序编号3、EPHEMERAL-临时目录节点客户端与 zookeeper 断开连接后，该节点被删除4、EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点 &emsp;&emsp;客户端与 zookeeper 断开连接后，该节点被删除，只是 Zookeeper 给该节点名称进行顺序编号 Zookeeper 通知机制&emsp;&emsp;client 端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 zk 的通知，然后 client 可以根据 znode 变化来做出业务上的改变等。 zookeeper watch 机制&emsp;&emsp;Watch 机制官方声明：一个 Watch 事件是一个一次性的触发器，当被设置了 Watch 的数据发生了改变的时候，则服务器将这个改变发送给设置了 Watch 的客户端，以便通知它们。Zookeeper 机制的特点： 一次性触发数据发生改变时，一个 watcher event 会被发送到 client，但是 client 只会收到一次这样的信息。 watcher event 异步发送 watcher 的通知事件从 server 发送到 client 是异步的，这就存在一个问题，不同的客户端和服务器之间通过 socket 进行通信，由于网络延迟或其他因素导致客户端在不通的时刻监听到事件，由于 Zookeeper 本身提供了 ordering guarantee，即客户端监听事件后，才会感知它所监视 znode 发生了变化。所以我们使用 Zookeeper 不能期望能够监控到节点每次的变化。Zookeeper 只能保证最终的一致性，而无法保证强一致性。 数据监视 Zookeeper 有数据监视和子数据监视 getdata() and exists()设置数据监视，getchildren()设置了子节点监视。 注册 watcher getData、exists、getChildren 触发 watcher create、delete、setData setData()会触发 znode 上设置的 data watch（如果 set 成功的话）。一个成功的 create() 操作会触发被创建的 znode 上的数据 watch，以及其父节点上的 child watch。而一个成功的 delete()操作将会同时触发一个 znode 的 data watch 和 child watch（因为这样就没有子节点了），同时也会触发其父节点的 childwatch。 当一个客户端连接到一个新的服务器上时，watch 将会被以任意会话事件触发。当与一个服务器失去连接的时候，是无法接收到 watch 的。而当 client 重新连接时，如果需要的话，所有先前注册过的 watch，都会被重新注册。通常这是完全透明的。只有在一个特殊情况下，watch 可能会丢失：对于一个未创建的 znode 的exist watch，如果在客户端断开连接期间被创建了，并且随后在客户端连接上之前又删除了，这种情况下，这个 watch 事件可能会被丢失。 Watch 是轻量级的，其实就是本地 JVM 的 Callback，服务器端只是存了是否有设置了 Watcher 的布尔类型 Zookeeper 做了什么1、命名服务2、配置管理3、集群管理4、分布式锁5、队列管理 zk 的命名服务（文件系统）&emsp;&emsp;命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局的路径，即是唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。 zk 的配置管理（文件系统、通知机制）&emsp;&emsp;程序分布式的部署在不同的机器上，将程序的配置信息放在 zk 的 znode 下，当有配置发生改变时，也就是znode 发生变化时，可以通过改变 zk 中某个目录节点的内容，利用 watcher 通知给各个客户端，从而更改配置。 Zookeeper 集群管理（文件系统、通知机制）&emsp;&emsp;所谓集群管理无在乎两点：是否有机器退出和加入、选举 master。&emsp;&emsp;对于第一点，所有机器约定在父目录下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与zookeeper 的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。&emsp;&emsp;新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount 又有了，对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为 master 就好。 Zookeeper 分布式锁（文件系统、通知机制）&emsp;&emsp;有了 zookeeper 的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。&emsp;&emsp;对于第一类，我们将 zookeeper 上的一个 znode 看作是一把锁，通过 createznode 的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的distribute_lock 节点就释放出锁。&emsp;&emsp;对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master 一样，编号最小的获得锁，用完删除，依次方便。 获取分布式锁的流程&emsp;&emsp;在获取分布式锁的时候在 locker 节点下创建临时顺序节点，释放锁的时候删除该临时节点。客户端调用createNode 方法在 locker 下创建临时顺序节点，然后调用 getChildren(“locker”)来获取 locker 下面的所有子节点，注意此时不用设置任何 Watcher。客户端获取到所有的子节点 path 之后，如果发现自己创建的节点在所有创建的子节点序号最小，那么就认为该客户端获取到了锁。如果发现自己创建的节点并非 locker 所有子节点中最小的，说明自己还没有获取到锁，此时客户端需要找到比自己小的那个节点，然后对其调用 exist()方法，同时对其注册事件监听器。之后，让这个被关注的节点删除，则客户端的 Watcher 会收到相应通知，此时再次判断自己创建的节点是否是 locker 子节点中序号最小的，如果是则获取到了锁，如果不是则重复以上步骤继续获取到比自己小的一个节点并注册监听。当前这个过程中还需要许多的逻辑判断。&emsp;&emsp;代码的实现主要是基于互斥锁，获取分布式锁的重点逻辑在于BaseDistributedLock，实现了基于Zookeeper 实现分布式锁的细节。 Zookeeper 队列管理（文件系统、通知机制）两种类型的队列：1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。2、队列按照 FIFO 方式进行入队和出队操作。第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。在特定的目录下创建PERSISTENT_SEQUENTIAL 节点，创建成功时 Watcher 通知等待的队列，队列删除序列号最小的节点用以消费。此场景下 Zookeeper 的 znode 用于消息存储，znode 存储的数据就是消息队列中的消息内容，SEQUENTIAL 序列号就是消息的编号，按序取出即可。由于创建的节点是持久化的，所以不必担心队列消息的丢失问题。 Zookeeper其他问题Zookeeper 数据复制Zookeeper 作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处：1、容错：一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作；2、提高系统的扩展能力 ：把负载分布到多个节点上，或者增加节点来提高系统的负载能力；3、提高性能：让客户端本地访问就近的节点，提高用户访问速度。从客户端读写访问的透明度来看，数据复制集群系统分下面两种：1、写主(WriteMaster) ：对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离；2、写任意(Write Any)：对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。 对 zookeeper 来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这也是它建立 observer 的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。 zookeeper 保证事务的顺序一致性&emsp;&emsp;zookeeper 采用了递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid，zxid 实际上是一个 64 位的数字，高 32 位是 epoch（时期; 纪元; 世; 新时代）用来标识 leader 是否发生改变，如果有新的 leader 产生出来，epoch 会自增，低 32 位用来递增计数。当新产生 proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。 Zookeeper 下 Server 工作状态每个 Server 在工作过程中有三种状态：LOOKING：当前 Server 不知道 leader 是谁，正在搜寻LEADING：当前 Server 即为选举出来的 leaderFOLLOWING：leader 已经选举出来，当前 Server 与之同步 zookeeper 如何选取主 leader当 leader 崩溃或者 leader 失去大多数的 follower，这时 zk 进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的 Server 都恢复到一个正确的状态。Zk 的选举算法有两种：一种是基于 basic paxos 实现的，另外一种是基于 fast paxos 算法实现的。系统默认的选举算法为 fast paxos。 1、Zookeeper 选主流程(basic paxos)（1）选举线程由当前 Server 发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server；（2）选举线程首先向所有 Server 发起一次询问(包括自己)；（3）选举线程收到回复后，验证是否是自己发起的询问(验证 zxid 是否一致)，然后获取对方的 id(myid)，并存储到当前询问对象列表中，最后获取对方提议的 leader 相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中；（4）收到所有 Server 回复以后，就计算出 zxid 最大的那个 Server，并将这个 Server 相关信息设置成下一次要投票的 Server；（5）线程将当前 zxid 最大的 Server 设置为当前 Server 要推荐的 Leader，如果此时获胜的 Server 获得 n/2+ 1 的 Server 票数，设置当前推荐的 leader 为获胜的 Server，将根据获胜的 Server 相关信息设置自己的状态，否则，继续这个过程，直到 leader 被选举出来。 通过流程分析我们可以得出：要使 Leader 获得多数Server 的支持，则 Server 总数必须是奇数 2n+1，且存活的 Server 的数目不得少于 n+1. 每个 Server 启动后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的 server 还会从磁盘快照中恢复数据和会话信息，zk 会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。 2、Zookeeper 选主流程(basic paxos)fast paxos 流程是在选举过程中，某 Server 首先向所有 Server 提议自己要成为 leader，当其它 Server 收到提议以后，解决 epoch 和 zxid 的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出 Leader。 Zookeeper 同步流程选完 Leader 以后，zk 就进入状态同步过程。1、Leader 等待 server 连接；2、Follower 连接 leader，将最大的 zxid 发送给 leader；3、Leader 根据 follower 的 zxid 确定同步点；4、完成同步后通知 follower 已经成为 uptodate 状态；5、Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。 机器中为什么会有 leader在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行 leader 选举。 zk 节点宕机如何处理Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。 如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失； 如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader。 ZK 集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在 ZK 节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。所以3 个节点的 cluster 可以挂掉 1 个节点(leader 可以得到 2 票&gt;1.5)2 个节点的 cluster 就不能挂掉任何 1 个节点了(leader 可以得到 1 票&lt;=1) zookeeper 负载均衡和 nginx 负载均衡区别zk 的负载均衡是可以调控，nginx 只是能调权重，其他需要可控的都需要自己写插件；但是 nginx 的吞吐量比zk 大很多，应该说按业务选择用哪种方式。","categories":[],"tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"https://www.ihadyou.cn/tags/zookeeper/"}]},{"title":"Sqoop一致性探讨","slug":"Sqoop一致性探讨","date":"2021-10-25T05:49:32.000Z","updated":"2021-10-25T06:41:50.836Z","comments":true,"path":"2021/10/25/Sqoop一致性探讨/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Sqoop%E4%B8%80%E8%87%B4%E6%80%A7%E6%8E%A2%E8%AE%A8/","excerpt":"","text":"Sqoop导入导出Null存储一致性问题Hive中的Null在底层是以“\\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用–input-null-string和–input-null-non-string两个参数。导入数据时采用–null-string和–null-non-string。 Sqoop数据导出一致性问题场景1：如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。 解决方案：由于Sqoop将导出过程分解为多个事务，因此失败的导出作业可能会导致部分数据提交到数据库。在某些情况下，这可能进一步导致后续作业因插入冲突而失败，在其他情况下，这又可能导致数据重复。您可以通过–staging-table选项指定暂存表来解决此问题，该选项用作用于暂存导出数据的辅助表。最后，分阶段处理的数据将在单个事务中移至目标表 命令：1234567891011sqoop export \\--connect jdbc:mysql://192.168.137.10:3306/user_behavior \\--username root \\--password 123456 \\--table app_cource_study_report \\--columns watch_video_cnt,complete_video_cnt,dt \\--fields-terminated-by &quot;\\t&quot; \\--export-dir &quot;/user/hive/warehouse/tmp.db/app_cource_study_analysis_$&#123;day&#125;&quot; \\--staging-table app_cource_study_report_tmp \\--clear-staging-table \\--input-null-string &#x27;\\N&#x27;","categories":[],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"https://www.ihadyou.cn/tags/sqoop/"}]},{"title":"Sqoop常用命令及参数","slug":"Sqoop常用命令及参数","date":"2021-10-25T05:48:34.000Z","updated":"2021-10-25T06:42:01.644Z","comments":true,"path":"2021/10/25/Sqoop常用命令及参数/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Sqoop%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%8F%8A%E5%8F%82%E6%95%B0/","excerpt":"","text":"Sqoop 常用命令及参数常用命令列举 序号 命令 类 说明 1 import ImportTool 将数据导入到集群 2 export ExportTool 将集群数据导出 3 codegen CodeGenTool 获取数据库中某张表数据生成Java 并打包Jar 4 create-hive-table CreateHiveTableTool 创建 Hive 表 5 eval EvalSqlTool 查看 SQL 执行结果 6 import-all-tables ImportAllTablesTool 导入某个数据库下所有表到 HDFS 中 7 job JobTool 用来生成一个 sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务 8 list-databases ListDatabasesTool 列出所有数据库名 9 list-tables ListTablesTool 列出某个数据库下所有表 10 merge MergeTool 将 HDFS 中不同目录下面的数据合在一起，并存放在指定的目录中 11 metastore MetastoreTool 记录 sqoop job 的元数据信息，如果不启动 metastore 实例，则默认的元数据存储目录为：~/.sqoop，如果要更改存储目录，可以 在 配 置 文 件sqoop-site.xml 中进行更改 命令&amp;参数详解对于不同的命令，有不同的参数. 公用参数：数据库连接 序号 参数 说明 1 –connect 连接关系型数据库的 URL 2 –connection-manager 指定要使用的连接管理类 3 –driver Hadoop 根目录 4 –help 打印帮助信息 5 –password 连接数据库的密码 6 –username 连接数据库的用户名 7 –verbose 在控制台打印出详细信息 公用参数：import 序号 参数 说明 1 –enclosed-by 给字段值前加上指定的字符 2 –escaped-by 对字段中的双引号加转义符 3 –fields-terminated-by 设定每个字段是以什么符号作为结束，默认为逗号 4 –lines-terminated-by 设定每行记录之间的分隔符，默认是\\n 5 –mysql-delimiters Mysql 默认的分隔符设置，字段之间以逗号分隔，行之间以\\n 分隔，默认转义符是\\，字段值以单引号包裹。 6 –optionally-enclosed-by 给带有双引号或单引号的字段值前后加上指定字符。 公用参数：export 序号 参数 说明 1 –input-enclosed-by 对字段值前后加上指定字符 2 –input-escaped-by 对含有转移符的字段做转义处理 3 –input-fields-terminated-by 字段之间的分隔符 4 –input-lines-terminated-by 行之间的分隔符 5 –input-optionally-enclosed-by 给带有双引号或单引号的字段前后加上指定字符 公用参数：hive 序号 参数 说明 1 –hive-delims-replacement 用自定义的字符串替换掉数据中的\\r\\n和\\013 \\010等字符 2 –hive-drop-import-delims 在导入数据到 hive 时，去掉数据中的\\r\\n\\013\\010 这样的字符 3 –map-column-hive 生成 hive 表时，可以更改生成字段的数据类型 4 –hive-partition-key 创建分区，后面直接跟分区名，分区字段的默认类型为string 5 –hive-partition-value 导入数据时，指定某个分区的值 6 –hive-home hive 的安装目录，可以通过该参数覆盖之前默认配置的目录 7 –hive-import 将数据从关系数据库中导入到 hive 表中 8 –hive-overwrite 覆盖掉在 hive 表中已经存在的数据 9 –create-hive-table 默认是 false，即，如果目标表已经存在了，那么创建任务失败。 10 –hive-table 后面接要创建的 hive 表,默认使用 MySQL 的表名 11 –table 指定关系数据库的表名 命令&amp;参数：import将关系型数据库中的数据导入到 HDFS（包括 Hive，HBase）中，如果导入的是 Hive，那么当 Hive 中没有对应表时，则自动创建。 1) 命令：如：导入数据到 hive 中123456$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--hive-import 如：增量导入数据到 hive 中，mode=appendappend 导入： 1234567891011$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; \\--target-dir /user/hive/warehouse/staff_hive \\--check-column id \\--incremental append \\--last-value 3 尖叫提示：append 不能与–hive-等参数同时使用（Append mode for hive imports is not yet supported. Please remove the parameter –append-mode） 如：增量导入数据到 hdfs 中，mode=lastmodified先在 mysql 中建表并插入几条数据： 12345mysql&gt; create table company.staff_timestamp(id int(4), name varchar(255), sex varchar(255),last_modified timestamp DEFAULT CURRENT_TIMESTAMP ON UPDATECURRENT_TIMESTAMP);mysql&gt; insert into company.staff_timestamp (id, name, sex) values(1, &#x27;AAA&#x27;, &#x27;female&#x27;);mysql&gt; insert into company.staff_timestamp (id, name, sex) values(2, &#x27;BBB&#x27;, &#x27;female&#x27;); 先导入一部分数据： 1234567$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff_timestamp \\--delete-target-dir \\--m 1 再增量导入一部分数据： 1234567891011mysql&gt; insert into company.staff_timestamp (id, name, sex) values(3, &#x27;CCC&#x27;, &#x27;female&#x27;);$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff_timestamp \\--check-column last_modified \\--incremental lastmodified \\--last-value &quot;2017-09-28 22:20:38&quot; \\--m 1 \\--append 尖叫提示：使用 lastmodified 方式导入数据要指定增量数据是要–append（追加）还是要–merge-key（合并）尖叫提示：last-value 指定的值是会包含于增量导入的数据中 2) 参数： 序号 参数 说明 1 - -append 将数据追加到 HDFS 中已经存在的 DataSet 中，如果使用该参数，sqoop 会把数据先导入到临时文件目录，再合并。 2 - -as-avrodatafile 将数据导入到一个 Avro 数据文件中 3 - -as-sequencefile 将数据导入到一个 sequence文件中 4 - -as-textfile 将数据导入到一个普通文本文件中 5 - -boundary-query 边界查询，导入的数据为该参数的值（一条 sql 语句）所执行的结果区间内的数据。 6 - -columns &lt;col1, col2, col3&gt; 指定要导入的字段 7 - -direct 直接导入模式，使用的是关系数据库自带的导入导出工具，以便加快导入导出过程。 8 - -direct-split-size 在使用上面 direct 直接导入的基础上，对导入的流按字节分块，即达到该阈值就产生一个新的文件 9 - -inline-lob-limit 设定大对象数据类型的最大值 10 –m 或–num-mappers 启动 N 个 map 来并行导入数据，默认 4 个。 11 –query 或–e 将查询结果的数据导入，使用时必须伴随参–target-dir，–hive-table，如果查询中有where 条件，则条件后必须加上$CONDITIONS 关键字 12 –split-by 按照某一列来切分表的工作单元，不能与–autoreset-to-one-mapper 连用 13 –table 关系数据库的表名 14 –target-dir 指定 HDFS 路径 15 –warehouse-dir 与 14 参数不能同时使用，导入数据到 HDFS 时指定的目录 16 –where 从关系数据库导入数据时的查询条件 17 –z 或–compress 允许压缩 18 –compression-codec 指定 hadoop 压缩编码类，默认为 gzip(Use Hadoop codecdefault gzip) 19 –null-string string 类型的列如果 null，替换为指定字符串 20 –null-non-string 非 string 类型的列如果 null，替换为指定字符串 21 –check-column 作为增量导入判断的列名 22 –incremental mode：append 或 lastmodified 23 –last-value 指定某一个值，用于标记增量导入的位置 命令&amp;参数：export从 HDFS（包括 Hive 和 HBase）中奖数据导出到关系型数据库中。 1) 命令：12345678$ bin/sqoop export \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--export-dir /user/company \\--input-fields-terminated-by &quot;\\t&quot; \\--num-mappers 1 2) 参数： 序号 参数 说明 1 –direct 利用数据库自带的导入导出工具，以便于提高效率 2 –export-dir 存放数据的 HDFS 的源目录 3 m 或–num-mappers 启动 N 个 map 来并行导入数据，默认 4 个 4 –table 指定导出到哪个 RDBMS 中的表 5 –update-key 对某一列的字段进行更新操作 6 –update-mode updateonlyallowinsert(默认) 7 –input-null-string 请参考 import 8 –input-null-non-string 请参考 import 9 –staging-table 创建一张临时表，用于存放所有事务的结果，然后将所有事务结果一次性导入到目标表中，防止错误。 10 –clear-staging-table 如果第 9 个参数非空，则可以在导出操作执行前，清空临时事务结果表 命令&amp;参数：codegen将关系型数据库中的表映射为一个 Java 类，在该类中有各列对应的各个字段。 1) 命令：12345678$ bin/sqoop codegen \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--bindir /home/admin/Desktop/staff \\--class-name Staff \\--fields-terminated-by &quot;\\t&quot; 2) 参数： 序号 参数 说明 1 –bindir 指定生成的 Java 文件、编译成的 class 文件及将生成文件打包为 jar 的文件输出路径 2 –class-name 设定生成的 Java 文件指定的名称 3 –outdir 生成 Java 文件存放的路径 4 –package-name 包名，如 com.z，就会生成 com和 z 两级目录 5 –input-null-non-string 在生成的 Java 文件中，可以将 null 字符串或者不存在的字符串设置为想要设定的值（例如空字符串） 6 –input-null-string 将 null 字符串替换成想要替换的值（一般与 5 同时使用） 7 –map-column-java 数据库字段在生成的 Java 文件中会映射成各种属性，且默认的数据类型与数据库类型保持对应关系。该参数可以改变默认类型，例如：–map-column-java id=long,name=String 8 –null-non-string 在生成 Java 文件时，可以将不存在或者 null 的字符串设置为其他值 9 –null-string 在生成 Java 文件时，将 null字符串设置为其他值（一般与8 同时用） 10 –table 对应关系数据库中的表名，生成的 Java 文件中的各个属性与该表的各个字段一一对应 命令&amp;参数：create-hive-table生成与关系数据库表结构对应的 hive 表结构。 1) 命令：123456$ bin/sqoop create-hive-table \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--hive-table hive_staff 2) 参数： 序号 参数 说明 1 –hive-home Hive 的安装目录，可以通过该参数覆盖掉默认的 Hive 目录 2 –hive-overwrite 覆盖掉在 Hive 表中已经存在的数据 3 –create-hive-table 默认是 false，如果目标表已经存在了，那么创建任务会失败 4 –hive-table 后面接要创建的 hive 表 5 –table 指定关系数据库的表名 命令&amp;参数：eval可以快速的使用 SQL 语句对关系型数据库进行操作，经常用于在 import 数据之前，了解一下 SQL 语句是否正确，数据是否正常，并可以将结果显示在控制台。 1) 命令：12345$ bin/sqoop eval \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--query &quot;SELECT * FROM staff&quot; 2) 参数： 序号 参数 说明 1 –query 或–e 后跟查询的 SQL 语句 命令&amp;参数：import-all-tables可以将 RDBMS 中的所有表导入到 HDFS 中，每一个表都对应一个 HDFS 目录 1) 命令：12345$ bin/sqoop import-all-tables \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--warehouse-dir /all_tables 2) 参数这些参数的含义均和 import 对应的含义一致| 序号 | 参数 || :———— | :———— || 1 | –as-avrodatafile || 2 | –as-sequencefile || 3 | –as-textfile || 4 | –direct || 5 | –direct-split-size || 6 | –inline-lob-limit || 7 | –m 或—num-mappers || 8 | –warehouse-dir || 9 | -z 或–compress || 10 | –compression-codec | 命令&amp;参数：job用来生成一个 sqoop 任务，生成后不会立即执行，需要手动执行。 1) 命令：123456789$ bin/sqoop job \\--create myjob -- import-all-tables \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000$ bin/sqoop job \\--list$ bin/sqoop job \\--exec myjob 尖叫提示：注意 import-all-tables 和它左边的–之间有一个空格尖叫提示：如果需要连接 metastore，则–meta-connect jdbc:hsqldb:hsql://linux01:16000/sqoop 2) 参数 序号 参数 说明 1 –create 创建 job 参数 2 –delete 删除一个 job 3 –exec 执行一个 job 4 –help 显示 job 帮助 5 –list 显示 job 列表 6 –meta-connect 用来连接 metastore 服务 7 –show 显示一个 job 的信息 8 –verbose 打印命令运行时的详细信息 尖叫提示：在执行一个 job 时，如果需要手动输入数据库密码，可以做如下优化 12345&lt;property&gt;&lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;description&gt;If true, allow saved passwords in the metastore.&lt;/description&gt;&lt;/property&gt; 命令&amp;参数：list-databases1234$ bin/sqoop list-databases \\--connect jdbc:mysql://hadoop102:3306/ \\--username root \\--password 000000 命令&amp;参数：list-tables1234$ bin/sqoop list-tables \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 命令&amp;参数：merge将 HDFS 中不同目录下面的数据合并在一起并放入指定目录中 数据环境：12345678910new_staff1 AAA male2 BBB male3 CCC male4 DDD maleold_staff1 AAA female2 CCC female3 BBB female6 DDD female 尖叫提示：上边数据的列之间的分隔符应该为\\t，行与行之间的分割符为\\n，如果直接复制，请检查之。 命令：创建 JavaBean： 12345678$ bin/sqoop codegen \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--bindir /home/admin/Desktop/staff \\--class-name Staff \\--fields-terminated-by &quot;\\t&quot; 开始合并： 1234567$ bin/sqoop merge \\--new-data /test/new/ \\--onto /test/old/ \\--target-dir /test/merged \\--jar-file /home/admin/Desktop/staff/Staff.jar \\--class-name Staff \\--merge-key id 结果： 123451 AAA MALE2 BBB MALE3 CCC MALE4 DDD MALE6 DDD FEMALE 参数： 序号 参数 说明 1 –new-data HDFS 待合并的数据目录，合并后在新的数据集中保留 2 –onto HDFS 合并后，重复的部分在新的数据集中被覆盖 3 –merge-key 合并键，一般是主键 ID 4 –jar-file 合并时引入的 jar 包，该 jar包是通过 Codegen 工具生成的 jar 包 5 –class-name 对应的表名或对象名，该class 类是包含在 jar 包中的 6 –target-dir 合并后的数据在 HDFS 里存放的目录 命令&amp;参数：metastore记录了 Sqoop job 的元数据信息，如果不启动该服务，那么默认 job 元数据的存储目录为~/.sqoop，可在 sqoop-site.xml 中修改。启动 sqoop 的 metastore 服务 1$ bin/sqoop metastore 关闭 sqoop 的 metastore 服务 1$ bin/sqoop metastore --shutdown","categories":[],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"https://www.ihadyou.cn/tags/sqoop/"}]},{"title":"Sqoop使用案例","slug":"Sqoop使用案例","date":"2021-10-25T05:47:30.000Z","updated":"2021-10-25T06:41:56.722Z","comments":true,"path":"2021/10/25/Sqoop使用案例/","link":"","permalink":"https://www.ihadyou.cn/2021/10/25/Sqoop%E4%BD%BF%E7%94%A8%E6%A1%88%E4%BE%8B/","excerpt":"","text":"Sqoop 原理将导入或导出命令翻译成 mapreduce 程序来实现。在翻译出的 mapreduce 中主要是对 inputformat 和 outputformat 进行定制。 测试 Sqoop 是否能够成功连接数据库12$ bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/--username root --password 000000 Sqoop 的简单使用案例导入数据在 Sqoop 中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做：导入，即使用 import 关键字。 （1）全部导入123456789$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--target-dir /user/company \\--delete-target-dir \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; （2）查询导入123456789$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--target-dir /user/company \\--delete-target-dir \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; \\--query &#x27;select name,sex from staff where id &lt;=1 and $CONDITIONS;&#x27; 提示：must contain &#39;$CONDITIONS&#39; in WHERE clause.如果 query 后使用的是双引号，则$CONDITIONS 前必须加转移符，防止 shell 识别为自己的变量。 （3）导入指定列12345678910$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--target-dir /user/company \\--delete-target-dir \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; \\--columns id,sex \\--table staff 提示：columns 中如果涉及到多列，用逗号分隔，分隔时不要添加空格 （4）使用 sqoop 关键字筛选查询导入数据12345678910$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--target-dir /user/company \\--delete-target-dir \\--num-mappers 1 \\--fields-terminated-by &quot;\\t&quot; \\--table staff \\--where &quot;id=1&quot; RDBMS 到 Hive12345678910$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--num-mappers 1 \\--hive-import \\--fields-terminated-by &quot;\\t&quot; \\--hive-overwrite \\--hive-table staff_hive 提示：该过程分为两步，第一步将数据导入到 HDFS，第二步将导入到 HDFS 的数据迁移到Hive 仓库，第一步默认的临时目录是/user/atguigu/表名 RDBMS 到 Hbase123456789101112$ bin/sqoop import \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table company \\--columns &quot;id,name,sex&quot; \\--column-family &quot;info&quot; \\--hbase-create-table \\--hbase-row-key &quot;id&quot; \\--hbase-table &quot;hbase_company&quot; \\--num-mappers 1 \\--split-by id 提示：sqoop1.4.6 只支持 HBase1.0.1 之前的版本的自动创建 HBase 表的功能解决方案：手动创建 HBase 表 导出数据在 Sqoop 中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群（RDBMS）中传输数据，叫做：导出，即使用 export 关键字。 HIVE/HDFS 到 RDBMS12345678$ bin/sqoop export \\--connect jdbc:mysql://hadoop102:3306/company \\--username root \\--password 000000 \\--table staff \\--num-mappers 1 \\--export-dir /user/hive/warehouse/staff_hive \\--input-fields-terminated-by &quot;\\t&quot; 提示：Mysql 中如果表不存在，不会自动创建 脚本打包使用 opt 格式的文件打包 sqoop 命令，然后执行 1) 创建一个.opt 文件12$ mkdir opt$ touch opt/job_HDFS2RDBMS.opt 2) 编写 sqoop 脚本12345678910111213141516$ vi opt/job_HDFS2RDBMS.optexport--connectjdbc:mysql://hadoop102:3306/company--usernameroot--password000000--tablestaff--num-mappers1--export-dir/user/hive/warehouse/staff_hive--input-fields-terminated-by&quot;\\t&quot; 3) 执行该脚本1$ bin/sqoop --options-file opt/job_HDFS2RDBMS.opt","categories":[],"tags":[{"name":"sqoop","slug":"sqoop","permalink":"https://www.ihadyou.cn/tags/sqoop/"}]},{"title":"Hive常用查询函数","slug":"hive常用查询函数","date":"2021-10-15T07:41:17.000Z","updated":"2021-10-25T06:40:53.231Z","comments":true,"path":"2021/10/15/hive常用查询函数/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/hive%E5%B8%B8%E7%94%A8%E6%9F%A5%E8%AF%A2%E5%87%BD%E6%95%B0/","excerpt":"","text":"常用查询函数1 空字段赋值NVL函数1.函数说明&emsp;&emsp;NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。 2.数据准备：采用员工表 3.查询：如果员工的comm为NULL，则用-1代替 1hive (default)&gt; select nvl(comm,-1) from emp; 4.查询：如果员工的comm为NULL，则用领导id代替 1hive (default)&gt; select nvl(comm,mgr) from emp; 2.CASE WHEN函数 数据准备 name dept_id sex 悟空 A 男 大海 A 男 宋宋 B 男 凤姐 A 女 婷姐 B 女 婷婷 B 女 2．需求求出不同部门男女各多少人。结果如下： 12A 2 1B 1 2 3．创建本地emp_sex.txt，导入数据[ihadu@hadoop102 datas]$ vi emp_sex.txt悟空 A 男大海 A 男宋宋 B 男凤姐 A 女婷姐 B 女婷婷 B 女 4．创建hive表并导入数据 1234567create table emp_sex(name string,dept_id string,sex string)row format delimited fields terminated by &quot;\\t&quot;;load data local inpath &#x27;/opt/module/datas/emp_sex.txt&#x27; into table emp_sex; 5．按需求查询数据 12345678select dept_id, sum(case sex when &#x27;男&#x27; then 1 else 0 end) male_count, sum(case sex when &#x27;女&#x27; then 1 else 0 end) female_countfrom emp_sexgroup by dept_id; 3.行转列1．相关函数说明&emsp;&emsp;CONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;&emsp;&emsp;CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;&emsp;&emsp;COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。 2．数据准备 name constellation blood_type 孙悟空 白羊座 A 大海 射手座 A 宋宋 白羊座 B 猪八戒 白羊座 A 凤姐 射手座 A 3．需求把星座和血型一样的人归类到一起。结果如下： 123射手座,A 大海|凤姐白羊座,A 孙悟空|猪八戒白羊座,B 宋宋 4．创建本地constellation.txt，导入数据 123456[ihadu@hadoop102 datas]$ vi person_info.txt孙悟空 白羊座 A大海 射手座 A宋宋 白羊座 B猪八戒 白羊座 A凤姐 射手座 A 5．创建hive表并导入数据 1234567create table person_info(name string,constellation string,blood_type string)row format delimited fields terminated by &quot;\\t&quot;;load data local inpath “/opt/module/datas/person_info.txt” into table person_info; 6．按需求查询数据 1234567891011select t1.base, concat_ws(&#x27;|&#x27;, collect_set(t1.name)) namefrom (select name, concat(constellation, &quot;,&quot;, blood_type) base from person_info) t1group by t1.base; 4.列转行1．函数说明Explode(col)：将hive一列中复杂的array或者map结构拆分成多行。 Lateral view用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。 2．数据准备 movie category 《金刚川》 悬疑,动作,科幻,剧情 《我和我的家乡》 悬疑,警匪,动作,心理,剧情 《心灵奇旅》 战争,动作,灾难 3．需求将电影分类中的数组数据展开。结果如下： 123456789101112《金刚川》 悬疑《金刚川》 动作《金刚川》 科幻《金刚川》 剧情《我和我的家乡》 悬疑《我和我的家乡》 警匪《我和我的家乡》 动作《我和我的家乡》 心理《我和我的家乡》 剧情《心灵奇旅》 战争《心灵奇旅》 动作《心灵奇旅》 灾难 4．创建本地movie.txt，导入数据 1234[ihadu@hadoop102 datas]$ vi movie.txt《疑犯追踪》 悬疑,动作,科幻,剧情《Lie to me》 悬疑,警匪,动作,心理,剧情《战狼2》 战争,动作,灾难 5．创建hive表并导入数据 1234567create table movie_info( movie string, category array&lt;string&gt;)row format delimited fields terminated by &quot;\\t&quot;collection items terminated by &quot;,&quot;;load data local inpath &quot;/opt/module/datas/movie.txt&quot; into table movie_info; 6．按需求查询数据 12345select movie, category_namefrom movie_info lateral view explode(category) table_tmp as category_name; 5.窗口函数1．相关函数说明OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化CURRENT ROW：当前行n PRECEDING：往前n行数据n FOLLOWING：往后n行数据UNBOUNDED：起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点LAG(col,n)：往前第n行数据LEAD(col,n)：往后第n行数据NTILE(n)：把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。 2．数据准备：name，orderdate，cost 1234567891011121314jack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94 3．需求（1）查询在2017年4月份购买过的顾客及总人数（2）查询顾客的购买明细及月购买总额（3）上述的场景,要将cost按照日期进行累加（4）查询顾客上次的购买时间（5）查询前20%时间的订单信息 4．创建本地business.txt，导入数据 1[ihadu@hadoop102 datas]$ vi business.txt 5．创建hive表并导入数据 1234567create table business(name string,orderdate string,cost int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;;load data local inpath &quot;/opt/module/datas/business.txt&quot; into table business; 6．按需求查询数据（1）查询在2017年4月份购买过的顾客及总人数 1234select name,count(*) over ()from businesswhere substring(orderdate,1,7) = &#x27;2017-04&#x27;group by name; （2）查询顾客的购买明细及月购买总额 123456select name, orderdate, cost, sum(cost) over(partition by month(orderdate))from business; （3）上述的场景,要将cost按照日期进行累加 123456789select name,orderdate,cost,sum(cost) over() as sample1,--所有行相加sum(cost) over(partition by name) as sample2,--按name分组，组内数据相加sum(cost) over(partition by name order by orderdate) as sample3,--按name分组，组内数据累加sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和sample3一样,由起点到当前行的聚合sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行from business; （4）查看顾客上次的购买时间 1234select name,orderdate,cost,lag(orderdate,1,&#x27;1900-01-01&#x27;) over(partition by name order by orderdate ) as time1,lag(orderdate,2) over (partition by name order by orderdate) as time2from business; （5）查询前20%时间的订单信息 12345select * from ( select name,orderdate,cost, ntile(5) over(order by orderdate) sorted from business) twhere sorted = 1; 6.Rank函数1．函数说明 RANK() 排序相同时会重复，总数不会变 DENSE_RANK() 排序相同时会重复，总数会减少 ROW_NUMBER() 会根据顺序计算 2．数据准备 name subject score 孙悟空 语文 87 孙悟空 数学 95 孙悟空 英语 68 大海 语文 94 大海 数学 56 大海 英语 84 宋宋 语文 64 宋宋 数学 86 宋宋 英语 84 婷婷 语文 65 婷婷 数学 85 婷婷 英语 78 3．需求计算每门学科成绩排名。 4．创建本地movie.txt，导入数据 1[ihadu@hadoop102 datas]$ vi score.txt 5．创建hive表并导入数据 123456create table score(name string,subject string, score int) row format delimited fields terminated by &quot;\\t&quot;;load data local inpath &#x27;/opt/module/datas/score.txt&#x27; into table score; 6．按需求查询数据 1234567select name,subject,score,rank() over(partition by subject order by score desc) rp,dense_rank() over(partition by subject order by score desc) drp,row_number() over(partition by subject order by score desc) rmpfrom score; 查询结果： name subject score rp drp rmp 孙悟空 数学 95 1 1 1 宋宋 数学 86 2 2 2 婷婷 数学 85 3 3 3 大海 数学 56 4 4 4 宋宋 英语 84 1 1 1 大海 英语 84 1 1 2 婷婷 英语 78 3 2 3 孙悟空 英语 68 4 3 4 大海 语文 94 1 1 1 孙悟空 语文 87 2 2 2 婷婷 语文 65 3 3 3 宋宋 语文 64 4 4 4","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"Hive数据查询实战","slug":"Hive数据查询实战","date":"2021-10-15T07:39:23.000Z","updated":"2021-10-25T06:41:02.050Z","comments":true,"path":"2021/10/15/Hive数据查询实战/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/Hive%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E5%AE%9E%E6%88%98/","excerpt":"","text":"一、数据准备为了演示查询操作，这里需要预先创建三张表，并加载测试数据。 数据文件 emp.txt 和 dept.txt 可以从本仓库的resources 目录下载。 1.1 员工表1234567891011121314 -- 建表语句 CREATE TABLE emp( empno INT, -- 员工表编号 ename STRING, -- 员工姓名 job STRING, -- 职位类型 mgr INT, hiredate TIMESTAMP, --雇佣日期 sal DECIMAL(7,2), --工资 comm DECIMAL(7,2), deptno INT) --部门编号 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot;; --加载数据LOAD DATA LOCAL INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE INTO TABLE emp; 1.2 部门表12345678910-- 建表语句CREATE TABLE dept( deptno INT, --部门编号 dname STRING, --部门名称 loc STRING --部门所在的城市)ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot;;--加载数据LOAD DATA LOCAL INPATH &quot;/usr/file/dept.txt&quot; OVERWRITE INTO TABLE dept; 1.3 分区表这里需要额外创建一张分区表，主要是为了演示分区查询： 123456789101112131415161718CREATE EXTERNAL TABLE emp_ptn( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot;;--加载数据LOAD DATA LOCAL INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=20)LOAD DATA LOCAL INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=30)LOAD DATA LOCAL INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=40)LOAD DATA LOCAL INPATH &quot;/usr/file/emp.txt&quot; OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=50) 二、单表查询2.1 SELECT12-- 查询表中全部数据SELECT * FROM emp; 2.2 WHERE12-- 查询 10 号部门中员工编号大于 7782 的员工信息 SELECT * FROM emp WHERE empno &gt; 7782 AND deptno = 10; 2.3 DISTINCTHive 支持使用 DISTINCT 关键字去重。 12-- 查询所有工作类型SELECT DISTINCT job FROM emp; 2.4 分区查询分区查询 (Partition Based Queries)，可以指定某个分区或者分区范围。 123-- 查询分区表中部门编号在[20,40]之间的员工SELECT emp_ptn.* FROM emp_ptnWHERE emp_ptn.deptno &gt;= 20 AND emp_ptn.deptno &lt;= 40; 2.5 LIMIT12-- 查询薪资最高的 5 名员工SELECT * FROM emp ORDER BY sal DESC LIMIT 5; 2.6 GROUP BYHive 支持使用 GROUP BY 进行分组聚合操作。 1234set hive.map.aggr=true;-- 查询各个部门薪酬综合SELECT deptno,SUM(sal) FROM emp GROUP BY deptno; hive.map.aggr 控制程序如何进行聚合。默认值为 false。如果设置为 true，Hive 会在 map 阶段就执行一次聚合。这可以提高聚合效率，但需要消耗更多内存。 2.7 ORDER AND SORT可以使用 ORDER BY 或者 Sort BY 对查询结果进行排序，排序字段可以是整型也可以是字符串：如果是整型，则按照大小排序；如果是字符串，则按照字典序排序。ORDER BY 和 SORT BY 的区别如下： 使用 ORDER BY 时会有一个 Reducer 对全部查询结果进行排序，可以保证数据的全局有序性； 使用 SORT BY 时只会在每个 Reducer 中进行排序，这可以保证每个 Reducer 的输出数据是有序的，但不能保证全局有序。 由于 ORDER BY 的时间可能很长，如果你设置了严格模式 (hive.mapred.mode = strict)，则其后面必须再跟一个 limit 子句。 注 ：hive.mapred.mode 默认值是 nonstrict ，也就是非严格模式。 12-- 查询员工工资，结果按照部门升序，按照工资降序排列SELECT empno, deptno, sal FROM emp ORDER BY deptno ASC, sal DESC; 2.8 HAVING可以使用 HAVING 对分组数据进行过滤。 12-- 查询工资总和大于 9000 的所有部门SELECT deptno,SUM(sal) FROM emp GROUP BY deptno HAVING SUM(sal)&gt;9000; 2.9 DISTRIBUTE BY如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这可以使用 DISTRIBUTE BY 字句。需要注意的是，DISTRIBUTE BY 虽然能把具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。情况如下： 把以下 5 个数据发送到两个 Reducer 上进行处理： 12345k1k2k4k3k1 Reducer1 得到如下乱序数据： 123k1k2k1 Reducer2 得到数据如下： 12k4k3 如果想让 Reducer 上的数据是有序的，可以结合 SORT BY 使用 (示例如下)，或者使用下面我们将要介绍的 CLUSTER BY。 12-- 将数据按照部门分发到对应的 Reducer 上处理SELECT empno, deptno, sal FROM emp DISTRIBUTE BY deptno SORT BY deptno ASC; 2.10 CLUSTER BY如果 SORT BY 和 DISTRIBUTE BY 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 CLUSTER BY 进行替换，同时 CLUSTER BY 可以保证数据在全局是有序的。 1SELECT empno, deptno, sal FROM emp CLUSTER BY deptno ; 三、多表联结查询Hive 支持内连接，外连接，左外连接，右外连接，笛卡尔连接，这和传统数据库中的概念是一致的，可以参见下图。 需要特别强调：JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定，否则就会先做笛卡尔积，再过滤，这会导致你得不到预期的结果 (下面的演示会有说明)。 3.1 INNER JOIN12345678-- 查询员工编号为 7369 的员工的详细信息SELECT e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptno WHERE empno=7369;--如果是三表或者更多表连接，语法如下SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 3.2 LEFT OUTER JOINLEFT OUTER JOIN 和 LEFT JOIN 是等价的。 1234-- 左连接SELECT e.*,d.*FROM emp e LEFT OUTER JOIN dept dON e.deptno = d.deptno; 3.3 RIGHT OUTER JOIN1234--右连接SELECT e.*,d.*FROM emp e RIGHT OUTER JOIN dept dON e.deptno = d.deptno; 执行右连接后，由于 40 号部门下没有任何员工，所以此时员工信息为 NULL。这个查询可以很好的复述上面提到的——JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定。你可以把 ON 改成 WHERE，你会发现无论如何都查不出 40 号部门这条数据，因为笛卡尔运算不会有 (NULL, 40) 这种情况。 3.4 FULL OUTER JOIN123SELECT e.*,d.*FROM emp e FULL OUTER JOIN dept dON e.deptno = d.deptno; 3.5 LEFT SEMI JOINLEFT SEMI JOIN （左半连接）是 IN/EXISTS 子查询的一种更高效的实现。 JOIN 子句中右边的表只能在 ON 子句中设置过滤条件; 查询结果只包含左边表的数据，所以只能 SELECT 左表中的列。 12345678-- 查询在纽约办公的所有员工信息SELECT emp.*FROM emp LEFT SEMI JOIN dept ON emp.deptno = dept.deptno AND dept.loc=&quot;NEW YORK&quot;;--上面的语句就等价于SELECT emp.* FROM empWHERE emp.deptno IN (SELECT deptno FROM dept WHERE loc=&quot;NEW YORK&quot;); 3.6 JOIN笛卡尔积连接，这个连接日常的开发中可能很少遇到，且性能消耗比较大，基于这个原因，如果在严格模式下 (hive.mapred.mode = strict)，Hive 会阻止用户执行此操作。 1SELECT * FROM emp JOIN dept; 四、JOIN优化4.1 STREAMTABLE在多表进行联结的时候，如果每个 ON 字句都使用到共同的列（如下面的 b.key），此时 Hive 会进行优化，将多表 JOIN 在同一个 map / reduce 作业上进行。同时假定查询的最后一个表（如下面的 c 表）是最大的一个表，在对每行记录进行 JOIN 操作时，它将尝试将其他的表缓存起来，然后扫描最后那个表进行计算。因此用户需要保证查询的表的大小从左到右是依次增加的。 1`SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key) JOIN c ON (c.key = b.key)` 然后，用户并非需要总是把最大的表放在查询语句的最后面，Hive 提供了 /*+ STREAMTABLE() */ 标志，用于标识最大的表，示例如下： 1234SELECT /*+ STREAMTABLE(d) */ e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptnoWHERE job=&#x27;CLERK&#x27;; 4.2 MAPJOIN如果所有表中只有一张表是小表，那么 Hive 把这张小表加载到内存中。这时候程序会在 map 阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在 map 就进行了 JOIN 操作，从而可以省略 reduce 过程，这样效率可以提升很多。Hive 中提供了 /*+ MAPJOIN() */ 来标记小表，示例如下： 1234SELECT /*+ MAPJOIN(d) */ e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptnoWHERE job=&#x27;CLERK&#x27;; 五、SELECT的其他用途查看当前数据库： 1SELECT current_database() 六、本地模式在上面演示的语句中，大多数都会触发 MapReduce, 少部分不会触发，比如 select * from emp limit 5 就不会触发 MR，此时 Hive 只是简单的读取数据文件中的内容，然后格式化后进行输出。在需要执行 MapReduce 的查询中，你会发现执行时间可能会很长，这时候你可以选择开启本地模式。 12--本地模式默认关闭，需要手动开启此功能SET hive.exec.mode.local.auto=true; 启用后，Hive 将分析查询中每个 map-reduce 作业的大小，如果满足以下条件，则可以在本地运行它： 作业的总输入大小低于：hive.exec.mode.local.auto.inputbytes.max（默认为 128MB）； map-tasks 的总数小于：hive.exec.mode.local.auto.tasks.max（默认为 4）； 所需的 reduce 任务总数为 1 或 0。 因为我们测试的数据集很小，所以你再次去执行上面涉及 MR 操作的查询，你会发现速度会有显著的提升。 参考资料 LanguageManual Select LanguageManual Joins LanguageManual GroupBy LanguageManual SortBy","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"Hive 常用DML操作","slug":"Hive常用DML操作","date":"2021-10-15T07:36:53.000Z","updated":"2021-10-25T06:40:44.138Z","comments":true,"path":"2021/10/15/Hive常用DML操作/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/Hive%E5%B8%B8%E7%94%A8DML%E6%93%8D%E4%BD%9C/","excerpt":"","text":"一、加载文件数据到表1.1 语法12LOAD DATA [LOCAL] INPATH &#x27;filepath&#x27; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] LOCAL 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件： 从本地文件系统加载文件时， filepath 可以是绝对路径也可以是相对路径 (建议使用绝对路径)； 从 HDFS 加载文件时候，filepath 为文件完整的 URL 地址：如 hdfs://namenode:port/user/hive/project/ data1 filepath 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)； 如果使用 OVERWRITE 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入； 加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区； 加载文件的格式必须与建表时使用 STORED AS 指定的存储格式相同。 使用建议： 不论是本地路径还是 URL 都建议使用完整的。虽然可以使用不完整的 URL 地址，此时 Hive 将使用 hadoop 中的 fs.default.name 配置来推断地址，但是为避免不必要的错误，建议使用完整的本地路径或 URL 地址； 加载对象是分区表时建议显示指定分区。在 Hive 3.0 之后，内部将加载 (LOAD) 重写为 INSERT AS SELECT，此时如果不指定分区，INSERT AS SELECT 将假设最后一组列是分区列，如果该列不是表定义的分区，它将抛出错误。为避免错误，还是建议显示指定分区。 1.2 示例新建分区表： 1234567891011CREATE TABLE emp_ptn( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot;; 从 HDFS 上加载数据到分区表： 1LOAD DATA INPATH &quot;hdfs://hadoop001:8020/mydir/emp.txt&quot; OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=20); emp.txt 文件可在本仓库的 resources 目录中下载 加载后表中数据如下,分区列 deptno 全部赋值成 20： 二、查询结果插入到表2.1 语法12345INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement; Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”=“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 immutable 属性的影响）; 可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区； 从 Hive 1.1.0 开始，TABLE 关键字是可选的； 从 Hive 1.2.0 开始 ，可以采用 INSERT INTO tablename(z，x，c1) 指明插入列； 可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下： 12345FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2][INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...; 2.2 动态插入分区12345INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement;INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; 在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 SELECT 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。 注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。以下是动态分区的相关配置： 配置 默认值 说明 hive.exec.dynamic.partition true 需要设置为 true 才能启用动态分区插入 hive.exec.dynamic.partition.mode strict 在严格模式 (strict) 下，用户必须至少指定一个静态分区，以防用户意外覆盖所有分区，在非严格模式下，允许所有分区都是动态的 hive.exec.max.dynamic.partitions.pernode 100 允许在每个 mapper/reducer 节点中创建的最大动态分区数 hive.exec.max.dynamic.partitions 1000 允许总共创建的最大动态分区数 hive.exec.max.created.files 100000 作业中所有 mapper/reducer 创建的 HDFS 文件的最大数量 hive.error.on.empty.partition false 如果动态分区插入生成空结果，是否抛出异常 2.3 示例 新建 emp 表，作为查询对象表 12345678910111213CREATE TABLE emp( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot;; -- 加载数据到 emp 表中 这里直接从本地加载load data local inpath &quot;/usr/file/emp.txt&quot; into table emp; ​ 完成后 emp 表中数据如下： 为清晰演示，先清空 emp_ptn 表中加载的数据： 1TRUNCATE TABLE emp_ptn; 静态分区演示：从 emp 表中查询部门编号为 20 的员工数据，并插入 emp_ptn 表中，语句如下： 12INSERT OVERWRITE TABLE emp_ptn PARTITION (deptno=20) SELECT empno,ename,job,mgr,hiredate,sal,comm FROM emp WHERE deptno=20; ​ 完成后 emp_ptn 表中数据如下： 接着演示动态分区： 123456-- 由于我们只有一个分区，且还是动态分区，所以需要关闭严格默认。因为在严格模式下，用户必须至少指定一个静态分区set hive.exec.dynamic.partition.mode=nonstrict;-- 动态分区 此时查询语句的最后一列为动态分区列，即 deptnoINSERT OVERWRITE TABLE emp_ptn PARTITION (deptno) SELECT empno,ename,job,mgr,hiredate,sal,comm,deptno FROM emp WHERE deptno=30; ​ 完成后 emp_ptn 表中数据如下： 三、使用SQL语句插入值12INSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES ( value [, value ...] ) 使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）； 如果目标表表支持 ACID 及其事务管理器，则插入后自动提交； 不支持支持复杂类型 (array, map, struct, union) 的插入。 四、更新和删除数据4.1 语法更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。 12345-- 更新UPDATE tablename SET column = value [, column = value ...] [WHERE expression]--删除DELETE FROM tablename [WHERE expression] 4.2 示例1. 修改配置 首先需要更改 hive-site.xml，添加如下配置，开启事务支持，配置完成后需要重启 Hive 服务。 123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hive.support.concurrency&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.enforce.bucketing&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt; &lt;value&gt;nonstrict&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.txn.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.compactor.initiator.on&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.in.test&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 2. 创建测试表 创建用于测试的事务表，建表时候指定属性 transactional = true 则代表该表是事务表。需要注意的是，按照官方文档 的说明，目前 Hive 中的事务表有以下限制： 必须是 buckets Table; 仅支持 ORC 文件格式； 不支持 LOAD DATA …语句。 123456CREATE TABLE emp_ts( empno int, ename String)CLUSTERED BY (empno) INTO 2 BUCKETS STORED AS ORCTBLPROPERTIES (&quot;transactional&quot;=&quot;true&quot;); 3. 插入测试数据 1INSERT INTO TABLE emp_ts VALUES (1,&quot;ming&quot;),(2,&quot;hong&quot;); 插入数据依靠的是 MapReduce 作业，执行成功后数据如下： 4. 测试更新和删除 12345--更新数据UPDATE emp_ts SET ename = &quot;lan&quot; WHERE empno=1;--删除数据DELETE FROM emp_ts WHERE empno=2; 更新和删除数据依靠的也是 MapReduce 作业，执行成功后数据如下： 五、查询结果写出到文件系统5.1 语法123INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] SELECT ... FROM ... OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入； 和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的； 写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下： 1234567-- 定义列分隔符为&#x27;\\t&#x27; insert overwrite local directory &#x27;./test-04&#x27; row format delimited FIELDS TERMINATED BY &#x27;\\t&#x27;COLLECTION ITEMS TERMINATED BY &#x27;,&#x27;MAP KEYS TERMINATED BY &#x27;:&#x27;select * from src; 5.2 示例这里我们将上面创建的 emp_ptn 表导出到本地文件系统，语句如下： 1234INSERT OVERWRITE LOCAL DIRECTORY &#x27;/usr/file/ouput&#x27;ROW FORMAT DELIMITEDFIELDS TERMINATED BY &#x27;\\t&#x27;SELECT * FROM emp_ptn; 导出结果如下：","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"Hive分区表和分桶表","slug":"Hive分区表和分桶表","date":"2021-10-15T07:31:54.000Z","updated":"2021-10-25T06:40:21.507Z","comments":true,"path":"2021/10/15/Hive分区表和分桶表/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/Hive%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8/","excerpt":"","text":"一、分区表1.1 概念Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。 分区为 HDFS 上表目录的子目录，数据按照分区存储在子目录中。如果查询的 where 字句的中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。 这里说明一下分区表并 Hive 独有的概念，实际上这个概念非常常见。比如在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。 1.2 使用场景通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。 1.3 创建分区表在 Hive 中可以使用 PARTITIONED BY 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试： 123456789101112CREATE EXTERNAL TABLE emp_partition( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot; LOCATION &#x27;/hive/emp_partition&#x27;; 1.4 加载数据到分区表加载数据到分区表时候必须要指定数据所处的分区： 1234# 加载部门编号为20的数据到表中LOAD DATA LOCAL INPATH &quot;/usr/file/emp20.txt&quot; OVERWRITE INTO TABLE emp_partition PARTITION (deptno=20)# 加载部门编号为30的数据到表中LOAD DATA LOCAL INPATH &quot;/usr/file/emp30.txt&quot; OVERWRITE INTO TABLE emp_partition PARTITION (deptno=30) 1.5 查看分区目录这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 deptno=20 和 deptno=30,这就是分区目录，分区目录下才是我们加载的数据文件。 1# hadoop fs -ls hdfs://hadoop001:8020/hive/emp_partition/ 这时候当你的查询语句的 where 包含 deptno=20，则就去对应的分区目录下进行查找，而不用扫描全表。 二、分桶表1.1 简介分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。 分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。 1.2 理解分桶表单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。 当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图： 图片引用自：HashMap vs. Hashtable 1.3 创建分桶表在 Hive 中，我们可以通过 CLUSTERED BY 指定分桶列，并通过 SORTED BY 指定桶中数据的排序参考列。下面为分桶表建表语句示例： 123456789101112CREATE EXTERNAL TABLE emp_bucket( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) CLUSTERED BY(empno) SORTED BY(empno ASC) INTO 4 BUCKETS --按照员工编号散列到四个 bucket 中 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot; LOCATION &#x27;/hive/emp_bucket&#x27;; 1.4 加载数据到分桶表这里直接使用 Load 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。 这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下： 1. 设置强制分桶1set hive.enforce.bucketing = true; --Hive 2.x 不需要这一步 在 Hive 0.x and 1.x 版本，必须使用设置 hive.enforce.bucketing = true，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。 2. CTAS导入数据1INSERT INTO TABLE emp_bucket SELECT * FROM emp; --这里的 emp 表就是一张普通的雇员表 可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致： 1.5 查看分桶文件bucket(桶) 本质上就是表目录下的具体文件： 三、分区表和分桶表结合使用分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例： 12345678910111213CREATE TABLE page_view_bucketed( viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING ) PARTITIONED BY(dt STRING) CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\001&#x27; COLLECTION ITEMS TERMINATED BY &#x27;\\002&#x27; MAP KEYS TERMINATED BY &#x27;\\003&#x27; STORED AS SEQUENCEFILE; 此时导入数据时需要指定分区： 123INSERT OVERWRITE page_view_bucketedPARTITION (dt=&#x27;2009-02-25&#x27;)SELECT * FROM page_view WHERE dt=&#x27;2009-02-25&#x27;;","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"Hive常用DDL操作","slug":"Hive常用DDL操作","date":"2021-10-15T07:20:08.000Z","updated":"2021-10-25T06:40:38.425Z","comments":true,"path":"2021/10/15/Hive常用DDL操作/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/Hive%E5%B8%B8%E7%94%A8DDL%E6%93%8D%E4%BD%9C/","excerpt":"","text":"一、Database1.1 查看数据列表1show databases; 1.2 使用数据库1USE database_name; 1.3 新建数据库语法： 1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name --DATABASE|SCHEMA 是等价的 [COMMENT database_comment] --数据库注释 [LOCATION hdfs_path] --存储在 HDFS 上的位置 [WITH DBPROPERTIES (property_name=property_value, ...)]; --指定额外属性 示例： 123CREATE DATABASE IF NOT EXISTS hive_test COMMENT &#x27;hive database for test&#x27; WITH DBPROPERTIES (&#x27;create&#x27;=&#x27;heibaiying&#x27;); 1.4 查看数据库信息语法： 1DESC DATABASE [EXTENDED] db_name; --EXTENDED 表示是否显示额外属性 示例： 1DESC DATABASE EXTENDED hive_test; 1.5 删除数据库语法： 1DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; 默认行为是 RESTRICT，如果数据库中存在表则删除失败。要想删除库及其中的表，可以使用 CASCADE 级联删除。 示例： 1DROP DATABASE IF EXISTS hive_test CASCADE; 二、创建表2.1 建表语法1234567891011121314151617181920CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name --表名 [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] --列名 列数据类型 [COMMENT table_comment] --表描述 [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] --分区表分区规则 [ CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS ] --分桶表分桶规则 [SKEWED BY (col_name, col_name, ...) ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] ] --指定倾斜列和值 [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &#x27;storage.handler.class.name&#x27; [WITH SERDEPROPERTIES (...)] ] -- 指定行分隔符、存储文件格式或采用自定义存储格式 [LOCATION hdfs_path] -- 指定表的存储位置 [TBLPROPERTIES (property_name=property_value, ...)] --指定表的属性 [AS select_statement]; --从查询结果创建表 2.2 内部表12345678910CREATE TABLE emp( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot;; 2.3 外部表1234567891011CREATE EXTERNAL TABLE emp_external( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot; LOCATION &#x27;/hive/emp_external&#x27;; 使用 desc format emp_external 命令可以查看表的详细信息如下： 2.4 分区表123456789101112CREATE EXTERNAL TABLE emp_partition( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot; LOCATION &#x27;/hive/emp_partition&#x27;; 2.5 分桶表123456789101112CREATE EXTERNAL TABLE emp_bucket( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) CLUSTERED BY(empno) SORTED BY(empno ASC) INTO 4 BUCKETS --按照员工编号散列到四个 bucket 中 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot; LOCATION &#x27;/hive/emp_bucket&#x27;; 2.6 倾斜表通过指定一个或者多个列经常出现的值（严重偏斜），Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，这使得性能得到提升。 123456789101112CREATE EXTERNAL TABLE emp_skewed( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) SKEWED BY (empno) ON (66,88,100) --指定 empno 的倾斜值 66,88,100 ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot; LOCATION &#x27;/hive/emp_skewed&#x27;; 2.7 临时表临时表仅对当前 session 可见，临时表的数据将存储在用户的暂存目录中，并在会话结束后删除。如果临时表与永久表表名相同，则对该表名的任何引用都将解析为临时表，而不是永久表。临时表还具有以下两个限制： 不支持分区列； 不支持创建索引。 12345678910CREATE TEMPORARY TABLE emp_temp( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot;; 2.8 CTAS创建表支持从查询语句的结果创建表： 1CREATE TABLE emp_copy AS SELECT * FROM emp WHERE deptno=&#x27;20&#x27;; 2.9 复制表结构语法： 123CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name --创建表表名 LIKE existing_table_or_view_name --被复制表的表名 [LOCATION hdfs_path]; --存储位置 示例： 1CREATE TEMPORARY EXTERNAL TABLE IF NOT EXISTS emp_co LIKE emp 2.10 加载数据到表加载数据到表中属于 DML 操作，这里为了方便大家测试，先简单介绍一下加载本地数据到表中： 12-- 加载数据到 emp 表中load data local inpath &quot;/usr/file/emp.txt&quot; into table emp; 其中 emp.txt 的内容如下，你可以直接复制使用，也可以到本仓库的resources 目录下载： 12345678910111213147369 SMITH CLERK 7902 1980-12-17 00:00:00 800.00 207499 ALLEN SALESMAN 7698 1981-02-20 00:00:00 1600.00 300.00 307521 WARD SALESMAN 7698 1981-02-22 00:00:00 1250.00 500.00 307566 JONES MANAGER 7839 1981-04-02 00:00:00 2975.00 207654 MARTIN SALESMAN 7698 1981-09-28 00:00:00 1250.00 1400.00 307698 BLAKE MANAGER 7839 1981-05-01 00:00:00 2850.00 307782 CLARK MANAGER 7839 1981-06-09 00:00:00 2450.00 107788 SCOTT ANALYST 7566 1987-04-19 00:00:00 1500.00 207839 KING PRESIDENT 1981-11-17 00:00:00 5000.00 107844 TURNER SALESMAN 7698 1981-09-08 00:00:00 1500.00 0.00 307876 ADAMS CLERK 7788 1987-05-23 00:00:00 1100.00 207900 JAMES CLERK 7698 1981-12-03 00:00:00 950.00 307902 FORD ANALYST 7566 1981-12-03 00:00:00 3000.00 207934 MILLER CLERK 7782 1982-01-23 00:00:00 1300.00 10 加载后可查询表中数据： 三、修改表3.1 重命名表语法： 1ALTER TABLE table_name RENAME TO new_table_name; 示例： 1ALTER TABLE emp_temp RENAME TO new_emp; --把 emp_temp 表重命名为 new_emp 3.2 修改列语法： 12ALTER TABLE table_name [PARTITION partition_spec] CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT]; 示例： 12345678-- 修改字段名和类型ALTER TABLE emp_temp CHANGE empno empno_new INT; -- 修改字段 sal 的名称 并将其放置到 empno 字段后ALTER TABLE emp_temp CHANGE sal sal_new decimal(7,2) AFTER ename;-- 为字段增加注释ALTER TABLE emp_temp CHANGE mgr mgr_new INT COMMENT &#x27;this is column mgr&#x27;; 3.3 新增列示例： 1ALTER TABLE emp_temp ADD COLUMNS (address STRING COMMENT &#x27;home address&#x27;); 四、清空表/删除表4.1 清空表语法： 12-- 清空整个表或表指定分区中的数据TRUNCATE TABLE table_name [PARTITION (partition_column = partition_col_value, ...)]; 目前只有内部表才能执行 TRUNCATE 操作，外部表执行时会抛出异常 Cannot truncate non-managed table XXXX。 示例： 1TRUNCATE TABLE emp_mgt_ptn PARTITION (deptno=20); 4.2 删除表语法： 1DROP TABLE [IF EXISTS] table_name [PURGE]; 内部表：不仅会删除表的元数据，同时会删除 HDFS 上的数据； 外部表：只会删除表的元数据，不会删除 HDFS 上的数据； 删除视图引用的表时，不会给出警告（但视图已经无效了，必须由用户删除或重新创建）。 五、其他命令5.1 Describe查看数据库： 1DESCRIBE|Desc DATABASE [EXTENDED] db_name; --EXTENDED 是否显示额外属性 查看表： 1DESCRIBE|Desc [EXTENDED|FORMATTED] table_name --FORMATTED 以友好的展现方式查看表详情 5.2 Show1. 查看数据库列表 12345-- 语法SHOW (DATABASES|SCHEMAS) [LIKE &#x27;identifier_with_wildcards&#x27;];-- 示例：SHOW DATABASES like &#x27;hive*&#x27;; LIKE 子句允许使用正则表达式进行过滤，但是 SHOW 语句当中的 LIKE 子句只支持 *（通配符）和 |（条件或）两个符号。例如 employees，emp *，emp * | * ees，所有这些都将匹配名为 employees 的数据库。 2. 查看表的列表 12345-- 语法SHOW TABLES [IN database_name] [&#x27;identifier_with_wildcards&#x27;];-- 示例SHOW TABLES IN default; 3. 查看视图列表 1SHOW VIEWS [IN/FROM database_name] [LIKE &#x27;pattern_with_wildcards&#x27;]; --仅支持 Hive 2.2.0 + 4. 查看表的分区列表 1SHOW PARTITIONS table_name; 5. 查看表/视图的创建语句 1SHOW CREATE TABLE ([db_name.]table_name|view_name);","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"hive CLI和Beeline命令行的基本使用","slug":"CLI和Beeline命令行的基本使用","date":"2021-10-15T07:14:24.000Z","updated":"2021-10-25T06:39:38.142Z","comments":true,"path":"2021/10/15/CLI和Beeline命令行的基本使用/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/CLI%E5%92%8CBeeline%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"","text":"一、Hive CLI1.1 Help使用 hive -H 或者 hive --help 命令可以查看所有命令的帮助，显示如下： 12345678910111213usage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --定义用户自定义变量 --database &lt;databasename&gt; Specify the database to use -- 指定使用的数据库 -e &lt;quoted-query-string&gt; SQL from command line -- 执行指定的 SQL -f &lt;filename&gt; SQL from files --执行 SQL 脚本 -H,--help Print help information -- 打印帮助信息 --hiveconf &lt;property=value&gt; Use value for given property --自定义配置 --hivevar &lt;key=value&gt; Variable subsitution to apply to hive --自定义变量 commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file --在进入交互模式之前运行初始化脚本 -S,--silent Silent mode in interactive shell --静默模式 -v,--verbose Verbose mode (echo executed SQL to the console) --详细模式 1.2 交互式命令行直接使用 Hive 命令，不加任何参数，即可进入交互式命令行。 1.3 执行SQL命令在不进入交互式命令行的情况下，可以使用 hive -e 执行 SQL 命令。 1hive -e &#x27;select * from emp&#x27;; 1.4 执行SQL脚本用于执行的 sql 脚本可以在本地文件系统，也可以在 HDFS 上。 12345# 本地文件系统hive -f /usr/file/simple.sql;# HDFS文件系统hive -f hdfs://hadoop001:8020/tmp/simple.sql; 其中 simple.sql 内容如下： 1select * from emp; 1.5 配置Hive变量可以使用 --hiveconf 设置 Hive 运行时的变量。 123hive -e &#x27;select * from emp&#x27; \\--hiveconf hive.exec.scratchdir=/tmp/hive_scratch \\--hiveconf mapred.reduce.tasks=4; hive.exec.scratchdir：指定 HDFS 上目录位置，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。 1.6 配置文件启动使用 -i 可以在进入交互模式之前运行初始化脚本，相当于指定配置文件启动。 1hive -i /usr/file/hive-init.conf; 其中 hive-init.conf 的内容如下： 1set hive.exec.mode.local.auto = true; hive.exec.mode.local.auto 默认值为 false，这里设置为 true ，代表开启本地模式。 1.7 用户自定义变量--define 和 --hivevar 在功能上是等价的，都是用来实现自定义变量，这里给出一个示例: 定义变量： 1hive --define n=ename --hiveconf --hivevar j=job; 在查询中引用自定义变量： 1234567# 以下两条语句等价hive &gt; select $&#123;n&#125; from emp;hive &gt; select $&#123;hivevar:n&#125; from emp;# 以下两条语句等价hive &gt; select $&#123;j&#125; from emp;hive &gt; select $&#123;hivevar:j&#125; from emp; 结果如下： 二、Beeline2.1 HiveServer2Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，所以产生了 HiveServer2。 HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务器。 HiveServer2 拥有自己的 CLI(Beeline)，Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于 HiveServer2 是 Hive 开发维护的重点 (Hive0.15 后就不再支持 hiveserver)，所以 Hive CLI 已经不推荐使用了，官方更加推荐使用 Beeline。 2.1 BeelineBeeline 拥有更多可使用参数，可以使用 beeline --help 查看。 2.2 常用参数在 Hive CLI 中支持的参数，Beeline 都支持，常用的参数如下。更多参数说明可以参见官方文档 Beeline Command Options 参数 说明 **-u ** 数据库地址 **-n ** 用户名 **-p ** 密码 **-d ** 驱动 (可选) **-e ** 执行 SQL 命令 **-f ** 执行 SQL 脚本 **-i (or)–init ** 在进入交互模式之前运行初始化脚本 **–property-file ** 指定配置文件 –hiveconf property**=**value 指定配置属性 –hivevar name**=**value 用户自定义属性，在会话级别有效 示例： 使用用户名和密码连接 Hive 1$ beeline -u jdbc:hive2://localhost:10000 -n username -p password 三、Hive配置可以通过三种方式对 Hive 的相关属性进行配置，分别介绍如下： 3.1 配置文件方式一为使用配置文件，使用配置文件指定的配置是永久有效的。Hive 有以下三个可选的配置文件： hive-site.xml ：Hive 的主要配置文件； hivemetastore-site.xml： 关于元数据的配置； hiveserver2-site.xml：关于 HiveServer2 的配置。 示例如下,在 hive-site.xml 配置 hive.exec.scratchdir： 12345&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/mydir&lt;/value&gt; &lt;description&gt;Scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; 3.2 hiveconf方式二为在启动命令行 (Hive CLI / Beeline) 的时候使用 --hiveconf 指定配置，这种方式指定的配置作用于整个 Session。 1hive --hiveconf hive.exec.scratchdir=/tmp/mydir 3.3 set方式三为在交互式环境下 (Hive CLI / Beeline)，使用 set 命令指定。这种设置的作用范围也是 Session 级别的，配置对于执行该命令后的所有命令生效。set 兼具设置参数和查看参数的功能。如下： 123456780: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir=/tmp/mydir;No rows affected (0.025 seconds)0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir;+----------------------------------+--+| set |+----------------------------------+--+| hive.exec.scratchdir=/tmp/mydir |+----------------------------------+--+ 3.4 配置优先级配置的优先顺序如下 (由低到高)：hive-site.xml - &gt;hivemetastore-site.xml- &gt; hiveserver2-site.xml - &gt; -- hiveconf- &gt; set 3.5 配置参数Hive 可选的配置参数非常多，在用到时查阅官方文档即可AdminManual Configuration","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"Hive安装","slug":"hive安装","date":"2021-10-15T07:13:13.000Z","updated":"2021-10-25T06:40:32.496Z","comments":true,"path":"2021/10/15/hive安装/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/hive%E5%AE%89%E8%A3%85/","excerpt":"","text":"一、安装Hive1.1 下载并解压下载所需版本的 Hive，这里我下载版本为 cdh5.15.2。下载地址：http://archive.cloudera.com/cdh5/cdh/5/ 12# 下载后进行解压 tar -zxvf hive-1.1.0-cdh5.15.2.tar.gz 1.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export HIVE_HOME=/usr/app/hive-1.1.0-cdh5.15.2export PATH=$HIVE_HOME/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 1.3 修改配置1. hive-env.sh 进入安装目录下的 conf/ 目录，拷贝 Hive 的环境配置模板 flume-env.sh.template 1cp hive-env.sh.template hive-env.sh 修改 hive-env.sh，指定 Hadoop 的安装路径： 1HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2 2. hive-site.xml 新建 hive-site.xml 文件，内容如下，主要是配置存放元数据的 MySQL 的地址、驱动、用户名和密码等信息： 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop001:3306/hadoop_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1.4 拷贝数据库驱动将 MySQL 驱动包拷贝到 Hive 安装目录的 lib 目录下, MySQL 驱动的下载地址为：https://dev.mysql.com/downloads/connector/j/ , 在本仓库的resources 目录下我也上传了一份，有需要的可以自行下载。 1.5 初始化元数据库 当使用的 hive 是 1.x 版本时，可以不进行初始化操作，Hive 会在第一次启动的时候会自动进行初始化，但不会生成所有的元数据信息表，只会初始化必要的一部分，在之后的使用中用到其余表时会自动创建； 当使用的 hive 是 2.x 版本时，必须手动初始化元数据库。初始化命令： 12# schematool 命令在安装目录的 bin 目录下，由于上面已经配置过环境变量，在任意位置执行即可schematool -dbType mysql -initSchema 这里我使用的是 CDH 的 hive-1.1.0-cdh5.15.2.tar.gz，对应 Hive 1.1.0 版本，可以跳过这一步。 1.6 启动由于已经将 Hive 的 bin 目录配置到环境变量，直接使用以下命令启动，成功进入交互式命令行后执行 show databases 命令，无异常则代表搭建成功。 1# hive 在 Mysql 中也能看到 Hive 创建的库和存放元数据信息的表 二、HiveServer2/beelineHive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，因此产生了 HiveServer2。HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务。 HiveServer2 拥有自己的 CLI 工具——Beeline。Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于目前 HiveServer2 是 Hive 开发维护的重点，所以官方更加推荐使用 Beeline 而不是 Hive CLI。以下主要讲解 Beeline 的配置方式。 2.1 修改Hadoop配置修改 hadoop 集群的 core-site.xml 配置文件，增加如下配置，指定 hadoop 的 root 用户可以代理本机上所有的用户。 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 之所以要配置这一步，是因为 hadoop 2.0 以后引入了安全伪装机制，使得 hadoop 不允许上层系统（如 hive）直接将实际用户传递到 hadoop 层，而应该将实际用户传递给一个超级代理，由该代理在 hadoop 上执行操作，以避免任意客户端随意操作 hadoop。如果不配置这一步，在之后的连接中可能会抛出 AuthorizationException 异常。 关于 Hadoop 的用户代理机制，可以参考：hadoop 的用户代理机制 或 Superusers Acting On Behalf Of Other Users 2.2 启动hiveserver2由于上面已经配置过环境变量，这里直接启动即可： 1# nohup hiveserver2 &amp; 2.3 使用beeline可以使用以下命令进入 beeline 交互式命令行，出现 Connected 则代表连接成功。 1# beeline -u jdbc:hive2://hadoop001:10000 -n root","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"Hive的简介及核心概念","slug":"hive的简介及核心概念","date":"2021-10-15T07:10:45.000Z","updated":"2021-10-25T06:41:10.475Z","comments":true,"path":"2021/10/15/hive的简介及核心概念/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/hive%E7%9A%84%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","excerpt":"","text":"一、简介Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。 特点： 简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析； 灵活性高，可以自定义用户函数 (UDF) 和存储格式； 为超大的数据集设计的计算和存储能力，集群扩展容易; 统一的元数据管理，可与 presto／impala／sparksql 等共享数据； 执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。 二、Hive的体系架构 2.1 command-line shell &amp; thrift/jdbc可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据： command-line shell：通过 hive 命令行的的方式来操作数据； thrift／jdbc：通过 thrift 协议按照标准的 JDBC 的方式操作数据。 2.2 Metastore在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。 Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。 2.3 HQL的执行流程Hive 在执行一条 HQL 的时候，会经过以下步骤： 语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree； 语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock； 生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree； 优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量； 生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务； 优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。 关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章：Hive SQL 的编译过程 三、数据类型3.1 基本数据类型Hive 表中的列支持以下基本数据类型： 大类 类型 Integers（整型） TINYINT—1 字节的有符号整数 SMALLINT—2 字节的有符号整数 INT—4 字节的有符号整数 BIGINT—8 字节的有符号整数 Boolean（布尔型） BOOLEAN—TRUE/FALSE Floating point numbers（浮点型） FLOAT— 单精度浮点型 DOUBLE—双精度浮点型 Fixed point numbers（定点数） DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2) String types（字符串） STRING—指定字符集的字符序列 VARCHAR—具有最大长度限制的字符序列 CHAR—固定长度的字符序列 Date and time types（日期时间类型） TIMESTAMP — 时间戳 TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度 DATE—日期类型 Binary types（二进制类型） BINARY—字节序列 TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下： TIMESTAMP WITH LOCAL TIME ZONE：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。 TIMESTAMP ：提交什么时间就保存什么时间，查询时也不做任何转换。 3.2 隐式转换Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。 3.3 复杂类型 类型 描述 示例 STRUCT 类似于对象，是字段的集合，字段的类型可以不同，可以使用 名称.字段名 方式进行访问 STRUCT (‘xiaoming’, 12 , ‘2018-12-12’) MAP 键值对的集合，可以使用 名称[key] 的方式访问对应的值 map(‘a’, 1, ‘b’, 2) ARRAY 数组是一组具有相同类型和名称的变量的集合，可以使用 名称[index] 访问对应的值 ARRAY(‘a’, ‘b’, ‘c’, ‘d’) 3.4 示例如下给出一个基本数据类型和复杂数据类型的使用示例： 1234567CREATE TABLE students( name STRING, -- 姓名 age INT, -- 年龄 subject ARRAY&lt;STRING&gt;, --学科 score MAP&lt;STRING,FLOAT&gt;, --各个学科考试成绩 address STRUCT&lt;houseNumber:int, street:STRING, city:STRING, province：STRING&gt; --家庭居住地址) ROW FORMAT DELIMITED FIELDS TERMINATED BY &quot;\\t&quot;; 四、内容格式当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。 所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示。 分隔符 描述 \\n 对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录 ^A (Ctrl+A) 分割字段 (列)，在 CREATE TABLE 语句中也可以使用八进制编码 \\001 来表示 ^B 用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割， 在 CREATE TABLE 语句中也可以使用八进制编码 \\002 表示 ^C 用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 \\003 表示 使用示例如下： 123456CREATE TABLE page_view(viewTime INT, userid BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\001&#x27; COLLECTION ITEMS TERMINATED BY &#x27;\\002&#x27; MAP KEYS TERMINATED BY &#x27;\\003&#x27; STORED AS SEQUENCEFILE; 五、存储格式5.1 支持的存储格式Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式： 格式 说明 TextFile 存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。 SequenceFile SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。 RCFile RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。 ORC Files ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。 Avro Files Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。 Parquet Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。 以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。 5.2 指定存储格式通常在创建表的时候使用 STORED AS 参数指定： 123456CREATE TABLE page_view(viewTime INT, userid BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\\001&#x27; COLLECTION ITEMS TERMINATED BY &#x27;\\002&#x27; MAP KEYS TERMINATED BY &#x27;\\003&#x27; STORED AS SEQUENCEFILE; 各个存储文件类型指定方式如下： STORED AS TEXTFILE STORED AS SEQUENCEFILE STORED AS ORC STORED AS PARQUET STORED AS AVRO STORED AS RCFILE 六、内部表和外部表内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下： 内部表 外部表 数据存储位置 内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 /user/hive/warehouse/数据库名.db/表名/ 目录下 外部表数据的存储位置创建表时由 Location 参数指定； 导入数据 在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理 外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置 删除表 删除元数据（metadata）和文件 只删除元数据（metadata）","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"基于ZooKeeper搭建Hadoop","slug":"基于ZooKeeper搭建Hadoop","date":"2021-10-15T07:10:00.000Z","updated":"2021-10-25T06:42:54.967Z","comments":true,"path":"2021/10/15/基于ZooKeeper搭建Hadoop/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/%E5%9F%BA%E4%BA%8EZooKeeper%E6%90%AD%E5%BB%BAHadoop/","excerpt":"","text":"一、高可用简介Hadoop 高可用 (High Availability) 分为 HDFS 高可用和 YARN 高可用，两者的实现基本类似，但 HDFS NameNode 对数据存储及其一致性的要求比 YARN ResourceManger 高得多，所以它的实现也更加复杂，故下面先进行讲解： 1.1 高可用整体架构HDFS 高可用架构如下： 图片引用自：https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/ HDFS 高可用架构主要由以下组件所构成： Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。 主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。 Zookeeper 集群：为主备切换控制器提供主备选举支持。 共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 1.2 基于 QJM 的共享存储系统的数据同步机制分析目前 Hadoop 支持使用 Quorum Journal Manager (QJM) 或 Network File System (NFS) 作为共享的存储系统，这里以 QJM 集群为例进行说明：Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog，当 Active NameNode 宕机后， Standby NameNode 在确认元数据完全同步之后就可以对外提供服务。 需要说明的是向 JournalNode 集群写入 EditLog 是遵循 “过半写入则成功” 的策略，所以你至少要有 3 个 JournalNode 节点，当然你也可以继续增加节点数量，但是应该保证节点总数是奇数。同时如果有 2N+1 台 JournalNode，那么根据过半写的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。 1.3 NameNode 主备切换NameNode 实现主备切换的流程下图所示： \\1. HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。 2. HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。 3. 如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。 4. ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。 5. ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。 6. ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。 1.4 YARN高可用YARN ResourceManager 的高可用与 HDFS NameNode 的高可用类似，但是 ResourceManager 不像 NameNode ，没有那么多的元数据信息需要维护，所以它的状态信息可以直接写到 Zookeeper 上，并依赖 Zookeeper 来进行主备选举。 二、集群规划按照高可用的设计目标：需要保证至少有两个 NameNode (一主一备) 和 两个 ResourceManager (一主一备) ，同时为满足“过半写入则成功”的原则，需要至少要有 3 个 JournalNode 节点。这里使用三台主机进行搭建，集群规划如下： 三、前置条件 所有服务器都安装有 JDK，安装步骤可以参见：Linux 下 JDK 的安装； 搭建好 ZooKeeper 集群，搭建步骤可以参见：Zookeeper 单机环境和集群环境搭建 所有服务器之间都配置好 SSH 免密登录。 四、集群配置4.1 下载并解压下载 Hadoop。这里我下载的是 CDH 版本 Hadoop，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 1# tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz 4.2 配置环境变量编辑 profile 文件： 1# vim /etc/profile 增加如下配置： 12export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2export PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH 执行 source 命令，使得配置立即生效： 1# source /etc/profile 4.3 修改配置进入 $&#123;HADOOP_HOME&#125;/etc/hadoop 目录下，修改配置文件。各个配置文件内容如下： 1. hadoop-env.sh12# 指定JDK的安装位置export JAVA_HOME=/usr/java/jdk1.8.0_201/ 2. core-site.xml12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定 namenode 的 hdfs 协议文件系统的通信地址 --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定 hadoop 集群存储临时文件的目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- ZooKeeper 集群的地址 --&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop002:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- ZKFC 连接到 ZooKeeper 超时时长 --&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. hdfs-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定 HDFS 副本的数量 --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- namenode 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔 --&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/namenode/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- datanode 节点数据（即数据块）的存放位置 --&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/datanode/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 集群服务的逻辑名称 --&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- NameNode ID 列表--&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- nn1 的 RPC 通信地址 --&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- nn2 的 RPC 通信地址 --&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop002:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- nn1 的 http 通信地址 --&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop001:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- nn2 的 http 通信地址 --&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop002:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- NameNode 元数据在 JournalNode 上的共享存储目录 --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- Journal Edit Files 的存储目录 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/journalnode/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 配置隔离机制，确保在任何给定时间只有一个 NameNode 处于活动状态 --&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 使用 sshfence 机制时需要 ssh 免密登录 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- SSH 超时时间 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 访问代理类，用于确定当前处于 Active 状态的 NameNode --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 开启故障自动转移 --&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4. yarn-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;configuration&gt; &lt;property&gt; &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 是否启用日志聚合 (可选) --&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 聚合日志的保存时间 (可选) --&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;86400&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 启用 RM HA --&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM 集群标识 --&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;my-yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM 的逻辑 ID 列表 --&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM1 的服务地址 --&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop002&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM2 的服务地址 --&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop003&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM1 Web 应用程序的地址 --&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop002:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM2 Web 应用程序的地址 --&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop003:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- ZooKeeper 集群的地址 --&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 启用自动恢复 --&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 用于进行持久化存储的类 --&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. mapred-site.xml1234567&lt;configuration&gt; &lt;property&gt; &lt;!--指定 mapreduce 作业运行在 yarn 上--&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. slaves配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 DataNode 服务和 NodeManager 服务都会被启动。 123hadoop001hadoop002hadoop003 4.4 分发程序将 Hadoop 安装包分发到其他两台服务器，分发后建议在这两台服务器上也配置一下 Hadoop 的环境变量。 1234# 将安装包分发到hadoop002scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/ hadoop002:/usr/app/# 将安装包分发到hadoop003scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/ hadoop003:/usr/app/ 五、启动集群5.1 启动ZooKeeper分别到三台服务器上启动 ZooKeeper 服务： 1zkServer.sh start 5.2 启动Journalnode分别到三台服务器的的 $&#123;HADOOP_HOME&#125;/sbin 目录下，启动 journalnode 进程： 1hadoop-daemon.sh start journalnode 5.3 初始化NameNode在 hadop001 上执行 NameNode 初始化命令： 1hdfs namenode -format 执行初始化命令后，需要将 NameNode 元数据目录的内容，复制到其他未格式化的 NameNode 上。元数据存储目录就是我们在 hdfs-site.xml 中使用 dfs.namenode.name.dir 属性指定的目录。这里我们需要将其复制到 hadoop002 上： 1scp -r /home/hadoop/namenode/data hadoop002:/home/hadoop/namenode/ 5.4 初始化HA状态在任意一台 NameNode 上使用以下命令来初始化 ZooKeeper 中的 HA 状态： 1hdfs zkfc -formatZK 5.5 启动HDFS进入到 hadoop001 的 $&#123;HADOOP_HOME&#125;/sbin 目录下，启动 HDFS。此时 hadoop001 和 hadoop002 上的 NameNode 服务，和三台服务器上的 DataNode 服务都会被启动： 1start-dfs.sh 5.6 启动YARN进入到 hadoop002 的 $&#123;HADOOP_HOME&#125;/sbin 目录下，启动 YARN。此时 hadoop002 上的 ResourceManager 服务，和三台服务器上的 NodeManager 服务都会被启动： 1start-yarn.sh 需要注意的是，这个时候 hadoop003 上的 ResourceManager 服务通常是没有启动的，需要手动启动： 1yarn-daemon.sh start resourcemanager 六、查看集群6.1 查看进程成功启动后，每台服务器上的进程应该如下： 12345678910111213141516171819202122232425[root@hadoop001 sbin]# jps4512 DFSZKFailoverController3714 JournalNode4114 NameNode3668 QuorumPeerMain5012 DataNode4639 NodeManager[root@hadoop002 sbin]# jps4499 ResourceManager4595 NodeManager3465 QuorumPeerMain3705 NameNode3915 DFSZKFailoverController5211 DataNode3533 JournalNode[root@hadoop003 sbin]# jps3491 JournalNode3942 NodeManager4102 ResourceManager4201 DataNode3435 QuorumPeerMain 6.2 查看Web UIHDFS 和 YARN 的端口号分别为 50070 和 8080，界面应该如下： 此时 hadoop001 上的 NameNode 处于可用状态： 而 hadoop002 上的 NameNode 则处于备用状态： hadoop002 上的 ResourceManager 处于可用状态： hadoop003 上的 ResourceManager 则处于备用状态： 同时界面上也有 Journal Manager 的相关信息： ## 七、集群的二次启动 上面的集群初次启动涉及到一些必要初始化操作，所以过程略显繁琐。但是集群一旦搭建好后，想要再次启用它是比较方便的，步骤如下（首选需要确保 ZooKeeper 集群已经启动）： 在 hadoop001 启动 HDFS，此时会启动所有与 HDFS 高可用相关的服务，包括 NameNode，DataNode 和 JournalNode： 1start-dfs.sh 在 hadoop002 启动 YARN： 1start-yarn.sh 这个时候 hadoop003 上的 ResourceManager 服务通常还是没有启动的，需要手动启动： 1yarn-daemon.sh start resourcemanager","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"}]},{"title":"HDFS JAVA API","slug":"HDFS API","date":"2021-10-15T07:06:12.000Z","updated":"2021-10-25T06:40:08.253Z","comments":true,"path":"2021/10/15/HDFS API/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/HDFS%20API/","excerpt":"","text":"一、 简介想要使用 HDFS API，需要导入依赖 hadoop-client。如果是 CDH 版本的 Hadoop，还需要额外指明其仓库地址： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.ihadu&lt;/groupId&gt; &lt;artifactId&gt;hdfs-java-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.15.2&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;!---配置 CDH 仓库地址--&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!--Hadoop-client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 二、API的使用2.1 FileSystemFileSystem 是所有 HDFS 操作的主入口。由于之后的每个单元测试都需要用到它，这里使用 @Before 注解进行标注。 12345678910111213141516171819202122232425private static final String HDFS_PATH = &quot;hdfs://192.168.0.106:8020&quot;;private static final String HDFS_USER = &quot;root&quot;;private static FileSystem fileSystem;@Beforepublic void prepare() &#123; try &#123; Configuration configuration = new Configuration(); // 这里我启动的是单节点的 Hadoop,所以副本系数设置为 1,默认值为 3 configuration.set(&quot;dfs.replication&quot;, &quot;1&quot;); fileSystem = FileSystem.get(new URI(HDFS_PATH), configuration, HDFS_USER); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (URISyntaxException e) &#123; e.printStackTrace(); &#125;&#125;@Afterpublic void destroy() &#123; fileSystem = null;&#125; 2.2 创建目录支持递归创建目录： 1234@Testpublic void mkDir() throws Exception &#123; fileSystem.mkdirs(new Path(&quot;/hdfs-api/test0/&quot;));&#125; 2.3 创建指定权限的目录FsPermission(FsAction u, FsAction g, FsAction o) 的三个参数分别对应：创建者权限，同组其他用户权限，其他用户权限，权限值定义在 FsAction 枚举类中。 12345@Testpublic void mkDirWithPermission() throws Exception &#123; fileSystem.mkdirs(new Path(&quot;/hdfs-api/test1/&quot;), new FsPermission(FsAction.READ_WRITE, FsAction.READ, FsAction.READ));&#125; 2.4 创建文件，并写入内容123456789101112@Testpublic void create() throws Exception &#123; // 如果文件存在，默认会覆盖, 可以通过第二个参数进行控制。第三个参数可以控制使用缓冲区的大小 FSDataOutputStream out = fileSystem.create(new Path(&quot;/hdfs-api/test/a.txt&quot;), true, 4096); out.write(&quot;hello hadoop!&quot;.getBytes()); out.write(&quot;hello spark!&quot;.getBytes()); out.write(&quot;hello flink!&quot;.getBytes()); // 强制将缓冲区中内容刷出 out.flush(); out.close();&#125; 2.5 判断文件是否存在12345@Testpublic void exist() throws Exception &#123; boolean exists = fileSystem.exists(new Path(&quot;/hdfs-api/test/a.txt&quot;)); System.out.println(exists);&#125; 2.6 查看文件内容查看小文本文件的内容，直接转换成字符串后输出： 123456@Testpublic void readToString() throws Exception &#123; FSDataInputStream inputStream = fileSystem.open(new Path(&quot;/hdfs-api/test/a.txt&quot;)); String context = inputStreamToString(inputStream, &quot;utf-8&quot;); System.out.println(context);&#125; inputStreamToString 是一个自定义方法，代码如下： 1234567891011121314151617181920212223/** * 把输入流转换为指定编码的字符 * * @param inputStream 输入流 * @param encode 指定编码类型 */private static String inputStreamToString(InputStream inputStream, String encode) &#123; try &#123; if (encode == null || (&quot;&quot;.equals(encode))) &#123; encode = &quot;utf-8&quot;; &#125; BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream, encode)); StringBuilder builder = new StringBuilder(); String str = &quot;&quot;; while ((str = reader.readLine()) != null) &#123; builder.append(str).append(&quot;\\n&quot;); &#125; return builder.toString(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null;&#125; 2.7 文件重命名1234567@Testpublic void rename() throws Exception &#123; Path oldPath = new Path(&quot;/hdfs-api/test/a.txt&quot;); Path newPath = new Path(&quot;/hdfs-api/test/b.txt&quot;); boolean result = fileSystem.rename(oldPath, newPath); System.out.println(result);&#125; 2.8 删除目录或文件123456789public void delete() throws Exception &#123; /* * 第二个参数代表是否递归删除 * + 如果 path 是一个目录且递归删除为 true, 则删除该目录及其中所有文件; * + 如果 path 是一个目录但递归删除为 false,则会则抛出异常。 */ boolean result = fileSystem.delete(new Path(&quot;/hdfs-api/test/b.txt&quot;), true); System.out.println(result);&#125; 2.9 上传文件到HDFS1234567@Testpublic void copyFromLocalFile() throws Exception &#123; // 如果指定的是目录，则会把目录及其中的文件都复制到指定目录下 Path src = new Path(&quot;D:\\\\BigData-Notes\\\\notes\\\\installation&quot;); Path dst = new Path(&quot;/hdfs-api/test/&quot;); fileSystem.copyFromLocalFile(src, dst);&#125; 2.10 上传大文件并显示上传进度123456789101112131415161718192021@Test public void copyFromLocalBigFile() throws Exception &#123; File file = new File(&quot;D:\\\\kafka.tgz&quot;); final float fileSize = file.length(); InputStream in = new BufferedInputStream(new FileInputStream(file)); FSDataOutputStream out = fileSystem.create(new Path(&quot;/hdfs-api/test/kafka5.tgz&quot;), new Progressable() &#123; long fileCount = 0; public void progress() &#123; fileCount++; // progress 方法每上传大约 64KB 的数据后就会被调用一次 System.out.println(&quot;上传进度：&quot; + (fileCount * 64 * 1024 / fileSize) * 100 + &quot; %&quot;); &#125; &#125;); IOUtils.copyBytes(in, out, 4096); &#125; 2.11 从HDFS上下载文件12345678910111213@Testpublic void copyToLocalFile() throws Exception &#123; Path src = new Path(&quot;/hdfs-api/test/kafka.tgz&quot;); Path dst = new Path(&quot;D:\\\\app\\\\&quot;); /* * 第一个参数控制下载完成后是否删除源文件,默认是 true,即删除; * 最后一个参数表示是否将 RawLocalFileSystem 用作本地文件系统; * RawLocalFileSystem 默认为 false,通常情况下可以不设置, * 但如果你在执行时候抛出 NullPointerException 异常,则代表你的文件系统与程序可能存在不兼容的情况 (window 下常见), * 此时可以将 RawLocalFileSystem 设置为 true */ fileSystem.copyToLocalFile(false, src, dst, true);&#125; 2.12 查看指定目录下所有文件的信息1234567public void listFiles() throws Exception &#123; FileStatus[] statuses = fileSystem.listStatus(new Path(&quot;/hdfs-api&quot;)); for (FileStatus fileStatus : statuses) &#123; //fileStatus 的 toString 方法被重写过，直接打印可以看到所有信息 System.out.println(fileStatus.toString()); &#125;&#125; FileStatus 中包含了文件的基本信息，比如文件路径，是否是文件夹，修改时间，访问时间，所有者，所属组，文件权限，是否是符号链接等，输出内容示例如下： 12345678910FileStatus&#123;path=hdfs://192.168.0.106:8020/hdfs-api/test; isDirectory=true; modification_time=1556680796191; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false&#125; 2.13 递归查看指定目录下所有文件的信息1234567@Testpublic void listFilesRecursive() throws Exception &#123; RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(new Path(&quot;/hbase&quot;), true); while (files.hasNext()) &#123; System.out.println(files.next()); &#125;&#125; 和上面输出类似，只是多了文本大小，副本系数，块大小信息。 123456789101112LocatedFileStatus&#123;path=hdfs://192.168.0.106:8020/hbase/hbase.version; isDirectory=false; length=7; replication=1; blocksize=134217728; modification_time=1554129052916; access_time=1554902661455; owner=root; group=supergroup;permission=rw-r--r--; isSymlink=false&#125; 2.14 查看文件的块信息123456789@Testpublic void getFileBlockLocations() throws Exception &#123; FileStatus fileStatus = fileSystem.getFileStatus(new Path(&quot;/hdfs-api/test/kafka.tgz&quot;)); BlockLocation[] blocks = fileSystem.getFileBlockLocations(fileStatus, 0, fileStatus.getLen()); for (BlockLocation block : blocks) &#123; System.out.println(block); &#125;&#125; 块输出信息有三个值，分别是文件的起始偏移量 (offset)，文件大小 (length)，块所在的主机名 (hosts)。 10,57028557,hadoop001 这里我上传的文件只有 57M(小于 128M)，且程序中设置了副本系数为 1，所有只有一个块信息。","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"}]},{"title":"HDFS常用shell命令","slug":"HDFS常用shell命令","date":"2021-10-15T07:04:06.000Z","updated":"2021-10-25T06:40:14.667Z","comments":true,"path":"2021/10/15/HDFS常用shell命令/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/HDFS%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4/","excerpt":"","text":"1. 显示当前目录结构 123456# 显示当前目录结构hadoop fs -ls &lt;path&gt;# 递归显示当前目录结构hadoop fs -ls -R &lt;path&gt;# 显示根目录下内容hadoop fs -ls / 2. 创建目录 1234# 创建目录hadoop fs -mkdir &lt;path&gt; # 递归创建目录hadoop fs -mkdir -p &lt;path&gt; 3. 删除操作 1234# 删除文件hadoop fs -rm &lt;path&gt;# 递归删除目录和文件hadoop fs -rm -R &lt;path&gt; 4. 从本地加载文件到 HDFS 123# 二选一执行即可hadoop fs -put [localsrc] [dst] hadoop fs - copyFromLocal [localsrc] [dst] 5. 从 HDFS 导出文件到本地 123# 二选一执行即可hadoop fs -get [dst] [localsrc] hadoop fs -copyToLocal [dst] [localsrc] 6. 查看文件内容 123# 二选一执行即可hadoop fs -text &lt;path&gt; hadoop fs -cat &lt;path&gt; 7. 显示文件的最后一千字节 123hadoop fs -tail &lt;path&gt; # 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节hadoop fs -tail -f &lt;path&gt; 8. 拷贝文件 1hadoop fs -cp [src] [dst] 9. 移动文件 1hadoop fs -mv [src] [dst] 10. 统计当前目录下各文件大小 默认单位字节 -s : 显示所有文件大小总和， -h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864） 1hadoop fs -du &lt;path&gt; 11. 合并下载多个文件 -nl 在每个文件的末尾添加换行符（LF） -skip-empty-file 跳过空文件 123hadoop fs -getmerge# 示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xmlhadoop fs -getmerge -nl /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml 12. 统计文件系统的可用空间信息 1hadoop fs -df -h / 13. 更改文件复制因子 1hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt; 更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子 -w : 请求命令是否等待复制完成 12# 示例hadoop fs -setrep -w 3 /user/hadoop/dir1 14. 权限控制 1234567# 权限控制和Linux上使用方式一致# 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。hadoop fs -chgrp [-R] GROUP URI [URI ...]# 修改文件或目录的访问权限 用户必须是文件的所有者或超级用户。hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...]# 修改文件的拥有者 用户必须是超级用户。hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] 15. 文件检测 1hadoop fs -test - [defsz] URI 可选选项： -d：如果路径是目录，返回 0。 -e：如果路径存在，则返回 0。 -f：如果路径是文件，则返回 0。 -s：如果路径不为空，则返回 0。 -r：如果路径存在且授予读权限，则返回 0。 -w：如果路径存在且授予写入权限，则返回 0。 -z：如果文件长度为零，则返回 0。 12# 示例hadoop fs -test -e filename","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"}]},{"title":"Hadoop集群环境搭建","slug":"Hadoop集群环境搭建","date":"2021-10-15T07:01:58.000Z","updated":"2021-10-25T06:40:01.175Z","comments":true,"path":"2021/10/15/Hadoop集群环境搭建/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/Hadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","excerpt":"","text":"一、集群规划这里搭建一个 3 节点的 Hadoop 集群，其中三台主机均部署 DataNode 和 NodeManager 服务，但只有 hadoop001 上部署 NameNode 和 ResourceManager 服务。 二、前置条件Hadoop 的运行依赖 JDK，需要预先安装。其安装步骤单独整理至： Linux 下 JDK 的安装 三、配置免密登录3.1 生成密匙在每台主机上使用 ssh-keygen 命令生成公钥私钥对： 1ssh-keygen 3.2 免密登录将 hadoop001 的公钥写到本机和远程机器的 ~/ .ssh/authorized_key 文件中： 123ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop003 3.3 验证免密登录12ssh hadoop002ssh hadoop003 四、集群搭建3.1 下载并解压下载 Hadoop。这里我下载的是 CDH 版本 Hadoop，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 1# tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz 3.2 配置环境变量编辑 profile 文件： 1# vim /etc/profile 增加如下配置： 12export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2export PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH 执行 source 命令，使得配置立即生效： 1# source /etc/profile 3.3 修改配置进入 $&#123;HADOOP_HOME&#125;/etc/hadoop 目录下，修改配置文件。各个配置文件内容如下： 1. hadoop-env.sh12# 指定JDK的安装位置export JAVA_HOME=/usr/java/jdk1.8.0_201/ 2. core-site.xml123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--指定 hadoop 集群存储临时文件的目录--&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. hdfs-site.xml12345678910&lt;property&gt; &lt;!--namenode 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔--&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/namenode/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!--datanode 节点数据（即数据块）的存放位置--&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/datanode/data&lt;/value&gt;&lt;/property&gt; 4. yarn-site.xml123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--resourcemanager 的主机名--&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. mapred-site.xml1234567&lt;configuration&gt; &lt;property&gt; &lt;!--指定 mapreduce 作业运行在 yarn 上--&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. slaves配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 DataNode 服务和 NodeManager 服务都会被启动。 123hadoop001hadoop002hadoop003 3.4 分发程序将 Hadoop 安装包分发到其他两台服务器，分发后建议在这两台服务器上也配置一下 Hadoop 的环境变量。 1234# 将安装包分发到hadoop002scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/ hadoop002:/usr/app/# 将安装包分发到hadoop003scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/ hadoop003:/usr/app/ 3.5 初始化在 Hadoop001 上执行 namenode 初始化命令： 1hdfs namenode -format 3.6 启动集群进入到 Hadoop001 的 $&#123;HADOOP_HOME&#125;/sbin 目录下，启动 Hadoop。此时 hadoop002 和 hadoop003 上的相关服务也会被启动： 1234# 启动dfs服务start-dfs.sh# 启动yarn服务start-yarn.sh 3.7 查看集群在每台服务器上使用 jps 命令查看服务进程，或直接进入 Web-UI 界面进行查看，端口为 50070。可以看到此时有三个可用的 Datanode： 点击 Live Nodes 进入，可以看到每个 DataNode 的详细情况： 接着可以查看 Yarn 的情况，端口号为 8088 ： 五、提交服务到集群提交作业到集群的方式和单机环境完全一致，这里以提交 Hadoop 内置的计算 Pi 的示例程序为例，在任何一个节点上执行都可以，命令如下： 1hadoop jar /usr/app/hadoop-2.6.0-cdh5.15.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"}]},{"title":"Hadoop单机版环境搭建","slug":"Hadoop单机版环境搭建","date":"2021-10-15T07:01:09.000Z","updated":"2021-10-25T06:39:53.901Z","comments":true,"path":"2021/10/15/Hadoop单机版环境搭建/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/Hadoop%E5%8D%95%E6%9C%BA%E7%89%88%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","excerpt":"","text":"一、前置条件Hadoop 的运行依赖 JDK，需要预先安装。 二、配置免密登录Hadoop 组件之间需要基于 SSH 进行通讯。 2.1 配置映射配置 ip 地址和主机名映射： 123vim /etc/hosts# 文件末尾增加192.168.43.202 hadoop001 2.2 生成公私钥执行下面命令行生成公匙和私匙： 1ssh-keygen -t rsa 3.3 授权进入 ~/.ssh 目录下，查看生成的公匙和私匙，并将公匙写入到授权文件： 1234567[root@@hadoop001 sbin]# cd ~/.ssh[root@@hadoop001 .ssh]# ll-rw-------. 1 root root 1675 3 月 15 09:48 id_rsa-rw-r--r--. 1 root root 388 3 月 15 09:48 id_rsa.pub# 写入公匙到授权文件[root@hadoop001 .ssh]# cat id_rsa.pub &gt;&gt; authorized_keys[root@hadoop001 .ssh]# chmod 600 authorized_keys 三、Hadoop(HDFS)环境搭建3.1 下载并解压下载 Hadoop 安装包，这里我下载的是 CDH 版本的，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 12# 解压tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz 3.2 配置环境变量1# vi /etc/profile 配置环境变量： 12export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2export PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH 执行 source 命令，使得配置的环境变量立即生效： 1# source /etc/profile 3.3 修改Hadoop配置进入 $&#123;HADOOP_HOME&#125;/etc/hadoop/ 目录下，修改以下配置： 1. hadoop-env.sh12# JDK安装路径export JAVA_HOME=/usr/java/jdk1.8.0_201/ 2. core-site.xml123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--指定 hadoop 存储临时文件的目录--&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. hdfs-site.xml指定副本系数和临时文件存储位置： 1234567&lt;configuration&gt; &lt;property&gt; &lt;!--由于我们这里搭建是单机版本，所以指定 dfs 的副本系数为 1--&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4. slaves配置所有从属节点的主机名或 IP 地址，由于是单机版本，所以指定本机即可： 1hadoop001 3.4 关闭防火墙不关闭防火墙可能导致无法访问 Hadoop 的 Web UI 界面： 1234# 查看防火墙状态sudo firewall-cmd --state# 关闭防火墙:sudo systemctl stop firewalld.service 3.5 初始化第一次启动 Hadoop 时需要进行初始化，进入 $&#123;HADOOP_HOME&#125;/bin/ 目录下，执行以下命令： 1[root@hadoop001 bin]# ./hdfs namenode -format 3.6 启动HDFS进入 $&#123;HADOOP_HOME&#125;/sbin/ 目录下，启动 HDFS： 1[root@hadoop001 sbin]# ./start-dfs.sh 3.7 验证是否启动成功方式一：执行 jps 查看 NameNode 和 DataNode 服务是否已经启动： 1234[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps9137 DataNode9026 NameNode9390 SecondaryNameNode 方式二：查看 Web UI 界面，端口为 50070： 四、Hadoop(YARN)环境搭建4.1 修改配置进入 $&#123;HADOOP_HOME&#125;/etc/hadoop/ 目录下，修改以下配置： 1. mapred-site.xml12345678# 如果没有mapred-site.xml，则拷贝一份样例文件后再修改cp mapred-site.xml.template mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2. yarn-site.xml1234567&lt;configuration&gt; &lt;property&gt; &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.2 启动服务进入 $&#123;HADOOP_HOME&#125;/sbin/ 目录下，启动 YARN： 1./start-yarn.sh 4.3 验证是否启动成功方式一：执行 jps 命令查看 NodeManager 和 ResourceManager 服务是否已经启动： 123456[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps9137 DataNode9026 NameNode12294 NodeManager12185 ResourceManager9390 SecondaryNameNode 方式二：查看 Web UI 界面，端口号为 8088：","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"}]},{"title":"集群资源管理器—YARN","slug":"集群资源管理器—YARN","date":"2021-10-15T07:00:12.000Z","updated":"2021-10-25T06:43:06.460Z","comments":true,"path":"2021/10/15/集群资源管理器—YARN/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/%E9%9B%86%E7%BE%A4%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E5%99%A8%E2%80%94YARN/","excerpt":"","text":"一、hadoop yarn 简介Apache YARN (Yet Another Resource Negotiator) 是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。 二、YARN架构 ResourceManagerResourceManager 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。ResourceManager 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。 2. NodeManagerNodeManager 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下： 启动时向 ResourceManager 注册并定时发送心跳消息，等待 ResourceManager 的指令； 维护 Container 的生命周期，监控 Container 的资源使用情况； 管理任务运行时的相关依赖，根据 ApplicationMaster 的需要，在启动 Container 之前将需要的程序及其依赖拷贝到本地。 3. ApplicationMaster在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 ApplicationMaster。ApplicationMaster 负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下： 根据应用的运行状态来决定动态计算资源需求； 向 ResourceManager 申请资源，监控申请的资源的使用情况； 跟踪任务状态和进度，报告资源的使用情况和应用的进度信息； 负责任务的容错。 4. ContainerContainer 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 Container 表示的。YARN 会为每个任务分配一个 Container，该任务只能使用该 Container 中描述的资源。ApplicationMaster 可在 Container 内运行任何类型的任务。例如，MapReduce ApplicationMaster 请求一个容器来启动 map 或 reduce 任务，而 Giraph ApplicationMaster 请求一个容器来运行 Giraph 任务。 三、YARN工作原理简述 Client 提交作业到 YARN 上； Resource Manager 选择一个 Node Manager，启动一个 Container 并运行 Application Master 实例； Application Master 根据实际需要向 Resource Manager 请求更多的 Container 资源（如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务）； Application Master 通过获取到的 Container 资源执行分布式计算。 四、YARN工作原理详述 1. 作业提交client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 (第 1 步) 。新的作业 ID(应用 ID) 由资源管理器分配 (第 2 步)。作业的 client 核实作业的输出, 计算输入的 split, 将作业的资源 (包括 Jar 包，配置文件, split 信息) 拷贝给 HDFS(第 3 步)。 最后, 通过调用资源管理器的 submitApplication() 来提交作业 (第 4 步)。 2. 作业初始化当资源管理器收到 submitApplciation() 的请求时, 就将该请求发给调度器 (scheduler), 调度器分配 container, 然后资源管理器在该 container 内启动应用管理器进程, 由节点管理器监控 (第 5 步)。 MapReduce 作业的应用管理器是一个主类为 MRAppMaster 的 Java 应用，其通过创造一些 bookkeeping 对象来监控作业的进度, 得到任务的进度和完成报告 (第 6 步)。然后其通过分布式文件系统得到由客户端计算好的输入 split(第 7 步)，然后为每个输入 split 创建一个 map 任务, 根据 mapreduce.job.reduces 创建 reduce 任务对象。 3. 任务分配如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务。 如果不是小作业, 那么应用管理器向资源管理器请求 container 来运行所有的 map 和 reduce 任务 (第 8 步)。这些请求是通过心跳来传输的, 包括每个 map 任务的数据位置，比如存放输入 split 的主机名和机架 (rack)，调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点, 或者分配给和存放输入 split 的节点相同机架的节点。 4. 任务运行当一个任务由资源管理器的调度器分配给一个 container 后，应用管理器通过联系节点管理器来启动 container(第 9 步)。任务由一个主类为 YarnChild 的 Java 应用执行， 在运行任务之前首先本地化任务需要的资源，比如作业配置，JAR 文件, 以及分布式缓存的所有文件 (第 10 步。 最后, 运行 map 或 reduce 任务 (第 11 步)。 YarnChild 运行在一个专用的 JVM 中, 但是 YARN 不支持 JVM 重用。 5. 进度和状态更新YARN 中的任务将其进度和状态 (包括 counter) 返回给应用管理器, 客户端每秒 (通 mapreduce.client.progressmonitor.pollinterval 设置) 向应用管理器请求进度更新, 展示给用户。 6. 作业完成除了向应用管理器请求作业进度外, 客户端每 5 分钟都会通过调用 waitForCompletion() 来检查作业是否完成，时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后, 应用管理器和 container 会清理工作状态， OutputCommiter 的作业清理方法也会被调用。作业的信息会被作业历史服务器存储以备之后用户核查。 五、提交作业到YARN上运行这里以提交 Hadoop Examples 中计算 Pi 的 MApReduce 程序为例，相关 Jar 包在 Hadoop 安装目录的 share/hadoop/mapreduce 目录下： 12# 提交格式: hadoop jar jar包路径 主类名称 主类参数# hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3 参考资料 初步掌握 Yarn 的架构及原理 Apache Hadoop 2.9.2 &gt; Apache Hadoop YARN","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"}]},{"title":"分布式计算框架—MapReduce","slug":"分布式计算框架—MapReduce","date":"2021-10-15T06:59:00.000Z","updated":"2021-10-25T06:42:18.988Z","comments":true,"path":"2021/10/15/分布式计算框架—MapReduce/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E2%80%94MapReduce/","excerpt":"","text":"一、MapReduce概述Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。 MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 map 以并行的方式处理，框架对 map 的输出进行排序，然后输入到 reduce 中。MapReduce 框架专门用于 键值对处理，它将作业的输入视为一组 对，并生成一组 `` 对作为输出。输出和输出的 key 和 value 都必须实现Writable 接口。 1(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output) 二、MapReduce编程模型简述这里以词频统计为例进行说明，MapReduce 处理的流程如下： input : 读取文本文件； splitting : 将文件按照行进行拆分，此时得到的 K1 行数，V1 表示对应行的文本内容； mapping : 并行将每一行按照空格进行拆分，拆分得到的 List(K2,V2)，其中 K2 代表每一个单词，由于是做词频统计，所以 V2 的值为 1，代表出现 1 次； shuffling：由于 Mapping 操作可能是在不同的机器上并行处理的，所以需要通过 shuffling 将相同 key 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 K2 为每一个单词，List(V2) 为可迭代集合，V2 就是 Mapping 中的 V2； Reducing : 这里的案例是统计单词出现的总次数，所以 Reducing 对 List(V2) 进行归约求和操作，最终输出。 MapReduce 编程模型中 splitting 和 shuffing 操作都是由框架实现的，需要我们自己编程实现的只有 mapping 和 reducing，这也就是 MapReduce 这个称呼的来源。 三、combiner &amp; partitioner 3.1 InputFormat &amp; RecordReadersInputFormat 将输出文件拆分为多个 InputSplit，并由 RecordReaders 将 InputSplit 转换为标准的&lt;key，value&gt;键值对，作为 map 的输出。这一步的意义在于只有先进行逻辑拆分并转为标准的键值对格式后，才能为多个 map 提供输入，以便进行并行处理。 3.2 Combinercombiner 是 map 运算后的可选操作，它实际上是一个本地化的 reduce 操作，它主要是在 map 计算出中间文件后做一个简单的合并重复 key 值的操作。这里以词频统计为例： map 在遇到一个 hadoop 的单词时就会记录为 1，但是这篇文章里 hadoop 可能会出现 n 多次，那么 map 输出文件冗余就会很多，因此在 reduce 计算前对相同的 key 做一个合并操作，那么需要传输的数据量就会减少，传输效率就可以得到提升。 但并非所有场景都适合使用 combiner，使用它的原则是 combiner 的输出不会影响到 reduce 计算的最终输入，例如：求总数，最大值，最小值时都可以使用 combiner，但是做平均值计算则不能使用 combiner。 不使用 combiner 的情况： 使用 combiner 的情况： 可以看到使用 combiner 的时候，需要传输到 reducer 中的数据由 12keys，降低到 10keys。降低的幅度取决于你 keys 的重复率，下文词频统计案例会演示用 combiner 降低数百倍的传输量。 3.3 Partitionerpartitioner 可以理解成分类器，将 map 的输出按照 key 值的不同分别分给对应的 reducer，支持自定义实现，下文案例会给出演示。 四、MapReduce词频统计案例4.1 项目简介这里给出一个经典的词频统计的案例：统计如下样本数据中每个单词出现的次数。 12345678910111213Spark HBaseHive Flink Storm Hadoop HBase SparkFlinkHBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase HiveHadoop Spark HBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase Hive 为方便大家开发，我在项目源码中放置了一个工具类 WordCountDataUtils，用于模拟产生词频统计的样本，生成的文件支持输出到本地或者直接写到 HDFS 上。 项目完整源码下载地址：hadoop-word-count 4.2 项目依赖想要进行 MapReduce 编程，需要导入 hadoop-client 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt; 4.3 WordCountMapper将每行数据按照指定分隔符进行拆分。这里需要注意在 MapReduce 中必须使用 Hadoop 定义的类型，因为 Hadoop 预定义的类型都是可序列化，可比较的，所有类型均实现了 WritableComparable 接口。 123456789101112public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] words = value.toString().split(&quot;\\t&quot;); for (String word : words) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; WordCountMapper 对应下图的 Mapping 操作： WordCountMapper 继承自 Mapper 类，这是一个泛型类，定义如下： 12345WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;public class Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123; ......&#125; KEYIN : mapping 输入 key 的类型，即每行的偏移量 (每行第一个字符在整个文本中的位置)，Long 类型，对应 Hadoop 中的 LongWritable 类型； VALUEIN : mapping 输入 value 的类型，即每行数据；String 类型，对应 Hadoop 中 Text 类型； KEYOUT ：mapping 输出的 key 的类型，即每个单词；String 类型，对应 Hadoop 中 Text 类型； VALUEOUT：mapping 输出 value 的类型，即每个单词出现的次数；这里用 int 类型，对应 IntWritable 类型。 4.4 WordCountReducer在 Reduce 中进行单词出现次数的统计： 123456789101112public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable value : values) &#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; 如下图，shuffling 的输出是 reduce 的输入。这里的 key 是每个单词，values 是一个可迭代的数据类型，类似 (1,1,1,...)。 4.4 WordCountApp组装 MapReduce 作业，并提交到服务器运行，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 组装作业 并提交到集群运行 */public class WordCountApp &#123; // 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参 private static final String HDFS_URL = &quot;hdfs://192.168.0.107:8020&quot;; private static final String HADOOP_USER_NAME = &quot;root&quot;; public static void main(String[] args) throws Exception &#123; // 文件输入路径和输出路径由外部传参指定 if (args.length &lt; 2) &#123; System.out.println(&quot;Input and output paths are necessary!&quot;); return; &#125; // 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常 System.setProperty(&quot;HADOOP_USER_NAME&quot;, HADOOP_USER_NAME); Configuration configuration = new Configuration(); // 指明 HDFS 的地址 configuration.set(&quot;fs.defaultFS&quot;, HDFS_URL); // 创建一个 Job Job job = Job.getInstance(configuration); // 设置运行的主类 job.setJarByClass(WordCountApp.class); // 设置 Mapper 和 Reducer job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 设置 Mapper 输出 key 和 value 的类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 设置 Reducer 输出 key 和 value 的类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常 FileSystem fileSystem = FileSystem.get(new URI(HDFS_URL), configuration, HADOOP_USER_NAME); Path outputPath = new Path(args[1]); if (fileSystem.exists(outputPath)) &#123; fileSystem.delete(outputPath, true); &#125; // 设置作业输入文件和输出文件的路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, outputPath); // 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度 boolean result = job.waitForCompletion(true); // 关闭之前创建的 fileSystem fileSystem.close(); // 根据作业结果,终止当前运行的 Java 虚拟机,退出程序 System.exit(result ? 0 : -1); &#125;&#125; 需要注意的是：如果不设置 Mapper 操作的输出类型，则程序默认它和 Reducer 操作输出的类型相同。 4.5 提交到服务器运行在实际开发中，可以在本机配置 hadoop 开发环境，直接在 IDE 中启动进行测试。这里主要介绍一下打包提交到服务器运行。由于本项目没有使用除 Hadoop 外的第三方依赖，直接打包即可： 1# mvn clean package 使用以下命令提交作业： 123hadoop jar /usr/appjar/hadoop-word-count-1.0.jar \\com.heibaiying.WordCountApp \\/wordcount/input.txt /wordcount/output/WordCountApp 作业完成后查看 HDFS 上生成目录： 12345# 查看目录hadoop fs -ls /wordcount/output/WordCountApp# 查看统计结果hadoop fs -cat /wordcount/output/WordCountApp/part-r-00000 五、词频统计案例进阶之Combiner5.1 代码实现想要使用 combiner 功能只要在组装作业时，添加下面一行代码即可： 12// 设置 Combinerjob.setCombinerClass(WordCountReducer.class); 5.2 执行结果加入 combiner 后统计结果是不会有变化的，但是可以从打印的日志看出 combiner 的效果： 没有加入 combiner 的打印日志： 加入 combiner 后的打印日志如下： 这里我们只有一个输入文件并且小于 128M，所以只有一个 Map 进行处理。可以看到经过 combiner 后，records 由 3519 降低为 6(样本中单词种类就只有 6 种)，在这个用例中 combiner 就能极大地降低需要传输的数据量。 六、词频统计案例进阶之Partitioner6.1 默认的Partitioner这里假设有个需求：将不同单词的统计结果输出到不同文件。这种需求实际上比较常见，比如统计产品的销量时，需要将结果按照产品种类进行拆分。要实现这个功能，就需要用到自定义 Partitioner。 这里先介绍下 MapReduce 默认的分类规则：在构建 job 时候，如果不指定，默认的使用的是 HashPartitioner：对 key 值进行哈希散列并对 numReduceTasks 取余。其实现如下： 12345678public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 6.2 自定义Partitioner这里我们继承 Partitioner 自定义分类规则，这里按照单词进行分类： 123456public class CustomPartitioner extends Partitioner&lt;Text, IntWritable&gt; &#123; public int getPartition(Text text, IntWritable intWritable, int numPartitions) &#123; return WordCountDataUtils.WORD_LIST.indexOf(text.toString()); &#125;&#125; 在构建 job 时候指定使用我们自己的分类规则，并设置 reduce 的个数： 1234// 设置自定义分区规则job.setPartitionerClass(CustomPartitioner.class);// 设置 reduce 个数job.setNumReduceTasks(WordCountDataUtils.WORD_LIST.size()); 6.3 执行结果执行结果如下，分别生成 6 个文件，每个文件中为对应单词的统计结果： 参考资料 分布式计算框架 MapReduce Apache Hadoop 2.9.2 &gt; MapReduce Tutorial MapReduce - Combiners","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"}]},{"title":"Hadoop分布式文件系统—HDFS","slug":"Hadoop分布式文件系统—HDFS","date":"2021-10-15T06:58:02.000Z","updated":"2021-10-25T06:39:46.533Z","comments":true,"path":"2021/10/15/Hadoop分布式文件系统—HDFS/","link":"","permalink":"https://www.ihadyou.cn/2021/10/15/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E2%80%94HDFS/","excerpt":"","text":"一、介绍HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。 二、HDFS 设计原理 2.1 HDFS 架构HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成： NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。 DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。 2.2 文件系统命名空间HDFS 的 文件系统命名空间 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。NameNode 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。 2.3 数据复制由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列块，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。 2.4 数据复制的实现原理大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是： 在写入程序位于 datanode 上时，就优先将写入文件的一个副本放置在该 datanode 上，否则放在随机 datanode 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。 如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 （复制系数 - 1）/机架数量 + 2，需要注意的是不允许同一个 dataNode 上具有同一个块的多个副本。 2.5 副本的选择为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。 2.6 架构的稳定性1. 心跳机制和重新复制每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。 2. 数据的完整性由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下： 当客户端创建 HDFS 文件时，它会计算文件的每个块的 校验和，并将 校验和 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 校验和 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。 3.元数据的磁盘故障FsImage 和 EditLog 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。 4.支持快照快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。 三、HDFS 的特点3.1 高容错由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。 3.2 高吞吐量HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。 3.3 大文件支持HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。 3.3 简单一致性模型HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。 3.4 跨平台移植性HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。","categories":[],"tags":[{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"}]},{"title":"Hive自定义UDF函数","slug":"hive自定义UDF函数","date":"2021-10-13T08:55:03.000Z","updated":"2021-10-25T06:41:21.886Z","comments":true,"path":"2021/10/13/hive自定义UDF函数/","link":"","permalink":"https://www.ihadyou.cn/2021/10/13/hive%E8%87%AA%E5%AE%9A%E4%B9%89UDF%E5%87%BD%E6%95%B0/","excerpt":"","text":"1. Hive自定义函数介绍当Hive提供的内置函数无法满足你的业务处理需要时，此时可以考虑使用用户自定义函数（UDF: user-defined function）。Hive 中常用的UDF有如下三种: UDF一条记录使用函数后输出还是一条记录，比如:upper/substr; UDAF(User-Defined Aggregation Funcation)多条记录使用函数后输出还是一条记录，比如: count/max/min/sum/avg; UDTF(User-Defined Table-Generating Functions)一条记录使用函数后输出多条记录，比如: lateral view explore();2. Hive自定义函数开发需求:开发自定义函数，使得在指定字段前加上“Hello:”字样。Hive 中 UDF函数开发步骤:(1）继承UDF 类。(2）重写evaluate方法，该方法支持重载，每行记录执行一次evaluate方法。 ##### 注意：1 UDF必须要有返回值,可以是null,但是不能为 void.2 推荐使用 Text/LongWritable等Hadoop的类型,而不是Java类型(当然使用 Java类型也是可以的)。 功能实现: ( 1)pom.xml中添加UDF函数开发的依赖包。12345678910111213141516171819202122232425262728293031323334&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.7.0&lt;/hadoop.version&gt; &lt;hive.version&gt;1.1.0-cdh5.7.0&lt;/hive.version&gt;&lt;/properties&gt;&lt;!--CDH 版本建议大家添加一个repository--&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependencies&gt;&lt;!--Hadoop依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupld&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactld&gt; &lt;version&gt;$ &#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!--Hive依赖--&gt;&lt;dependency&gt; &lt;groupld&gt;org.apache.hive&lt;/groupld&gt; &lt;artifactId&gt;hive-exec&lt;/artifactId&gt; &lt;version&gt;$ &#123; hive.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupld&gt;org.apacne.hive&lt;/groupId&gt; &lt;artifactld&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$ &#123; hive.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;/dependencies&gt; (2）开发UDF函数。1234567891011121314151617181920212223242526272829303132package com.kgc.bigdata.hadoop.hive;import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;/***功能:输入xxx，输出:Hello: xxx**开发UDF 函数的步骤 * 1) extends UDF*2）重写evaluate方法，注意该方法是支持重载的*/public class HelloUDF extends UDF&#123;/***对于UDF 函数的evaluate的参数和返回值，个人建议使用Writable* @param name* @return*/public Text evaluate(Text name)&#123;return new Text(&quot;Hello: &quot; + name); &#125;public Text evaluate(Text name,IntWritable age)&#123;return new Text(&quot;Hello: &quot; +name + &quot; , age :&quot; + age); &#125;/功能测试public static void main(String[] args)&#123;HelloUDF udf = new HelloUDF(System.out.println(udf.evaluate(new Text(&quot;zhangsan&quot;)));System.out.println(udf.evaluate(new Text(&quot;zhangsan&quot;), new IntWritable(20))); &#125;&#125; (3）编译jar包上传到服务器。(4)将自定义UDF 函数添加到Hive 中去。 12add JAR/home/hadoop/lib/hive-1.0.jar;create temporary function sayHello as &#x27;com.kgc.bigdata.hadoop.hive.HelloUDF&#x27;; (5)使用自定义函数。//通过show functions可以看到我们自定义的sayHello函数show functions;//将员工表的ename作为自定义UDF函数的参数值，即可查看到最终的输出结果 1select empno, ename, sayHello(ename) from emp;","categories":[],"tags":[{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"}]},{"title":"Kafka分区","slug":"kafka分区","date":"2021-10-13T08:50:57.000Z","updated":"2021-10-25T06:41:33.205Z","comments":true,"path":"2021/10/13/kafka分区/","link":"","permalink":"https://www.ihadyou.cn/2021/10/13/kafka%E5%88%86%E5%8C%BA/","excerpt":"","text":"[TOC] 设置topic下的分区数 在 config/server.properties 配置文件中, 可以设置一个全局的分区数量, 这个分区数量的含义是: 每个主题下的分区数量, 默认为 1 也可以在创建主题的时候, 使用 –partitions 参数指定分区数量 1bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic my_topic --partitions 2 --replication-factor 1 3.查看已创建主题的分区数量: 1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my_topic 生产者与分区org.apache.kafka.clients.producer.internals.DefaultPartitioner 默认的分区策略是： 如果在发消息的时候指定了分区，则消息投递到指定的分区 如果没有指定分区，但是消息的key不为空，则基于key的哈希值来选择一个分区 如果既没有指定分区，且消息的key也是空，则用轮询的方式选择一个分区 消费者与分区首先需要了解的是: 消费者是以组的名义订阅主题消息, 消费者组里边包含多个消费者实例. 主题下边包含多个分区 消费者实例与主题下分区的分配关系 kafka 集群上有两个节点, 4 个分区 A组有 2 个消费者实例 (两个消费线程) B组有 4 个消费者实例 由图可以看出, A组的消费者C1, C2 平均要消费两个分区的数据, 而 B 组的消费者平均消费 一 个分区的数据 ( 最理想的状态 ), 得到的结论是 : 一条消息只能被一个消费组中的一个消费者实例消费到, (换句话说, 不可能出现组中的两个消费者负责同一个分区, 同组内消费者不会重复消费 ) 等等, 考虑的场景还不够, 下边再提些问题 : 如果分区数大于或等于组中的消费者实例数, 那就没有问题, 但是如果消费者实例的数量 &gt; 主题下分区数量, 那么按照默认的策略 ( 之所以强调默认策略是因为可以自定义策略 ), 有一些消费者是多余的, 一直接不到消息而处于空闲状态. 但是假设有消费者实例就是不安分, 就造成了多个消费者负责同一个分区, 这样会造成什么 ? (重复消费就太可怕了) 我们知道，Kafka它在设计的时候就是要保证分区下消息的顺序，也就是说消息在一个分区中的顺序是怎样的，那么消费者在消费的时候看到的就是什么样的顺序，那么要做到这一点就首先要保证消息是由消费者主动拉取的（pull），其次还要保证一个分区只能由一个消费者负责。倘若，两个消费者负责同一个分区，那么就意味着两个消费者同时读取分区的消息，由于消费者自己可以控制读取消息的offset (偏移量)，就有可能C1才读到2，而C2读到1，C1还没提交 offset，这时C2读到2了，相当于多线程读取同一个消息，会造成消息处理的重复，且不能保证消息的顺序，这就跟主动推送（push）无异。 消费者分区分配策略 (两种)range策略是基于每个主题的，对于每个主题，我们以数字顺序排列可用分区，以字典顺序排列消费者。然后，将分区数量除以消费者总数，以确定分配给每个消费者的分区数量。如果没有平均划分（PS：除不尽），那么最初的几个消费者将有一个额外的分区。 简而言之: range分配策略针对的是主题（也就是说，这里所说的分区指的某个主题的分区，消费者值的是订阅这个主题的消费者组中的消费者实例） 首先，将分区按数字顺序排行序，消费者按消费者名称的字典顺序排好序. 然后，用分区总数除以消费者总数。如果能够除尽，则皆大欢喜，平均分配；若除不尽，则位于排序前面的消费者将多负责一个分区. 例如，假设有两个消费者C0和C1，两个主题t0和t1，并且每个主题有3个分区，分区的情况是这样的：t0p0，t0p1，t0p2，t1p0，t1p1，t1p2 那么，基于以上信息，最终消费者分配分区的情况是这样的： C0: [t0p0, t0p1, t1p0, t1p1] C1: [t0p2, t1p2] 因为，对于主题t0，分配的结果是C0负责P0和P1，C1负责P2；对于主题t2，也是如此，综合起来就是这个结果 上面的过程用图形表示的话大概是这样的 : roundrobin (轮询) roundronbin分配策略的具体实现是org.apache.kafka.clients.consumer.RoundRobinAssignor 轮询分配策略是基于所有可用的消费者和所有可用的分区的 与前面的range策略最大的不同就是它不再局限于某个主题 如果所有的消费者实例的订阅都是相同的，那么这样最好了，可用统一分配，均衡分配 例如，假设有两个消费者C0和C1，两个主题t0和t1，每个主题有3个分区，分别是t0p0，t0p1，t0p2，t1p0，t1p1，t1p2 那么，最终分配的结果是这样的： C0: [t0p0, t0p2, t1p1] C1: [t0p1, t1p0, t1p2] 用图形表示大概是这样的:","categories":[],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://www.ihadyou.cn/tags/kafka/"}]},{"title":"linux环境变量加载顺序","slug":"linux环境变量加载顺序","date":"2021-10-13T08:49:14.000Z","updated":"2021-10-25T06:41:41.141Z","comments":true,"path":"2021/10/13/linux环境变量加载顺序/","link":"","permalink":"https://www.ihadyou.cn/2021/10/13/linux%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F/","excerpt":"","text":"01、环境变量文件描述/etc/profile: 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行,并从/etc/profile.d目录的配置文件中搜集shell的设置./etc/bashrc: 为每一个运行bash shell的用户执行此文件.当bash shell被打开时,该文件被读取. //用户级别的环境变量，用户可以覆盖全局变量~/.bash_profile: 每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件.~/.bashrc: 该文件包含专用于你的bash shell的bash信息,当登录时以及每次打开新的shell时,该文件被读取.~/.bash_logout: 当每次退出系统(退出bash shell)时,执行该文件. /etc/profile中设定的变量(全局)的可以作用于任何用户,而~/.bashrc等中设定的变量(局部)只能继承/etc/profile中的变量,他们是”父子”关系. ~/.bash_profile 是交互式、login 方式进入 bash 运行的~/.bashrc 是交互式 non-login 方式进入 bash 运行的通常二者设置大致相同，所以通常前者会调用后者 一、系统环境变量：/etc/profile ：这个文件预设了几个重要的变量，例如PATH, USER, LOGNAME, MAIL, INPUTRC, HOSTNAME, HISTSIZE, umask等等。 为系统的每个用户设置环境信息。当用户第一次登陆时，该文件执行，并从/etc/profile.d目录中的配置文件搜索shell的设置（可以用于设定针对全系统所有用户的环境变量，环境变量周期是永久的） /etc/bashrc ：这个文件主要预设umask以及PS1。这个PS1就是我们在敲命令时，前面那串字符了，例如 [root@localhost ~]#,当bash shell被打开时,该文件被读取 这个文件是针对所有用户的bash初始化文件，在此设定中的环境信息将应用与所有用户的shell中，此文件会在用户每次打开shell时执行一次。（即每次新开一个终端，都会执行/etc/bashrc）** 二、用户环境变量：.bash_profile ：定义了用户的个人化路径与环境变量的文件名称。每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次。（在这个文件中有执行.bashrc的脚本） .bashrc ：该文件包含专用于你的shell的bash信息,当登录时以及每次打开新的shell时,该该文件被读取。例如你可以将用户自定义的alias或者自定义变量写到这个文件中。 .bash_history ：记录命令历史用的。 .bash_logout ：当退出shell时，会执行该文件。可以把一些清理的工作放到这个文件中。 linux加载配置项时通过下面方式首先 加载/etc/profile配置 然后 加载/ect/profile.d/下面的所有脚本 然后 加载当前用户 .bash_profile 然后 加载.bashrc 最后 加载 [/etc/bashrc] /etc/profile → /etc/profile.d/*.sh → ~/.bash_profile → ~/.bashrc → [/etc/bashrc]","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://www.ihadyou.cn/tags/linux/"}]},{"title":"初识kudu","slug":"初识kudu","date":"2021-10-13T08:46:55.000Z","updated":"2021-11-01T06:52:52.997Z","comments":true,"path":"2021/10/13/初识kudu/","link":"","permalink":"https://www.ihadyou.cn/2021/10/13/%E5%88%9D%E8%AF%86kudu/","excerpt":"","text":"1、 kudu简介1.1、kudu是什么简单来说:dudu是一个与hbase类似的列式存储分布式数据库。官方给kudu的定位是:在更新更及时的基础上实现更快的数据分析 1.2、为什么需要kudu1.2.1、hdfs与hbase数据存储的缺点目前数据存储有了HDFS与hbase，为什么还要额外的弄一个kudu呢。HDFS:使用列式存储格式Apache Parquet，Apache ORC，适合离线分析，不支持单条纪录级别的update操作，随机读写性能差。HBASE:可以进行高效随机读写，却并不适用于基于SQL的数据分析方向，大批量数据获取时的性能较差。正因为HDFS与HBASE有上面这些缺点，KUDU较好的解决了HDFS与HBASE的这些缺点，它不及HDFS批处理快，也不及HBase随机读写能力强，但是反过来它比HBase批处理快（适用于OLAP的分析场景），而且比HDFS随机读写能力强（适用于实时写入或者更新的场景），这就是它能解决的问题。 2、架构介绍2.1、基本架构 2.1.1、概念 Table（表）：一张table是数据存储在kudu的位置。Table具有schema和全局有序的primary key(主键)。Table被分为很多段，也就是tablets. Tablet (段)：一个tablet是一张table连续的segment，与其他数据存储引擎或关系型数据的partition相似。Tablet存在副本机制，其中一个副本为leader tablet。任何副本都可以对读取进行服务，并且写入时需要在所有副本对应的tablet server之间达成一致性。 Tablet server：存储tablet和为tablet向client提供服务。对于给定的tablet，一个tablet server充当leader，其他tablet server充当该tablet的follower副本。只有leader服务写请求，leader与follower为每个服务提供读请求。 Master：主要用来管理元数据(元数据存储在只有一个tablet的catalog table中)，即tablet与表的基本信息，监听tserver的状态 Catalog Table: 元数据表，用来存储table(schema、locations、states)与tablet（现有的tablet列表，每个tablet及其副本所处tserver，tablet当前状态以及开始和结束键）的信息。 3、存储机制3.1 存储结构全景图 3.2 存储结构解析 一个Table包含多个Tablet，其中Tablet的数量是根据hash或者range进行设置 一个Tablet中包含MetaData信息和多个RowSet信息 一个Rowset中包含一个MemRowSet与0个或多个DiskRowset，其中MemRowSet存储insert的数据，一旦MemRowSet写满会flush到磁盘生成一个或多个DiskRowSet，此时MemRowSet清空。MemRowSet默认写满1G或者120s flush一次(注意:memRowSet是行式存储，DiskRowSet是列式存储，MemRowSet基于primary key有序)。每隔tablet中会定期对一些diskrowset做compaction操作，目的是对多个diskRowSet进行重新排序，以此来使其更有序并减少diskRowSet的数量，同时在compaction的过程中慧慧resolve掉deltaStores当中的delete记录 一个DiskRowSet包含baseData与DeltaStores两部分，其中baseData存储的数据看起来不可改变，DeltaStores中存储的是改变的数据 DeltaStores包含一个DeltaMemStores和多个DeltaFile,其中DeltaMemStores放在内存，用来存储update与delete数据，一旦DeltaMemStores写满，会flush成DeltaFile。当DeltaFile过多会影响查询性能，所以KUDU每隔一段时间会执行compaction操作，将其合并到baseData中，主要是resolve掉update数据。 4、kudu的工作机制4.1 概述1、kudu主要角色分为master与tserver2、master主要负责:管理元数据信息，监听server，当server宕机后负责tablet的重分配3、tserver主要负责tablet的存储与和数据的增删改查 4.2 内部实现原理图4.3 读流程4.3.1 概述客户端将要读取的数据信息发送给master，master对其进行一定的校验，比如表是否存在，字段是否存在。Master返回元数据信息给client，然后client与tserver建立连接，通过metaData找到数据所在的RowSet，首先加载内存里面的数据(MemRowSet与DeltMemStore),然后加载磁盘里面的数据，最后返回最终数据给client. 4.3.2 详细步骤图 4.3.3 详细步骤解析1、客户端master请求查询表指定数据2、master对请求进行校验，校验表是否存在，schema中是否存在指定查询的字段，主键是否存在3、master通过查询catalog Table返回表，将tablet对应的tserver信息、tserver状态等元数据信息返回给client4、client与tserver建立连接，通过metaData找到primary key对应的RowSet。5、首先加载RowSet内存中MemRowSet与DeltMemStore中的数据6、然后加载磁盘中的数据，也就是DiskRowSet中的BaseData与DeltFile中的数据7、返回数据给Client8、继续4-7步骤，直到拿到所有数据返回给client 4.4、插入流程4.4.1 概述Client首先连接master，获取元数据信息。然后连接tserver，查找MemRowSet与DeltMemStore中是否存在相同primary key，如果存在，则报错;如果不存在，则将待插入的数据写入WAL日志，然后将数据写入MemRowSet。 4.4.2 详细步骤图 4.4.3 详细步骤解析1、client向master请求预写表的元数据信息2、master会进行一定的校验，表是否存在，字段是否存在等3、如果master校验通过，则返回表的分区、tablet与其对应的tserver给client；如果校验失败则报错给client。4、client根据master返回的元数据信息，将请求发送给tablet对应的tserver.5、tserver首先会查询内存中MemRowSet与DeltMemStore中是否存在与待插入数据主键相同的数据，如果存在则报错6、tserver会讲写请求预写到WAL日志，用来server宕机后的恢复操作7、将数据写入内存中的MemRowSet中，一旦MemRowSet的大小达到1G或120s后，MemRowSet会flush成一个或DiskRowSet,用来将数据持久化8、返回client数据处理完毕 4.5、数据更新流程4.5.1 概述Client首先向master请求元数据，然后根据元数据提供的tablet信息，连接tserver，根据数据所处位置的不同，有不同的操作:在内存MemRowSet中的数据，会将更新信息写入数据所在行的mutation链表中；在磁盘中的数据，会将更新信息写入DeltMemStore中。 4.5.2、详细步骤图 4.5.3 详细步骤解析1、client向master请求预更新表的元数据，首先master会校验表是否存在，字段是否存在，如果校验通过则会返回给client表的分区、tablet、tablet所在tserver信息2、client向tserver发起更新请求3、将更新操作预写如WAL日志，用来在server宕机后的数据恢复4、根据tserver中待更新的数据所处位置的不同，有不同的处理方式:如果数据在内存中，则从MemRowSet中找到数据所处的行，然后在改行的mutation链表中写入更新信息，在MemRowSet flush的时候，将更新合并到baseData中如果数据在DiskRowSet中，则将更新信息写入DeltMemStore中，DeltMemStore达到一定大小后会flush成DeltFile。5、更新完毕后返回消息给client。","categories":[],"tags":[{"name":"kudu","slug":"kudu","permalink":"https://www.ihadyou.cn/tags/kudu/"}]},{"title":"消息队列易错指南","slug":"消息队列易错指南","date":"2021-10-13T08:38:04.000Z","updated":"2021-10-15T07:17:45.680Z","comments":true,"path":"2021/10/13/消息队列易错指南/","link":"","permalink":"https://www.ihadyou.cn/2021/10/13/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%98%93%E9%94%99%E6%8C%87%E5%8D%97/","excerpt":"","text":"1.消息队列的坑之非幂等（1）幂等性概念所谓幂等性就是无论多少次操作和第一次的操作结果一样。如果消息被多次消费，很有可能造成数据的不一致。而如果消息不可避免地被消费多次，如果我们开发人员能通过技术手段保证数据的前后一致性，那也是可以接受的 。 RabbitMQ、RocketMQ、Kafka 消息队列中间件都有可能出现消息重复消费问题。这种问题并不是 MQ 自己保证的，而是需要开发人员来保证。 这几款消息队列中间都是是全球最牛的分布式消息队列，那肯定考虑到了消息的幂等性。我们以 Kafka 为例，看看 Kafka 是怎么保证消息队列的幂等性。 Kafka 有一个 偏移量 的概念，代表着消息的序号，每条消息写到消息队列都会有一个偏移量，消费者消费了数据之后，每过一段固定的时间，就会把消费过的消息的偏移量提交一下，表示已经消费过了，下次消费就从偏移量后面开始消费。 （2）避坑指南微信支付结果通知场景 微信官方文档上提到微信支付通知结果可能会推送多次，需要开发者自行保证幂等性。第一次我们可以直接修改订单状态（如支付中 -&gt; 支付成功），第二次就根据订单状态来判断，如果不是支付中，则不进行订单处理逻辑。 插入数据库场景 每次插入数据时，先检查下数据库中是否有这条数据的主键 id，如果有，则进行更新操作。 写 Redis 场景 Redis 的 Set 操作天然幂等性，所以不用考虑 Redis 写数据的问题。 其他场景方案 生产者发送每条数据时，增加一个全局唯一 id，类似订单 id。每次消费时，先去 Redis 查下是否有这个 id，如果没有，则进行正常处理消息，且将 id 存到 Redis。如果查到有这个 id，说明之前消费过，则不要进行重复处理这条消息。 不同业务场景，可能会有不同的幂等性方案，大家选择合适的即可，上面的几种方案只是提供常见的解决思路。 2.消息队列的坑之消息丢失 消息丢失会带来什么问题？如果是订单下单、支付结果通知、扣费相关的消息丢失，则可能造成财务损失，如果量很大，就会给甲方带来巨大损失 （1）生产者存放消息的过程中丢失消息解决方案 事务机制（不推荐，异步方式） 对于 RabbitMQ 来说，生产者发送数据之前开启 RabbitMQ 的事务机制channel.txselect ，如果消息没有进队列，则生产者受到异常报错，并进行回滚 channel.txRollback，然后重试发送消息；如果收到了消息，则可以提交事务 channel.txCommit。但这是一个同步的操作，会影响性能。 confirm 机制（推荐，异步方式） 我们可以采用另外一种模式： confirm 模式来解决同步机制的性能问题。每次生产者发送的消息都会分配一个唯一的 id，如果写入到了 RabbitMQ 队列中，则 RabbitMQ 会回传一个 ack 消息，说明这个消息接收成功。如果 RabbitMQ 没能处理这个消息，则回调 nack 接口。说明需要重试发送消息。 也可以自定义超时时间 + 消息 id 来实现超时等待后重试机制。但可能出现的问题是调用 ack 接口时失败了，所以会出现消息被发送两次的问题，这个时候就需要保证消费者消费消息的幂等性。 事务模式 和 confirm 模式的区别： 事务机制是同步的，提交事务后悔被阻塞直到提交事务完成后。 confirm 模式异步接收通知，但可能接收不到通知。需要考虑接收不到通知的场景。 （2）消息队列丢失消息消息队列的消息可以放到内存中，或将内存中的消息转到硬盘（比如数据库）中，一般都是内存和硬盘中都存有消息。如果只是放在内存中，那么当机器重启了，消息就全部丢失了。如果是硬盘中，则可能存在一种极端情况，就是将内存中的数据转换到硬盘的期间中，消息队列出问题了，未能将消息持久化到硬盘。 解决方案 创建 Queue 的时候将其设置为持久化。这个地方没搞懂，欢迎探讨解答。 发送消息的时候将消息的 deliveryMode 设置为 2 。 开启生产者 confirm 模式，可以重试发送消息。 （3）消费者丢失消息消费者刚拿到数据，还没开始处理消息，结果进程因为异常退出了，消费者没有机会再次拿到消息。 解决方案 关闭 RabbitMQ 的自动 ack，每次生产者将消息写入消息队列后，就自动回传一个 ack 给生产者。 消费者处理完消息再主动 ack，告诉消息队列我处理完了。 问题： 那这种主动 ack 有什么漏洞了？如果 主动 ack 的时候挂了，怎么办？ 则可能会被再次消费，这个时候就需要幂等处理了。 问题： 如果这条消息一直被重复消费怎么办？ 则需要有加上重试次数的监测，如果超过一定次数则将消息丢失，记录到异常表或发送异常通知给值班人员。 （4）RabbitMQ 消息丢失总结 （5）Kafka 消息丢失场景：Kafka 的某个 broker（节点）宕机了，重新选举 leader （写入的节点）。如果 leader 挂了，follower 还有些数据未同步完，则 follower 成为 leader 后，消息队列会丢失一部分数据。 解决方案 给 topic 设置 replication.factor 参数，值必须大于 1，要求每个 partition 必须有至少 2 个副本。 给 kafka 服务端设置 min.insyc.replicas 必须大于 1，表示一个 leader 至少一个 follower 还跟自己保持联系。 3. 消息队列的坑之消息乱序 用户先下单成功，然后取消订单，如果顺序颠倒，则最后数据库里面会有一条下单成功的订单。 RabbitMQ 场景： 生产者向消息队列按照顺序发送了 2 条消息，消息1：增加数据 A，消息2：删除数据 A。 期望结果：数据 A 被删除。 但是如果有两个消费者，消费顺序是：消息2、消息 1。则最后结果是增加了数据 A。 RabbitMQ 解决方案： 将 Queue 进行拆分，创建多个内存 Queue，消息 1 和 消息 2 进入同一个 Queue。 创建多个消费者，每一个消费者对应一个 Queue。 Kafka 场景： 创建了 topic，有 3 个 partition。 创建一条订单记录，订单 id 作为 key，订单相关的消息都丢到同一个 partition 中，同一个生产者创建的消息，顺序是正确的。 为了快速消费消息，会创建多个消费者去处理消息，而为了提高效率，每个消费者可能会创建多个线程来并行的去拿消息及处理消息，处理消息的顺序可能就乱序了。 Kafka 解决方案： 解决方案和 RabbitMQ 类似，利用多个 内存 Queue，每个线程消费 1个 Queue。 具有相同 key 的消息 进同一个 Queue。 4. 消息队列的坑之消息积压消息积压：消息队列里面有很多消息来不及消费。 场景 1： 消费端出了问题，比如消费者都挂了，没有消费者来消费了，导致消息在队列里面不断积压。 场景 2： 消费端出了问题，比如消费者消费的速度太慢了，导致消息不断积压。 比如线上正在做订单活动，下单全部走消息队列，如果消息不断积压，订单都没有下单成功 ，那么会造成很大的损失 解决方案：解铃还须系铃人 修复代码层面消费者的问题，确保后续消费速度恢复或尽可能加快消费的速度。 停掉现有的消费者。 临时建立好原先 5 倍的 Queue 数量。 临时建立好原先 5 倍数量的 消费者。 将堆积的消息全部转入临时的 Queue，消费者来消费这些 Queue。 5. 消息队列的坑之消息过期失效 RabbitMQ 可以设置过期时间，如果消息超过一定的时间还没有被消费，则会被 RabbitMQ 给清理掉。消息就丢失了 解决方案： 准备好批量重导的程序 手动将消息闲时批量重导 6. 消息队列的坑之队列写满 当消息队列因消息积压导致的队列快写满，所以不能接收更多的消息了。生产者生产的消息将被丢弃。 解决方案： 判断哪些是无用的消息，RabbitMQ 可以进行 Purge Message 操作。 如果是有用的消息，则需要将消息快速消费，将消息里面的内容转存到数据库。 准备好程序将转存在数据库中的消息再次重导到消息队列。 闲时重导消息到消息队列。 原文链接：https://www.cnblogs.com/jackson0714/p/fenbushi.html","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://www.ihadyou.cn/tags/hbase/"},{"name":"hive","slug":"hive","permalink":"https://www.ihadyou.cn/tags/hive/"},{"name":"zookeeper","slug":"zookeeper","permalink":"https://www.ihadyou.cn/tags/zookeeper/"},{"name":"sqoop","slug":"sqoop","permalink":"https://www.ihadyou.cn/tags/sqoop/"},{"name":"hadoop","slug":"hadoop","permalink":"https://www.ihadyou.cn/tags/hadoop/"},{"name":"kafka","slug":"kafka","permalink":"https://www.ihadyou.cn/tags/kafka/"},{"name":"linux","slug":"linux","permalink":"https://www.ihadyou.cn/tags/linux/"},{"name":"kudu","slug":"kudu","permalink":"https://www.ihadyou.cn/tags/kudu/"}]}